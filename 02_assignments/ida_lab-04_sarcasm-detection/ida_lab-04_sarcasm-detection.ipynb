{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Лабораторна робота 4.</center></h1>\n",
    "<h2><center>Виявлення сарказму за текстовими повідомленнями з використанням логістичної регресії</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Виконав:** Прізвище І.П.\n",
    "\n",
    "**Варіант:** №__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В цій лабораторній роботі ми будемо подувати модель логістичної регресії для розв'язання задачі класифікації саркастичних текстів. Для цього використаємо набір даних з [наукової статі](https://arxiv.org/abs/1704.05579) «A Large Self-Annotated Corpus for Sarcasm» з понад 1 мільйоном коментарів із платформи Reddit, що позначені авторами статті як саркастичні або ні. Оброблена версія цього набору даних міститься на платформі Kaggle у формі [Набору даних Kaggle](https://www.kaggle.com/datasets/danofer/sarcasm/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зміст\n",
    "\n",
    "- [4.1. Попередній аналіз даних](#lab-4.1)\n",
    "- [4.2. Векторизація TF-IDF та логістична регресія](#lab-4.2)\n",
    "- [4.3. Інтерпретація та порівняння моделі](#lab-4.3)\n",
    "- [4.4. Розширене вдосконалення моделі](#lab-4.4)\n",
    "- [4.5. Практичне застосування результатів інтелектуального аналізу даних](#lab-4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спершу завантажте цільовий набір даних `train-balanced-sarcasm.csv` із ресурсу Kaggle за [посиланням](https://www.kaggle.com/datasets/danofer/sarcasm/data?select=train-balanced-sarcasm.csv). Збережіть файл .csv в теку з робочим файлом лабораторної роботи.\n",
    "\n",
    "Оригінальний набір даних містить 1,3 млн. саркастичних коментарів з платформи інтернет-коментарів Reddit. Його було створено через вилучення коментарів з Reddit, що містять тег `\\s` (\"сарказм\"). Цей тег переважно використовується користувачами, щоб вказати на жартівливий тон їхнього тексту, тобто такий запис не повинен сприйматися серйозно, і, як правило, є надійним показником саркастичного змісту запису.\n",
    "\n",
    "Дані мають збалансовану та незбалансовану (тобто справжній розподіл) версії. Реальне співвідношення несаркастичних до саркастичних коментарів становить приблизно 1:100. За потреби оригінальний набір даних можна отримати за [посиланням](https://github.com/NLPrinceton/SARC).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:38.704867600Z",
     "start_time": "2023-11-21T14:39:38.692862900Z"
    },
    "_uuid": "ed87ab2845921166bb73ca854bfe1ef013c035e9"
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"train-balanced-sarcasm.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:40.010896800Z",
     "start_time": "2023-11-21T14:39:38.706865200Z"
    },
    "_uuid": "ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4"
   },
   "outputs": [],
   "source": [
    "# Виконаємо завантаження основних бібліотек\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:43.077555Z",
     "start_time": "2023-11-21T14:39:40.012897800Z"
    },
    "_uuid": "b23e4fc7a1973d60e0c6da8bd60f3d921542a856"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижче переглянемо зразки цільового набору в розрізі основних ознак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:43.092558100Z",
     "start_time": "2023-11-21T14:39:43.078556900Z"
    },
    "_uuid": "4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:43.818068200Z",
     "start_time": "2023-11-21T14:39:43.095558600Z"
    },
    "_uuid": "0a7ed9557943806c6813ad59c3d5ebdb403ffd78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1010826 entries, 0 to 1010825\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   label           1010826 non-null  int64 \n",
      " 1   comment         1010771 non-null  object\n",
      " 2   author          1010826 non-null  object\n",
      " 3   subreddit       1010826 non-null  object\n",
      " 4   score           1010826 non-null  int64 \n",
      " 5   ups             1010826 non-null  int64 \n",
      " 6   downs           1010826 non-null  int64 \n",
      " 7   date            1010826 non-null  object\n",
      " 8   created_utc     1010826 non-null  object\n",
      " 9   parent_comment  1010826 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 77.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"
   },
   "source": [
    "Деяких коментарів бракує в наборі, тому ми видалимо відповідні рядки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.123072100Z",
     "start_time": "2023-11-21T14:39:43.777084800Z"
    },
    "_uuid": "97b2d85627fcde52a506dbdd55d4d6e4c87d3f08"
   },
   "outputs": [],
   "source": [
    "train_df.dropna(subset=[\"comment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d51637ee70dca7693737ad0da1dbb8c6ce9230b"
   },
   "source": [
    "Переконаємось, що розглядуваний набір даних є справді збалансованим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.138071100Z",
     "start_time": "2023-11-21T14:39:44.132073900Z"
    },
    "_uuid": "addd77c640423d30fd146c8d3a012d3c14481e11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    505403\n",
       "1    505368\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b836574e5093c5eb2e9063fefe1c8d198dcba79"
   },
   "source": [
    "Розділимо дані на частини для навчання та валідації."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.389073100Z",
     "start_time": "2023-11-21T14:39:44.137067500Z"
    },
    "_uuid": "c200add4e1dcbaa75164bbcc73b9c12ecb863c96"
   },
   "outputs": [],
   "source": [
    "train_texts, valid_texts, y_train, y_valid = train_test_split(\n",
    "    train_df[\"comment\"], train_df[\"label\"], random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba1a8f65032c5954476a68e01b607655145b746d"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.1. Попередній аналіз даних</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.405070600Z",
     "start_time": "2023-11-21T14:39:44.390070Z"
    },
    "_uuid": "c2c613ee2052a2c0379682adf5c23d1f751f4c3b"
   },
   "outputs": [],
   "source": [
    "# from wordcloud import STOPWORDS, WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.442068900Z",
     "start_time": "2023-11-21T14:39:44.409069Z"
    },
    "_uuid": "ae7333d67f6a17673d2aa16aed3017e2fbef9b58"
   },
   "outputs": [],
   "source": [
    "# Підготуємо об'єкт класу WordCloud для \n",
    "# можливого візуального аналізу  \n",
    "# my_wordcloud = WordCloud(\n",
    "#     background_color=\"black\",\n",
    "#     stopwords=STOPWORDS,\n",
    "#     max_words=200,\n",
    "#     max_font_size=100,\n",
    "#     random_state=17,\n",
    "#     width=800,\n",
    "#     height=400,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 1</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Когортний аналіз довжини тексту:**  \n",
    "- Мета: з’ясувати, як особливості довжини повідомлення впливають на логістичну регресію, що класифікує сарказм у `train-balanced-sarcasm.csv`.  \n",
    "- Кроки: \n",
    "  1. Завантажте дані, залиште стовпці `comment` та ціль `label`. Додайте похідні ознаки: кількість символів, слів, середню довжину слова, частку великих літер, щільність знаків пунктуації.\n",
    "  2. Нормалізуйте ознаки (наприклад, `StandardScaler`) та натренуйте `LogisticRegression` з регуляризацією L2 та `class_weight='balanced'` у 5-кратній `StratifiedKFold` валідації.\n",
    "  3. Побудуйте графік log-odds прогнозу проти децилів довжини й прокоментуйте, в яких діапазонах модель найчастіше помиляється.\n",
    "- Підказки: використовуйте `np.log1p` для стабілізації розподілів; підгляньте в `df['comment'].str.count(r'[?!]')`; збережіть пайплайн через `sklearn.pipeline.Pipeline`.  \n",
    "- Перевірка: порівняйте ROC AUC моделі на цих ознаках із наївним класифікатором та додайте коротку інтерпретацію коефіцієнтів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – Стемінг vs лематизація в єдиному конвеєрі:**  \n",
    "- Мета: зіставити ефективність двох лінгвістичних конвеєрів (стемінг і лематизація) для `LogisticRegression` у задачі виявлення сарказму.  \n",
    "- Кроки:\n",
    "  1. Побудуйте два `Pipeline`: (а) `TfidfVectorizer` + `PorterStemmer` через кастомний токенайзер; (б) `spacy` `nlp.pipe()` для лем.\n",
    "  2. Для кожного конвеєра виконайте `GridSearchCV` за параметрами `ngram_range`, `max_df`, `C`.\n",
    "  3. На валідаційній вибірці порівняйте F1, ROC AUC і проаналізуйте 15 найсильніших коефіцієнтів моделі.\n",
    "- Підказки: кешуйте результат лемматизації у стовпці, щоби не запускати `spacy` повторно; скористайтеся `FunctionTransformer` для попередньої обробки.  \n",
    "- Перевірка: оформіть таблицю з порівнянням метрик та зробіть висновок, яка стратегія краще для саркастичних висловлювань.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Частотні фільтри та n-грамні ознаки:**  \n",
    "- Мета: дослідити, як вибір порогів `min_df`/`max_df` і діапазону n-грам впливає на логістичну регресію.  \n",
    "- Кроки:\n",
    "  1. Підготуйте три конфігурації `CountVectorizer`: (1,1) з `min_df=5`, (1,2) з `max_df=0.95`, (1,3) з обрізанням до 30 тис. ознак.\n",
    "  2. Для кожної конфігурації тренуйте `LogisticRegression` (saga) з L1-регуляризацією, використовуючи `train_test_split` і `StratifiedKFold` для внутрішньої валідації.\n",
    "  3. Побудуйте криву навчання (train/test score vs кількість прикладів) та поясніть перенавчання/недонавчання.\n",
    "- Підказки: використовуйте `sklearn.model_selection.learning_curve`; для великих матриць задійте `sparse` формат.  \n",
    "- Перевірка: підсумуйте результати в таблиці, виділіть найкращу конфігурацію та аргументуйте вибір.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – Балансування класів і ваги моделі:**  \n",
    "- Мета: оцінити, чи покращують `class_weight`, under/over-sampling та порогове налаштування результати логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Згенеруйте базову модель TF-IDF + `LogisticRegression` без балансування.\n",
    "  2. Створіть два додаткові пайплайни: (а) `class_weight='balanced'`; (б) `SMOTE` або `RandomUnderSampler` із `imblearn` перед логістичною регресією.\n",
    "  3. Для кожного підходу підберіть оптимальний поріг класифікації за кривою precision-recall.\n",
    "- Підказки: використовуйте `imblearn.pipeline.Pipeline`; метрику f2-score можна застосувати, якщо важливі позитивні (саркастичні) кейси.  \n",
    "- Перевірка: вкажіть, як змінилися recall і precision та чи варто застосовувати балансування для цієї вибірки.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Регістр та символьні n-грами:**  \n",
    "- Мета: дослідити чутливість логістичної регресії до регістру, поєднавши словні та символьні TF-IDF ознаки.  \n",
    "- Кроки:\n",
    "  1. Побудуйте два корпуси: сирий текст і текст, нормалізований до нижнього регістру.\n",
    "  2. Створіть `FeatureUnion`, що об’єднує словні n-грам TF-IDF і символьні n-грам TF-IDF (3–5).\n",
    "  3. Порівняйте моделі, які тренуються на сирому й на нормалізованому тексті, за F1 та log-loss.\n",
    "- Підказки: `sklearn.pipeline.FeatureUnion` дозволяє з’єднати кілька векторизаторів; не забудьте про `max_features`.  \n",
    "- Перевірка: проаналізуйте коефіцієнти, які відповідають за капс-лок слова, і зробіть висновок щодо важливості регістру.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Видалення стоп-слів і відбір ознак:**  \n",
    "- Мета: оцінити вплив різних списків стоп-слів і методів відбору ознак на логістичну регресію.  \n",
    "- Кроки:\n",
    "  1. Зберіть три списки стоп-слів: стандартний `nltk`, розширений (додайте власні), адаптивний (за частотою в несаркастичних текстах).\n",
    "  2. Для кожного списку побудуйте TF-IDF матрицю та застосуйте `SelectKBest` (chi2 або mutual information) для вибору 5/10/20 тис. ознак.\n",
    "  3. Тренуйте логістичну регресію та порівняйте метрики на валідації, заміряйте час навчання.\n",
    "- Підказки: автоматизуйте перебір параметрів через `ParameterGrid`; збережіть результати в `DataFrame`.  \n",
    "- Перевірка: побудуйте графік залежності f1-score від кількості ознак для різних списків стоп-слів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Біграми, триграми й контекстні маркери:**  \n",
    "- Мета: виявити найсильніші колокації, що сигналізують сарказм, та інтегрувати їх у логістичну модель.  \n",
    "- Кроки:\n",
    "  1. Створіть TF-IDF з `ngram_range=(1,3)` та обмежте словник словами, що трапляються щонайменше 10 разів.\n",
    "  2. Обчисліть PMI (pointwise mutual information) для біграм саркастичних повідомлень і додайте бінарні ознаки присутності топ-50 колокацій до матриці ознак.\n",
    "  3. Натренуйте логістичну регресію та дослідіть, як змінюються коефіцієнти при додаванні цих бінарних ознак.\n",
    "- Підказки: використайте `sklearn.preprocessing.OneHotEncoder` або `DictVectorizer` для бінарних ознак.  \n",
    "- Перевірка: підготуйте список із 10 колокацій, що отримали найбільші ваги, та коротко поясніть їх семантику.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Моделювання числових патернів у тексті:**  \n",
    "- Мета: перевірити, як числові згадки (дати, відсотки, суми) впливають на передбачення сарказму логістичною регресією.  \n",
    "- Кроки:\n",
    "  1. Витягніть із `comment` регулярними виразами числа та класифікуйте їх за типами (рік, відсоток, гроші, порядкові).\n",
    "  2. Побудуйте змішану матрицю ознак: TF-IDF + числові агрегати (кількість чисел, типи, середнє значення).\n",
    "  3. Тренуйте логістичну регресію та оцініть, як додавання числових ознак змінює AUC і точність.\n",
    "- Підказки: зручно зберігати тип числа окремим стовпцем і перетворювати через `OneHotEncoder`.  \n",
    "- Перевірка: створіть bar-chart впливу коефіцієнтів при числових ознаках і сформулюйте висновок.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Пунктуаційні профілі саркастичних коментарів:**  \n",
    "- Мета: побудувати логістичну модель, що враховує насиченість пунктуацією як окремий блок ознак.  \n",
    "- Кроки:\n",
    "  1. Зрахуйте частоту `!`, `?`, `...`, лапок, емодзі для кожного коментаря; нормуйте на довжину тексту.\n",
    "  2. Об’єднайте пунктуаційні ознаки з TF-IDF і протестуйте дві моделі: без пунктуації та з пунктуацією.\n",
    "  3. Додатково дослідіть взаємодію `?!` та `!?` як біграми символів, ввівши бінарні ознаки.\n",
    "- Підказки: використовуйте `Series.str.count`; для емодзі можна застосувати `regex` з категорією `\\p{Extended_Pictographic}`.  \n",
    "- Перевірка: проаналізуйте, які пунктуаційні патерни мають найвищі позитивні коефіцієнти, та зробіть висновки.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – Цитати та звертання у саркастичних діалогах:**  \n",
    "- Мета: з’ясувати, як присутність цитат, тегів адресата й маркерів діалогу впливають на логістичну регресію.  \n",
    "- Кроки:\n",
    "  1. За допомогою regex знайдіть цитати (`\"...\"`, `> текст`) і звертання (`@username`, `dear` тощо), створіть відповідні ознаки.\n",
    "  2. Об’єднайте ці ознаки з TF-IDF і натренуйте логістичну регресію, порівнявши її з базовою моделлю без цитат.\n",
    "  3. Проведіть error analysis на 30 прикладах, де модель з цитатами змінила прогноз, і опишіть патерни.\n",
    "- Підказки: використовуйте `np.where` для створення бінарних ознак; звертайте увагу на колонку `parent_comment`, щоб уточнити контекст.  \n",
    "- Перевірка: додайте короткий звіт про зміну precision/recall після введення нових ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – Інтенсивність CAPS LOCK і подовжених слів:**  \n",
    "- Мета: оцінити роль підвищеної візуальної експресії (повністю верхній регістр, подовження літер) у виявленні сарказму.  \n",
    "- Кроки:\n",
    "  1. Порахуйте частку слів, записаних повністю великими літерами, та кількість подовжених токенів (`soooo`).\n",
    "  2. Додайте ці показники до пайплайна TF-IDF + `LogisticRegression` і проведіть крос-валідацію.\n",
    "  3. Сегментуйте коментарі за порогами CAPS (0, <5%, ≥5%) та порівняйте середні log-odds прогнозу.\n",
    "- Підказки: для пошуку подовжених слів використовуйте regex `r'(\\\\w)\\\\1{2,}'`; не забудьте про нормування на довжину.  \n",
    "- Перевірка: опишіть, чи допомагають ці ознаки підвищити recall саркастичних випадків.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Спеціальні символи, емодзі та міжмовні вставки:**  \n",
    "- Мета: інтегрувати у логістичну модель сигнали з емодзі, символів сарказму (`¯\\\\_(ツ)_/¯`) та іншомовних вставок.  \n",
    "- Кроки:\n",
    "  1. Створіть словник емодзі/ASCII-артів, які часто позначають сарказм; додайте бінарні ознаки їх присутності.\n",
    "  2. За допомогою `langdetect` визначте мову кожного речення і порахуйте частку іншомовних фраз у коментарі.\n",
    "  3. Зберіть комбінований пайплайн TF-IDF + емодзі + мовні частки та натренуйте логістичну регресію; оцініть вплив додаткових ознак.\n",
    "- Підказки: кешуйте `langdetect` через `apply`; емодзі можна зберегти в JSON і завантажити як довідник.  \n",
    "- Перевірка: надайте таблицю коефіцієнтів для топ-емодзі та поясніть їхню роль.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – Контекст батьківського коментаря:**  \n",
    "- Мета: використати інформацію з `parent_comment`, щоб покращити логістичну регресію для сарказму.  \n",
    "- Кроки:\n",
    "  1. З’єднайте `comment` та `parent_comment`, сформуйте ознаки: TF-IDF коментаря, TF-IDF батьківського тексту, різницю їхніх векторів (наприклад, шляхом віднімання середніх).\n",
    "  2. Побудуйте `FeatureUnion`, що передає обидва вектори у логістичну регресію (дві гілки зі своїми векторизаторами).\n",
    "  3. Перевірте, чи поліпшується точність/recall на саркастичних зразках із явним контрастом між репліками.\n",
    "- Підказки: використайте `ColumnTransformer`, щоб обробляти `comment` і `parent_comment` окремо; збережіть приклади з найбільшим приростом log-odds.  \n",
    "- Перевірка: проаналізуйте 5–7 кейсів, де контекст батьківської репліки змінив рішення моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – Абревіатури та акустичні скорочення:**  \n",
    "- Мета: дослідити, як наявність абревіатур (LOL, IMO) впливає на результат логістичної регресії та чи варто їх нормалізувати.  \n",
    "- Кроки:\n",
    "  1. Витягніть усі токени у верхньому регістрі довжиною ≥2 символів, побудуйте словник частот для саркастичних/несаркастичних класів.\n",
    "  2. Створіть дві версії корпусу: із збереженими абревіатурами та з розгорнутими формами (замінюйте через вузький словник).\n",
    "  3. Порівняйте логістичні моделі на цих двох корпусах, оцініть зміну метрик та коефіцієнтів.\n",
    "- Підказки: використовуйте `re.findall(r'\\\\b[A-Z]{2,}\\\\b', text)`; для розгортання готуйте mapper через `dict`.  \n",
    "- Перевірка: зробіть висновок, чи доречно розгортати абревіатури в задачі сарказму.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Запозичення, сленг і мемні маркери:**  \n",
    "- Мета: з’ясувати роль іншомовного сленгу та мемних виразів у сарказмі.  \n",
    "- Кроки:\n",
    "  1. Підготуйте словник сленгу/мемів (наприклад, з відкритих списків) і порахуйте їх присутність у `comment`.\n",
    "  2. Додайте бінарні та кількісні ознаки (кількість сленгових токенів, їхня частка) до TF-IDF матриці.\n",
    "  3. Проаналізуйте моделі з/без цих ознак, звернувши увагу на precision для класу сарказму.\n",
    "- Підказки: сформуйте словник у форматі CSV та прочитайте його через `pd.read_csv`; використовуйте `tokenizer` з `sklearn`, щоб застосовувати словник.  \n",
    "- Перевірка: надайте короткий словничок із 10 мемів, що отримали найбільшу вагу.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Гіперпосилання та цитовані джерела:**  \n",
    "- Мета: проаналізувати, чи впливають URL, згадки доменів та джерела на передбачення сарказму.  \n",
    "- Кроки:\n",
    "  1. Видобудьте всі URL (`re.findall(r'http\\\\S+')`), витягніть домен, класифікуйте домени за категоріями (новини, соцмережі, інше).\n",
    "  2. Створіть ознаки: кількість URL, типи доменів, довжина найкоротшого/найдовшого URL.\n",
    "  3. Інтегруйте ці ознаки в логістичну регресію та оціни, як змінюється log-loss і calibration curve.\n",
    "- Підказки: скористайтеся `urllib.parse.urlparse`; для категоризації доменів можна побудувати ручне зіставлення.  \n",
    "- Перевірка: додайте висновок, чи URL слугують сильним маркером сарказму.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Дублікатні патерни та шаблонні фрази:**  \n",
    "- Мета: виявити повторювані фрази, що часто позначають сарказм, та інтегрувати їх у модель.  \n",
    "- Кроки:\n",
    "  1. Знайдіть точні та майже точні дублікати (`duplicated`, `n-gram hashing`) серед `comment`; збережіть шаблони з високою частотою.\n",
    "  2. Створіть бінарні ознаки для топ-100 повторюваних фраз та додайте їх до TF-IDF.\n",
    "  3. Перевірте, чи покращується recall для сарказму після введення шаблонів.\n",
    "- Підказки: використовуйте `sklearn.feature_extraction.text.HashingVectorizer` для швидкого пошуку схожих текстів; дублікати можна групувати через `groupby`.  \n",
    "- Перевірка: опишіть 5 шаблонів, що підвищили ймовірність сарказму, та вкажіть їхні коефіцієнти.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Інтеграція сентиментних ознак:**  \n",
    "- Мета: поєднати результати аналізу тональності з логістичною регресією для сарказму.  \n",
    "- Кроки:\n",
    "  1. Обчисліть сентимент `compound` за допомогою `VADER` або `TextBlob` для кожного `comment`.\n",
    "  2. Згенеруйте додаткові ознаки: полярність, суб’єктивність, різниця тональності між `comment` і `parent_comment`.\n",
    "  3. Додайте ці ознаки до TF-IDF + LogisticRegression та оцініть, як змінюються precision-recall криві.\n",
    "- Підказки: нормуйте тональність, використовуючи `MinMaxScaler`; передбачте можливість від’ємних значень.  \n",
    "- Перевірка: наведіть приклади саркастичних коментарів із позитивною тональністю та проаналізуйте прогноз.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – Мовна ідентифікація та багатомовність:**  \n",
    "- Мета: дослідити вплив мішаних мов на логістичну регресію та необхідність мовних підмоделей.  \n",
    "- Кроки:\n",
    "  1. Визначте мову кожного коментаря (`langdetect`, `fasttext`) і розділіть корпус на англомовний, змішаний та інший.\n",
    "  2. Навчіть окремі логістичні моделі для кожної групи та модель з глобальними параметрами.\n",
    "  3. Порівняйте їхні метрики та проаналізуйте, чи варто будувати ensemble за мовою.\n",
    "- Підказки: збережіть мовні теги в колонці `language`; використовуйте ваги `sample_weight`, якщо групи нерівні.  \n",
    "- Перевірка: сформуйте рекомендацію щодо обробки багатомовних даних у цій лабораторній.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Пояснюваність і калібрування логістичної моделі:**  \n",
    "- Мета: забезпечити інтерпретованість рішень логістичної регресії та налаштувати ймовірності на тесті.  \n",
    "- Кроки:\n",
    "  1. Натренуйте найкращий пайплайн із попередніх експериментів та збережіть його.\n",
    "  2. Застосуйте `CalibratedClassifierCV` (Platt scaling, isotonic) і порівняйте Brier score до та після калібрування.\n",
    "  3. Використайте `eli5` або `shap` для візуалізації топ-позитивних і негативних ознак та підготуйте короткий звіт.\n",
    "- Підказки: для `shap` використовуйте `LinearExplainer`; обмежте кількість ознак до 5 тис., щоби побудова SHAP не була надто дорогою.  \n",
    "- Перевірка: додайте у висновках приклад пояснення для одного саркастичного коментаря та оцініть, наскільки модель упевнена після калібрування."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "416321f19f5a27290bc5622e8b3384b7bbbd28c6"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.2. Векторизація TF-IDF та логістична регресія</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:39:44.442068900Z",
     "start_time": "2023-11-21T14:39:44.424075300Z"
    },
    "_uuid": "3048a070a56b08eb4e5fe2c54b6d14905031e74a"
   },
   "outputs": [],
   "source": [
    "# Побудуємо біграми, обмежимо максимальну кількість ознак\n",
    "# і мінімальну частоту слів\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, min_df=2)\n",
    "# Створимо об'єкт мультиноміальної логістичної регресії,\n",
    "# яка також відома як класифікатор softmax\n",
    "logit = LogisticRegression(C=1, n_jobs=4, solver=\"lbfgs\", random_state=17, verbose=1)\n",
    "# Задамо sklearn's pipeline\n",
    "tfidf_logit_pipeline = Pipeline([(\"tf_idf\", tf_idf), (\"logit\", logit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:14.349574100Z",
     "start_time": "2023-11-21T14:39:44.500067700Z"
    },
    "_uuid": "8756bac7457218e4daf08ec276211f03971c17fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.5 s\n",
      "Wall time: 41.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tf_idf&#x27;,\n",
       "                 TfidfVectorizer(max_features=5000, min_df=2,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                (&#x27;logit&#x27;,\n",
       "                 LogisticRegression(C=1, n_jobs=4, random_state=17,\n",
       "                                    verbose=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('steps',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">steps&nbsp;</td>\n",
       "            <td class=\"value\">[(&#x27;tf_idf&#x27;, ...), (&#x27;logit&#x27;, ...)]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transform_input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transform_input&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('memory',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">memory&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"tf_idf__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">input&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;content&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('encoding',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">encoding&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;utf-8&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decode_error',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">decode_error&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;strict&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('strip_accents',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">strip_accents&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('lowercase',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">lowercase&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('preprocessor',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">preprocessor&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tokenizer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tokenizer&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('analyzer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">analyzer&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;word&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('stop_words',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">stop_words&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('token_pattern',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">token_pattern&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;(?u)\\\\b\\\\w\\\\w+\\\\b&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ngram_range',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ngram_range&nbsp;</td>\n",
       "            <td class=\"value\">(1, ...)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_df',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_df&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_df',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_df&nbsp;</td>\n",
       "            <td class=\"value\">2</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_features&nbsp;</td>\n",
       "            <td class=\"value\">5000</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('vocabulary',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">vocabulary&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('binary',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">binary&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dtype&nbsp;</td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.float64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('norm',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">norm&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('use_idf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">use_idf&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('smooth_idf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">smooth_idf&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sublinear_tf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sublinear_tf&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"logit__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">17</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">4</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tf_idf',\n",
       "                 TfidfVectorizer(max_features=5000, min_df=2,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('logit',\n",
       "                 LogisticRegression(C=1, n_jobs=4, random_state=17,\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_logit_pipeline.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:18.961782300Z",
     "start_time": "2023-11-21T14:40:14.338567600Z"
    },
    "_uuid": "d2e47f77f999c2fb5aee9ef1de1542bc93de4c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.12 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_pred = tfidf_logit_pipeline.predict(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:19.008778Z",
     "start_time": "2023-11-21T14:40:18.963793600Z"
    },
    "_uuid": "a8f93efc3db12910eaa6d7944feebb2418714203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точність за валідаційним набором даних:      0.6956\n"
     ]
    }
   ],
   "source": [
    "print(f\"Точність за валідаційним набором даних: \\\n",
    "    {accuracy_score(y_valid, valid_pred): .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:19.281356900Z",
     "start_time": "2023-11-21T14:40:18.997781500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Розбиття набору даних\n",
    "train_texts, valid_texts, y_train, y_valid = train_test_split(\n",
    "    train_df[\"comment\"], train_df[\"label\"], random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:40:19.296352100Z",
     "start_time": "2023-11-21T14:40:19.284354200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Функція для інкапсуляції виконання пайплайну\n",
    "def run_pipeline(train_texts, y_train, valid_texts):\n",
    "    # Налаштування TfidfVectorizer\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=20000, min_df=5)\n",
    "\n",
    "    # Створення Деревa Рішень з обмеженою глибиною\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=10, random_state=17)\n",
    "\n",
    "    # Налаштування пайплайну\n",
    "    tfidf_tree_pipeline = Pipeline([(\"tf_idf\", tf_idf), (\"decision_tree\", decision_tree)])\n",
    "\n",
    "    # Навчання моделі\n",
    "    tfidf_tree_pipeline.fit(train_texts, y_train)\n",
    "\n",
    "    # Виконання прогнозів\n",
    "    valid_pred = tfidf_tree_pipeline.predict(valid_texts)\n",
    "    \n",
    "    return valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:41:17.593161300Z",
     "start_time": "2023-11-21T14:40:19.311353300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         33388541 function calls (33388518 primitive calls) in 134.360 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       23    9.231    0.401  221.675    9.638 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:1962(_run_once)\n",
      "        2    0.000    0.000  128.563   64.282 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3663(run_code)\n",
      "        2    0.000    0.000  128.563   64.282 {built-in method builtins.exec}\n",
      "        1    0.003    0.003  128.546  128.546 C:\\Users\\radiu\\AppData\\Local\\Temp\\ipykernel_5432\\1472510363.py:2(run_pipeline)\n",
      "      4/1    0.011    0.003  117.143  117.143 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:1348(wrapper)\n",
      "       23    3.845    0.167  109.827    4.775 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\selectors.py:310(select)\n",
      "       23    3.934    0.171   99.810    4.340 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\selectors.py:304(_select)\n",
      "       23   56.153    2.441   79.757    3.468 {built-in method select.select}\n",
      "        1    0.000    0.000   62.480   62.480 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:554(_fit)\n",
      "        1    0.000    0.000   62.480   62.480 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\joblib\\memory.py:325(__call__)\n",
      "  1010771    3.544    0.000   31.056    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:68(_analyze)\n",
      "  1010771   10.621    0.000   17.200    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:238(_word_ngrams)\n",
      "        2    4.093    2.047   12.357    6.179 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1247(_count_vocab)\n",
      "        1    0.002    0.002    8.957    8.957 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:739(predict)\n",
      "        1    0.000    0.000    8.911    8.911 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2111(transform)\n",
      "        1    0.001    0.001    8.867    8.867 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1399(transform)\n",
      "        1    0.000    0.000    7.568    7.568 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:604(fit)\n",
      "        1    0.010    0.010    7.567    7.567 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:993(fit)\n",
      "        1    7.557    7.557    7.557    7.557 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:231(_fit)\n",
      "  1010771    7.110    0.000    7.110    0.000 {method 'findall' of 're.Pattern' objects}\n",
      "        1    1.831    1.831    6.216    6.216 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1194(_sort_features)\n",
      "        6    4.230    0.705    4.230    0.705 {built-in method builtins.sorted}\n",
      "  9110097    3.563    0.000    3.563    0.000 {method 'join' of 'str' objects}\n",
      "  1010771    3.447    0.000    3.447    0.000 {method 'extend' of 'array.array' objects}\n",
      "        1    0.000    0.000    2.964    2.964 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:1527(_fit_transform_one)\n",
      "        1    0.000    0.000    2.964    2.964 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079(fit_transform)\n",
      "        1    0.017    0.017    2.736    2.736 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1331(fit_transform)\n",
      "        1    2.027    2.027    2.719    2.719 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1208(_limit_features)\n",
      " 10120959    2.692    0.000    2.692    0.000 {method 'append' of 'list' objects}\n",
      "  1010771    1.298    0.000    1.862    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:206(decode)\n",
      "    39/38    1.719    0.044    1.720    0.045 {built-in method numpy.asarray}\n",
      "  1010773    1.431    0.000    1.431    0.000 {method 'extend' of 'list' objects}\n",
      "  1010771    0.683    0.000    1.340    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:42(_preprocess)\n",
      "2021837/2021836    0.679    0.000    0.679    0.000 {built-in method builtins.len}\n",
      "  1010771    0.657    0.000    0.657    0.000 {method 'lower' of 'str' objects}\n",
      "  1011678    0.565    0.000    0.565    0.000 {built-in method builtins.isinstance}\n",
      "  1010779    0.554    0.000    0.554    0.000 {method 'keys' of 'dict' objects}\n",
      "  1010784    0.469    0.000    0.469    0.000 {method 'values' of 'dict' objects}\n",
      "  1010772    0.438    0.000    0.438    0.000 {built-in method builtins.min}\n",
      "       10    0.000    0.000    0.377    0.038 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:734(check_array)\n",
      "        7    0.012    0.002    0.376    0.054 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:534(_ensure_sparse_format)\n",
      "        4    0.000    0.000    0.375    0.094 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1101(sort_indices)\n",
      "        2    0.364    0.182    0.364    0.182 {built-in method scipy.sparse._sparsetools.csr_sort_indices}\n",
      "        5    0.000    0.000    0.361    0.072 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:2845(validate_data)\n",
      "        1    0.000    0.000    0.357    0.357 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_index.py:29(__getitem__)\n",
      "        1    0.000    0.000    0.356    0.356 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_csr.py:264(_get_sliceXarray)\n",
      "        1    0.000    0.000    0.356    0.356 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:648(_minor_index_fancy)\n",
      "        1    0.284    0.284    0.284    0.284 {built-in method scipy.sparse._sparsetools.csr_column_index2}\n",
      "        2    0.243    0.121    0.243    0.121 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:921(_document_frequency)\n",
      "        1    0.000    0.000    0.230    0.230 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:452(asformat)\n",
      "        1    0.005    0.005    0.230    0.230 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_csr.py:63(tocsc)\n",
      "        1    0.210    0.210    0.210    0.210 {built-in method scipy.sparse._sparsetools.csr_tocsc}\n",
      "        2    0.111    0.056    0.191    0.095 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1683(transform)\n",
      "        2    0.157    0.078    0.157    0.078 {method 'take' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.135    0.135 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:492(sum)\n",
      "        1    0.006    0.006    0.134    0.134 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:1293(sum)\n",
      "        1    0.000    0.000    0.127    0.127 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:910(__rmatmul__)\n",
      "        1    0.000    0.000    0.127    0.127 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:886(_rmatmul_dispatch)\n",
      "        1    0.000    0.000    0.127    0.127 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:773(_matmul_dispatch)\n",
      "        1    0.000    0.000    0.127    0.127 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:387(_matmul_vector)\n",
      "        1    0.127    0.127    0.127    0.127 {built-in method scipy.sparse._sparsetools.csc_matvec}\n",
      "        1    0.000    0.000    0.081    0.081 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1635(fit)\n",
      "        2    0.000    0.000    0.078    0.039 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_data.py:69(astype)\n",
      "        1    0.071    0.071    0.071    0.071 {built-in method scipy.sparse._sparsetools.csr_column_index1}\n",
      "       11    0.063    0.006    0.063    0.006 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        8    0.001    0.000    0.063    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:90(_assert_all_finite)\n",
      "        7    0.000    0.000    0.061    0.009 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:2338(sum)\n",
      "        8    0.000    0.000    0.061    0.008 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:69(_wrapreduction)\n",
      "        2    0.000    0.000    0.059    0.030 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:187(wrapper)\n",
      "        2    0.040    0.020    0.058    0.029 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1902(normalize)\n",
      "       40    0.045    0.001    0.045    0.001 {built-in method numpy.array}\n",
      "        9    0.000    0.000    0.044    0.005 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:30(__init__)\n",
      "        1    0.016    0.016    0.043    0.043 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:506(predict)\n",
      "       14    0.042    0.003    0.042    0.003 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.036    0.018 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_data.py:32(_deduped_data)\n",
      "        2    0.000    0.000    0.036    0.018 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1054(sum_duplicates)\n",
      "        2    0.000    0.000    0.036    0.018 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1025(has_canonical_format)\n",
      "        2    0.036    0.018    0.036    0.018 {built-in method scipy.sparse._sparsetools.csr_has_canonical_format}\n",
      "        2    0.003    0.001    0.035    0.018 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_arraysetops_impl.py:145(unique)\n",
      "        2    0.014    0.007    0.033    0.016 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_arraysetops_impl.py:348(_unique1d)\n",
      "        5    0.000    0.000    0.024    0.005 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:51(_wrapfunc)\n",
      "        1    0.000    0.000    0.020    0.020 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:482(_validate_X_predict)\n",
      "        2    0.000    0.000    0.019    0.009 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:2879(cumsum)\n",
      "        2    0.019    0.009    0.019    0.009 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "        2    0.005    0.003    0.016    0.008 {method '__exit__' of 'sqlite3.Connection' objects}\n",
      "        4    0.000    0.000    0.014    0.003 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1166(_with_data)\n",
      "        4    0.013    0.003    0.013    0.003 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        4    0.000    0.000    0.011    0.003 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1069(has_sorted_indices)\n",
      "        3    0.011    0.004    0.011    0.004 {built-in method scipy.sparse._sparsetools.csr_has_sorted_indices}\n",
      "        1    0.000    0.000    0.008    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:201(check_classification_targets)\n",
      "        1    0.000    0.000    0.008    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:228(type_of_target)\n",
      "        3    0.008    0.003    0.008    0.003 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.008    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_unique.py:81(cached_unique)\n",
      "        2    0.000    0.000    0.008    0.004 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_unique.py:105(<genexpr>)\n",
      "        1    0.000    0.000    0.008    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_unique.py:62(_cached_unique)\n",
      "        1    0.000    0.000    0.008    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\externals\\array_api_compat\\_internal.py:32(wrapped_f)\n",
      "        1    0.000    0.000    0.008    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\externals\\array_api_compat\\common\\_aliases.py:253(unique_values)\n",
      "        1    0.000    0.000    0.008    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:191(_compute_missing_values_in_feature_mask)\n",
      "        1    0.000    0.000    0.008    0.008 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:172(assert_all_finite)\n",
      "        1    0.006    0.006    0.006    0.006 {built-in method numpy._core._multiarray_umath._unique_hash}\n",
      "        1    0.000    0.000    0.005    0.005 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:1250(argmax)\n",
      "        1    0.005    0.005    0.005    0.005 {method 'argmax' of 'numpy.ndarray' objects}\n",
      "        4    0.000    0.000    0.003    0.001 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:463(_validate_params)\n",
      "    19/16    0.000    0.000    0.003    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_tags.py:298(get_tags)\n",
      "        2    0.003    0.001    0.003    0.001 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "        5    0.002    0.000    0.002    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1726(__sklearn_tags__)\n",
      "        1    0.000    0.000    0.002    0.002 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:916(copy)\n",
      "        1    0.002    0.002    0.002    0.002 {built-in method numpy.ascontiguousarray}\n",
      "        8    0.000    0.000    0.002    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:3373(signature)\n",
      "        8    0.000    0.000    0.002    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:3094(from_callable)\n",
      "        8    0.000    0.000    0.002    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2487(_signature_from_callable)\n",
      "       24    0.000    0.000    0.002    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\events.py:87(_run)\n",
      "        1    0.000    0.000    0.002    0.002 {method 'sum' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.002    0.002 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\_methods.py:49(_sum)\n",
      "        6    0.000    0.000    0.002    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:28(validate_parameter_constraints)\n",
      "        5    0.000    0.000    0.002    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:233(get_params)\n",
      "       24    0.000    0.000    0.002    0.000 {method 'run' of '_contextvars.Context' objects}\n",
      "        2    0.002    0.001    0.002    0.001 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numeric.py:171(ones)\n",
      "        5    0.000    0.000    0.002    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:202(_get_param_names)\n",
      "        8    0.001    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2383(_signature_from_function)\n",
      "       12    0.000    0.000    0.001    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\iostream.py:118(_run_event_pipe_gc)\n",
      "        1    0.001    0.001    0.001    0.001 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numeric.py:324(full)\n",
      "    86/84    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:104(make_constraint)\n",
      "       31    0.000    0.000    0.001    0.000 {built-in method builtins.next}\n",
      "      7/6    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:1670(check_is_fitted)\n",
      "       12    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\contextlib.py:145(__exit__)\n",
      "       11    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:1532(_get_index_dtype)\n",
      "       24    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\tasks.py:703(sleep)\n",
      "       11    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:264(get_index_dtype)\n",
      "        2    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:281(get_params)\n",
      "        2    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py:29(_get_params)\n",
      "      198    0.000    0.000    0.001    0.000 <frozen abc>:117(__instancecheck__)\n",
      "       68    0.000    0.000    0.001    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2754(__init__)\n",
      "        1    0.000    0.000    0.001    0.001 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_index.py:212(_validate_indices)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\_config.py:233(config_context)\n",
      "      198    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:43(_raise_or_warn_if_not_fitted)\n",
      "      271    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "       52    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\_lib\\_sparse.py:10(issparse)\n",
      "        9    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:166(check_format)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
      "       12    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\iostream.py:127(_event_pipe_gc)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:781(call_later)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\futures.py:310(_set_result_unless_cancelled)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:1208(__sklearn_tags__)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\codeop.py:113(__call__)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\_config.py:63(set_config)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:1129(argsort)\n",
      "       19    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:454(__sklearn_tags__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:1488(check_random_state)\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:381(_num_samples)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:322(_validate_steps)\n",
      "        9    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numerictypes.py:385(isdtype)\n",
      "       49    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\_config.py:35(get_config)\n",
      "      7/6    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:1632(_is_fitted)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py:81(_validate_names)\n",
      "       12    0.000    0.000    0.000    0.000 {method 'set_result' of '_asyncio.Future' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1097(__sklearn_tags__)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:805(call_at)\n",
      "        9    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:444(check_shape)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:834(__sklearn_tags__)\n",
      "        9    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1109(prune)\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:3042(__init__)\n",
      "       14    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:502(is_satisfied_by)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:823(call_soon)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:515(__sklearn_tags__)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\ipkernel.py:781(_clean_thread_parent_frames)\n",
      "       61    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\_config.py:27(_get_threadlocal_config)\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\externals\\array_api_compat\\numpy\\_aliases.py:89(asarray)\n",
      "       24    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\getlimits.py:698(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:708(__set__)\n",
      "       13    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:589(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_index.py:313(_asindices)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:3631(set)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:852(_call_soon)\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\contextlib.py:303(helper)\n",
      "        4    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:1176(is_classifier)\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\contextlib.py:136(__enter__)\n",
      "      128    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "       68    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\enum.py:695(__call__)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\events.py:113(__init__)\n",
      "       36    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\threading.py:1136(is_alive)\n",
      "       24    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\events.py:36(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:689(set)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\events.py:157(cancel)\n",
      "     13/3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\copy.py:119(deepcopy)\n",
      "       19    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:645(parent)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\_ufunc_config.py:470(inner)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_csr.py:22(transpose)\n",
      "       48    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:772(time)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:693(__sklearn_tags__)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:736(_asarray_with_order)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:486(__contains__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
      "       26    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:387(get_namespace)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\externals\\array_api_compat\\common\\_aliases.py:199(_unique_kwargs)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:415(build_analyzer)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\_methods.py:41(_amax)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method numpy.zeros}\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:764(unwrap)\n",
      "        9    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_data.py:21(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\base.py:1159(__sklearn_tags__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Local\\Temp\\ipykernel_5432\\2777130815.py:1(<module>)\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\contextlib.py:108(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:3290(bind)\n",
      "       13    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:596(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:129(is_multilabel)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:3148(_bind)\n",
      "      183    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_matrix.py:18(_csc_container)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:718(_validate)\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\_ufunc_config.py:446(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\copy.py:248(_reconstruct)\n",
      "       23    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "       33    0.000    0.000    0.000    0.000 <frozen abc>:121(__subclasscheck__)\n",
      "       39    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:296(__init__)\n",
      "        9    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numerictypes.py:372(_preprocess_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:414(check_memory)\n",
      "        9    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:138(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method numpy.frombuffer}\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\_ufunc_config.py:462(__exit__)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:729(_is_extension_array_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\base.py:840(__iter__)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:2697(_check_feature_names)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\joblib\\memory.py:1009(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:176(get_annotations)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:457(create_future)\n",
      "       39    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:365(nnz)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:254(_ascontainer)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\threading.py:1479(enumerate)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:350(isintlike)\n",
      "       76    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:3089(<genexpr>)\n",
      "       13    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:2344(_is_pandas_df)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\generic.py:6306(__getattr__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:446(_check_method_params)\n",
      "       25    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:94(ndim)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2132(__sklearn_tags__)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:2790(_check_n_features)\n",
      "       33    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "       68    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\enum.py:1156(__new__)\n",
      "        7    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:302(isclass)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:610(_prune_tree)\n",
      "       49    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "       19    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:318(_use_interchange_protocol)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:676(__get__)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:599(asmatrix)\n",
      "       27    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:472(<genexpr>)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:2380(_get_feature_names)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:406(validateaxis)\n",
      "        6    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:112(getdtype)\n",
      "        9    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:372(isshape)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numeric.py:401(full_like)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\joblib\\memory.py:1044(cache)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:530(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_available_if.py:39(__get__)\n",
      "       80    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:264(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:3052(max)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:328(_num_features)\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method time.monotonic}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:3474(validate)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5449(_can_hold_identifiers_and_holds_name)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\copy.py:201(_deepcopy_tuple)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1479(__sklearn_tags__)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:313(_is_arraylike_not_scalar)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:690(_pandas_dtype_needs_early_conversion)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\warnings.py:170(simplefilter)\n",
      "       15    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:368(is_satisfied_by)\n",
      "       68    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}\n",
      "       68    0.000    0.000    0.000    0.000 {method '__contains__' of 'frozenset' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:1286(__sklearn_is_fitted__)\n",
      "       35    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:346(build_tokenizer)\n",
      "       94    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "       13    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:597(<genexpr>)\n",
      "      179    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2807(name)\n",
      "        7    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\series.py:834(_values)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:671(_ensure_no_complex_data)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:1512(_notify_trait)\n",
      "       33    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:437(iscoroutinefunction)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\externals\\array_api_compat\\common\\_helpers.py:660(_check_device)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2962(apply_defaults)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1486(_make_int_array)\n",
      "       39    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:118(_getnnz)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_metadata_requests.py:140(_routing_enabled)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:305(_is_arraylike)\n",
      "       18    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\_lib\\_util.py:141(_prune_array)\n",
      "       86    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_matrix.py:65(get_shape)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_available_if.py:27(_check)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1085(has_sorted_indices)\n",
      "       17    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
      "       70    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_data.py:24(dtype)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\warnings.py:188(_add_filter)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method numpy._core._multiarray_umath._make_extobj}\n",
      "        7    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2023(internal_values)\n",
      "       25    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:345(isscalarlike)\n",
      "       16    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\threading.py:1112(ident)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:629(get)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'reset' of '_contextvars.ContextVar' objects}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:1523(notify_change)\n",
      "        4    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:359(_iter)\n",
      "       36    0.000    0.000    0.000    0.000 {method 'is_done' of '_thread._ThreadHandle' objects}\n",
      "       32    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\re\\__init__.py:287(compile)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'set' of '_contextvars.ContextVar' objects}\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\events.py:73(cancel)\n",
      "       22    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:300(is_satisfied_by)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:36(__getitem__)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.vars}\n",
      "      130    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2819(kind)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1952(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\selector_events.py:740(_process_events)\n",
      "      8/7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\matrixlib\\defmatrix.py:174(__array_finalize__)\n",
      "       21    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:98(_shape_as_2d)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:399(_has_code_flag)\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method _heapq.heappop}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:1527(_notify_observers)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:25(upcast)\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\enum.py:199(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:698(is_satisfied_by)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:727(_cross_validate)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:960(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'min' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\copy.py:218(_deepcopy_dict)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_arraysetops_impl.py:131(_unpack_tuple)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:538(is_string_dtype)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "       17    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:386(isfunction)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\re\\__init__.py:330(_compile)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:564(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numeric.py:1964(isscalar)\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\series.py:710(dtype)\n",
      "       11    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\getlimits.py:709(min)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\_methods.py:45(_amin)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_bunch.py:30(__init__)\n",
      "       24    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:3624(validate_elements)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:376(_check_stop_words_consistency)\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method math.isnan}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1385(_is_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\accessor.py:220(__get__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:75(check)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_index.py:423(_compatible_boolean_index)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\warnings.py:488(__enter__)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:184(_support_missing_values)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\compilerop.py:180(extra_flags)\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method _heapq.heappush}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\tree\\_classes.py:127(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:3523(ndim)\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2012(dtype)\n",
      "       48    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:2060(get_debug)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\random.py:893(getrandbits)\n",
      "       13    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\getlimits.py:722(max)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numerictypes.py:475(issubdtype)\n",
      "       23    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\history.py:1016(_writeout_output_cache)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1200(is_bool_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:574(condition)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:366(get_stop_words)\n",
      "       13    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:385(format)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method _contextvars.copy_context}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1155(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:211(reshape)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:139(is_object_dtype)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:681(_check_estimator_name)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\arrays\\sparse\\accessor.py:29(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:572(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\joblib\\memory.py:109(_store_backend_factory)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:1223(<genexpr>)\n",
      "       12    0.000    0.000    0.000    0.000 {method 'cancelled' of '_asyncio.Future' objects}\n",
      "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1390(_handle_fromlist)\n",
      "       24    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "       24    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:554(_check_closed)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:438(__init__)\n",
      "       36    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\threading.py:605(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:1904(ravel)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:515(_warn_for_unused_params)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\arrays\\sparse\\accessor.py:53(_validate)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:319(build_preprocessor)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numerictypes.py:293(issubclass_)\n",
      "       14    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\copy.py:232(_keep_alive)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:2304(validate)\n",
      "        1    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\functools.py:440(_unwrap_partialmethod)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\asyncio\\base_events.py:1957(_timer_handle_cancelled)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:190(_check_stop_list)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_tags.py:133(<lambda>)\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method _asyncio.get_running_loop}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1048(has_canonical_format)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\series.py:981(__array__)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:573(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:293(supported_float_dtypes)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\warnings.py:509(__exit__)\n",
      "       19    0.000    0.000    0.000    0.000 <string>:2(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:422(_has_coroutine_mark)\n",
      "       36    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\multiarray.py:588(can_cast)\n",
      "       19    0.000    0.000    0.000    0.000 {built-in method _operator.index}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\copyreg.py:98(__newobj__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
      "       27    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:477(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:447(_check_params)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\enum.py:1337(value)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\threading.py:303(__enter__)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\traitlets\\traitlets.py:3486(validate_elements)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:336(get_sum_dtype)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method _operator.lt}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\copy.py:253(<genexpr>)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:129(_deprecate_force_all_finite)\n",
      "       16    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\typing.py:2371(cast)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method nt.urandom}\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\threading.py:318(_is_owned)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_base.py:1587(isspmatrix)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:466(_validate_vocabulary)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:496(_check_vocabulary)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:427(_final_estimator)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:248(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\warnings.py:462(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:699(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1444(_is_dtype_type)\n",
      "        6    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:127(_check_array_api_dispatch)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\threading.py:312(_release_save)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:531(is_string_or_object_np_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:540(is_satisfied_by)\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\_ufunc_config.py:436(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'indices' of 'slice' objects}\n",
      "        5    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:231(_is_numpy_namespace)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\shape_base.py:21(atleast_1d)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3615(compare)\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\functools.py:435(_unwrap_partial)\n",
      "        4    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\multiarray.py:1106(copyto)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:2875(_cumsum_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\ma\\core.py:6879(is_masked)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\joblib\\logger.py:67(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'intersection' of 'set' objects}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:550(is_satisfied_by)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1409(_get_dtype)\n",
      "       11    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:310(is_satisfied_by)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:506(_validate_ngram_range)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\generic.py:42(_instancecheck)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\generic.py:670(_info_axis)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:608(_major_slice)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:306(ismethod)\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method _operator.ge}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:1125(_argsort_dispatcher)\n",
      "        7    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\validation.py:1179(_check_large_sparse)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:402(isdense)\n",
      "       13    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_csr.py:127(_swap)\n",
      "        3    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:3519(_ndim_dispatcher)\n",
      "        8    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_csc.py:145(_swap)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\series.py:723(dtypes)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:1299(user_global_ns)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1279(is_extension_array_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\indexing.py:162(iloc)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:3102(parameters)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\ma\\core.py:1411(getmask)\n",
      "        3    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1959(_block)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:59(upcast_char)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2524(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\_config\\__init__.py:34(using_copy_on_write)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\generic.py:37(_check)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2042(_check_params)\n",
      "       10    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\copy.py:173(_deepcopy_atomic)\n",
      "        7    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:2333(_sum_dispatcher)\n",
      "       12    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2811(default)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\multiarray.py:391(where)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\multiarray.py:115(empty_like)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:744(dtype)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _operator.gt}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'items' of 'mappingproxy' objects}\n",
      "        4    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\_user_interface.py:35(_print_elapsed_time)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\utils\\fixes.py:239(_preserve_dia_indices_dtype)\n",
      "        3    0.000    0.000    0.000    0.000 {function Bunch.__getitem__ at 0x000001D57218A7A0}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1629(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\numeric.py:394(_full_like_dispatcher)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:123(classes)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\joblib\\memory.py:322(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:125(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:3047(_max_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'wrap' of 'numpy._core._multiarray_umath._array_converter' objects}\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:685(user_ns)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:475(_get_metadata_for_step)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:376(__len__)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\sklearn\\pipeline.py:439(_log_message)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x00007FF8F31F5050}\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\history.py:1065(hold)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\multiarray.py:927(bincount)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\shape_base.py:17(_atleast_1d_dispatcher)\n",
      "        2    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_arraysetops_impl.py:139(_unique_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:981(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\site-packages\\scipy\\sparse\\_csc.py:36(tocsc)\n",
      "        2    0.000    0.000    0.000    0.000 c:\\Users\\radiu\\miniconda3\\envs\\courses\\Lib\\inspect.py:2901(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\lib\\_function_base_impl.py:912(_copy_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\multiarray.py:188(concatenate)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:206(_reshape_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:1246(_argmax_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:1900(_ravel_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 C:\\Users\\radiu\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\_core\\multiarray.py:698(result_type)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1d50429dd30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Профілювання виконання пайплайну\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "# Виконання пайплайну\n",
    "valid_pred = run_pipeline(train_texts, y_train, valid_texts)\n",
    "profiler.disable()\n",
    "\n",
    "# Виведення результатів профілювання\n",
    "stats = pstats.Stats(profiler).sort_stats('cumtime')\n",
    "stats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:42:13.060465400Z",
     "start_time": "2023-11-21T14:42:13.003466500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5876696228229512"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result = accuracy_score(y_valid, valid_pred)\n",
    "\n",
    "my_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 2</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Триграмний TF-IDF із регуляризаційним скануванням:**  \n",
    "- Мета: з’ясувати, як чисті триграми впливають на логістичну регресію та які значення коефіцієнта `C` забезпечують оптимальний баланс між якістю та узагальненням.  \n",
    "- Кроки:\n",
    "  1. Завантажте `train-balanced-sarcasm.csv`, сформуйте вихідні тексти з колонок `comment` та ціль `label`, очистіть HTML і спецсимволи.\n",
    "  2. Побудуйте `Pipeline` із `TfidfVectorizer(ngram_range=(3,3), min_df=5, max_features=60000)` і `LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)`.\n",
    "  3. За допомогою `GridSearchCV` перевірте `C` у діапазоні [0.01, 0.1, 1, 5, 10], оцінюйте F1 у 5-кратній `StratifiedKFold` валідації, проаналізуйте, як триграми змінюють результати проти уніграм і біграм.\n",
    "- Підказки: використовуйте `joblib.Parallel` або `n_jobs=-1`, щоб пришвидшити пошук; для контролю витрат пам’яті застосуйте `dtype=np.float32` у TF-IDF.  \n",
    "- Перевірка: зведіть метрики в таблицю, наведіть приклади топових триграм із найвищими коефіцієнтами й сформулюйте висновок щодо доцільності чистих триграм.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – Комбіновані словні n-грами та регуляризація L1:**  \n",
    "- Мета: дослідити, як поєднання уні-, бі- та триграм із L1-регуляризацією впливає на розрідженнякоефіцієнтів логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Підготуйте тексти `comment`, обмежте вибірку до англомовних рядків із допомогою `langdetect` (за потреби).\n",
    "  2. Побудуйте `TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9, sublinear_tf=True)`; навчіть `LogisticRegression(penalty='l1', solver='saga', C=1, max_iter=500)`.\n",
    "  3. Порівняйте щільність коефіцієнтів (частка нульових ваг) з моделлю L2, дослідіть вплив L1 на точність і стабільність.\n",
    "- Підказки: використовуйте `sklearn.metrics.classification_report` для оцінки класу сарказму; зафіксуйте час навчання.  \n",
    "- Перевірка: підготуйте графік порівняння щільності коефіцієнтів L1/L2 та коротко поясніть, у яких умовах L1 виправданий.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Символьні n-грами та словникові ознаки:**  \n",
    "- Мета: оцінити, як символьні шінгли доповнюють словні n-грами в логістичній регресії.  \n",
    "- Кроки:\n",
    "  1. Створіть два векторизатори: `TfidfVectorizer(ngram_range=(1,2), analyzer='word')` і `TfidfVectorizer(ngram_range=(3,5), analyzer='char', min_df=10)`.\n",
    "  2. Об’єднайте їх у `FeatureUnion`, додайте до пайплайна `LogisticRegression(penalty='l2', solver='liblinear', C=0.5)`.\n",
    "  3. Проведіть 5-кратну стратифіковану валідацію, окремо зафіксуйте внесок символьних ознак у покращення F1 та log-loss.\n",
    "- Підказки: використовуйте `TruncatedSVD` на символьних ознаках для швидкого огляду латентних патернів; обмежте пам’ять через `max_features`.  \n",
    "- Перевірка: підготуйте порівняльну таблицю метрик для чисто словної, чисто символьної та комбінованої моделей.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – Контекст батьківського коментаря в пайплайні:**  \n",
    "- Мета: інтегрувати `parent_comment` у логістичну регресію та перевірити, як контраст між репліками впливає на прогноз.  \n",
    "- Кроки:\n",
    "  1. Завантажте `train-balanced-sarcasm.csv`, залиште пари (`comment`, `parent_comment`), заповніть пропуски.\n",
    "  2. Побудуйте `ColumnTransformer`, що містить окремі TF-IDF для `comment` та `parent_comment`, а також `FunctionTransformer`, який обчислює різницю середніх TF-IDF.\n",
    "  3. Навчіть `LogisticRegression` (solver='saga', penalty='l2', C=2) у 5-кратній валідації, дослідіть приклади з найбільшим приростом log-odds від контексту.\n",
    "- Підказки: нормуйте різницю векторів через `StandardScaler`; логуванням збережіть топові пари коментарів.  \n",
    "- Перевірка: опишіть 5 кейсів, у яких контекст батьківського коментаря змінив класифікацію, та наведіть метрики.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Генералізація на часових сесіях:**  \n",
    "- Мета: перевірити, чи стабільна логістична регресія при розбитті `train-balanced-sarcasm.csv` на часові відрізки.  \n",
    "- Кроки:\n",
    "  1. Визначте часовий індикатор (наприклад, `created_utc`), відсортуйте дані та сформуйте `TimeSeriesSplit` на 5 фолдів без перемішування.\n",
    "  2. Використайте пайплайн TF-IDF (1–2-грам) + `LogisticRegression(C=1.5, penalty='l2')`.\n",
    "  3. Оцініть різницю метрик між останнім фолдом (майбутнє) і попередніми, порівняйте з випадковим розбиттям.\n",
    "- Підказки: застосуйте `min_df=4`, аби уникнути надто рідкісних n-грам; контролюйте баланс класів у кожному фолді.  \n",
    "- Перевірка: оформіть звіт із таблицею метрик по фолдах та висновком про часове дрейфування.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Калібрування логістичної регресії:**  \n",
    "- Мета: оцінити якість ймовірнісних прогнозів логістичної моделі та покращити калібрування.  \n",
    "- Кроки:\n",
    "  1. Побудуйте пайплайн TF-IDF (1–2-грам) + `LogisticRegression(C=1, solver='lbfgs')`, тренуйте на 80% вибірки.\n",
    "  2. Застосуйте `CalibratedClassifierCV` з методами Platt та isotonic на валідаційних даних.\n",
    "  3. Порівняйте Brier score, reliability curve та вплив на precision-recall при різних порогах.\n",
    "- Підказки: використовуйте `sklearn.calibration.calibration_curve`; для стабільності задайте `cv=3`.  \n",
    "- Перевірка: підготуйте невеликий звіт із графіками калібрування і рекомендаціями щодо вибору методу.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Тональна маска та TF-IDF:**  \n",
    "- Мета: поєднати TF-IDF ознаки з тональністю коментарів для підсилення логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Обчисліть `VADER` або `TextBlob` полярність для `comment`, додайте її як додаткову числову ознаку.\n",
    "  2. Побудуйте пайплайн `ColumnTransformer`, що об’єднує TF-IDF (1–2-грам) та тональність, застосуйте `StandardScaler` до числових ознак.\n",
    "  3. Навчіть `LogisticRegression(C=0.8, solver='saga')`, проаналізуйте, як тональність змінює рішення на прикладах позитивного та негативного сарказму.\n",
    "- Підказки: обмежте словник TF-IDF до 40 тис. ознак; використовуйте `FunctionTransformer`, щоб централізувати тональність.  \n",
    "- Перевірка: наведіть 3 кейси, де тональність суттєво змінила log-odds, та опишіть спостереження.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Крос-валідація зі стратифікованими батчами:**  \n",
    "- Мета: налаштувати процес крос-валідації для великих даних, розпаралеливши обчислення та контролюючи баланс класів у батчах.  \n",
    "- Кроки:\n",
    "  1. Розбийте датасет на батчі по 50 тис. рядків, у кожному батчі виконуйте навчання TF-IDF + `LogisticRegression` з однаковими гіперпараметрами.\n",
    "  2. Використайте `StratifiedKFold` поверх батчів, об’єднайте результати через вагове середнє.\n",
    "  3. Порівняйте метрики з повноцінним навчанням на всьому наборі, оцініть компроміс швидкість/якість.\n",
    "- Підказки: застосовуйте `partial_fit` для квазіонлайнового навчання (solver='saga'); логуванням фіксуйте час і пам'ять.  \n",
    "- Перевірка: створіть графік «швидкість навчання vs F1», надайте рекомендації щодо оптимального розміру батча.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Вимірювання стабільності ознак:**  \n",
    "- Мета: з’ясувати, наскільки стабільні топові TF-IDF ознаки логістичної регресії при різних випадкових розбиттях.  \n",
    "- Кроки:\n",
    "  1. Проведіть 10 повторів `StratifiedShuffleSplit` (train 80%, test 20%), щоразу тренуйте TF-IDF (1–2-грам) + `LogisticRegression(C=1)`.\n",
    "  2. Для кожного повтору збережіть топ-30 позитивних і негативних коефіцієнтів.\n",
    "  3. Обчисліть коефіцієнт Жаккара між списками топ-ознак та побудуйте heatmap стабільності.\n",
    "- Підказки: використовуйте `pandas.DataFrame` для агрегації ознак; кешуйте матриці TF-IDF, щоб не перераховувати.  \n",
    "- Перевірка: підготуйте висновок про стабільність важливих ознак і вплив випадкових розбиттів на інтерпретованість.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – Лінійний стекінг із базовими моделями:**  \n",
    "- Мета: оцінити, чи покращує логістичну регресію стекінг із іншими лінійними моделями (наприклад, Ridge, SGD).  \n",
    "- Кроки:\n",
    "  1. Навчіть окремі базові моделі: TF-IDF + `RidgeClassifier`, TF-IDF + `SGDClassifier(loss='log_loss')`.\n",
    "  2. Використайте `StackingClassifier`, де мета-моделлю виступає `LogisticRegression`, з 5-кратною крос-валідацією.\n",
    "  3. Порівняйте метрики стекінгу з одиночним пайплайном логістичної регресії.\n",
    "- Підказки: зменшіть розмір словника до 30 тис. ознак, щоб стекінг був обчислюваним; використовуйте `passthrough=True`, аби логістична модель бачила TF-IDF напряму.  \n",
    "- Перевірка: оформіть таблицю з метриками, дайте висновок про доцільність стекінгу й витрати часу.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – Вагові схеми TF-IDF і log-count ratio:**  \n",
    "- Мета: порівняти класичний TF-IDF і log-count ratio (LCR) як вагові схеми перед логістичною регресією.  \n",
    "- Кроки:\n",
    "  1. Підготуйте TF-IDF матрицю (1–2-грам), окремо обчисліть LCR (за підходом NB-SVM) для тих самих ознак.\n",
    "  2. Навчіть дві логістичні регресії (solver='liblinear', C=1): одну з TF-IDF, іншу з LCR трансформованими ознаками.\n",
    "  3. Порівняйте точність, F1 і швидкість, проаналізуйте різницю у топових коефіцієнтах.\n",
    "- Підказки: для LCR використовуйте `np.log((p+alpha)/(q+alpha))`; налаштуйте `alpha=1`.  \n",
    "- Перевірка: підготуйте короткий технічний звіт із формулами та прикладами ознак, які змінили знаки ваг.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Регуляризація Elastic-Net:**  \n",
    "- Мета: дослідити, як поєднання L1 та L2 у логістичній регресії впливає на відбір ознак у `train-balanced-sarcasm.csv`.  \n",
    "- Кроки:\n",
    "  1. Побудуйте TF-IDF (1–2-грам, min_df=4), масштабування не потрібне.\n",
    "  2. Навчіть `LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio` від 0.1 до 0.9), використавши `GridSearchCV`.\n",
    "  3. Проаналізуйте, як змінюється кількість ненульових коефіцієнтів і метрики при різних `l1_ratio`.\n",
    "- Підказки: задайте `max_iter=1000`; використовуйте `n_jobs=-1` для паралелізації.  \n",
    "- Перевірка: побудуйте графік залежності F1 та кількості ознак від `l1_ratio`, поясніть оптимальну точку.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – Тональні та пунктуаційні взаємодії:**  \n",
    "- Мета: перевірити, чи взаємодія пунктуації та тональності підвищує точність логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Додайте ознаки частоти `!`, `?`, `...` до TF-IDF, нормуйте на довжину тексту.\n",
    "  2. Обчисліть тональність (VADER) і створіть взаємодійні ознаки «пунктуація × тональність» через `PolynomialFeatures(include_bias=False)`.\n",
    "  3. Навчіть `LogisticRegression(C=1, solver='lbfgs')`, порівняйте з моделлю без взаємодій.\n",
    "- Підказки: використовуйте `ColumnTransformer`, щоб поєднати розріджені й щільні ознаки; стандартизуйте взаємодії.  \n",
    "- Перевірка: проаналізуйте зміну log-loss і наведіть приклади коментарів, де взаємодія вплинула на рішення.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – Аналіз помилок із SHAP-значеннями:**  \n",
    "- Мета: підвищити пояснюваність логістичної регресії через аналіз помилок за допомогою SHAP.  \n",
    "- Кроки:\n",
    "  1. Натренуйте базовий пайплайн TF-IDF (1–2-грам) + `LogisticRegression(C=1)`.\n",
    "  2. Виділіть 100 найбільш упевнених помилок (false positives/negatives).\n",
    "  3. Застосуйте `shap.LinearExplainer`, візуалізуйте top-п’ять ознак, що призвели до помилки, та сформуйте рекомендації щодо обробки.\n",
    "- Підказки: обмежте словник TF-IDF до 25 тис. ознак, щоб SHAP не був занадто важким; використовуйте `shap.summary_plot`.  \n",
    "- Перевірка: підготуйте текстовий звіт із прикладами помилок і гіпотезами щодо покращення моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Перенавчання та регуляризація по фолдах:**  \n",
    "- Мета: дослідити, як логістична регресія схильна до перенавчання на різних величинах регуляризації.  \n",
    "- Кроки:\n",
    "  1. Розбийте дані на train/val/test (70/15/15) з фіксованим `random_state`.\n",
    "  2. Навчіть моделі з `C` у діапазоні [0.01, 0.1, 1, 10, 100], фіксуючи решту параметрів.\n",
    "  3. Для кожної моделі обчисліть train та val F1, побудуйте графік залежності й прокоментуйте перенавчання.\n",
    "- Підказки: використовуйте `sklearn.metrics.f1_score`; зберігайте проміжні метрики у `DataFrame`.  \n",
    "- Перевірка: сформулюйте висновок про оптимальний `C` і наведіть приклади, де занадто велике `C` шкодить.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Адаптивний RandomizedSearchCV:**  \n",
    "- Мета: налаштувати гіперпараметри логістичної регресії за допомогою випадкового пошуку з адаптивним розширенням діапазонів.  \n",
    "- Кроки:\n",
    "  1. Визначте стартовий діапазон для `C` (10^-3 … 10^2) та `penalty` (L1, L2, elasticnet з `l1_ratio`).\n",
    "  2. Запустіть `RandomizedSearchCV` із `n_iter=50`, проаналізуйте найкращі конфігурації.\n",
    "  3. Розширте пошук навколо топ-3 конфігурацій (ще 30 ітерацій), порівняйте з базовим `GridSearch`.\n",
    "- Підказки: зберігайте історію пошуку для подальшого аналізу; використовуйте `scoring='f1'`.  \n",
    "- Перевірка: підготуйте графік розподілу F1 для різних ітерацій і висновок щодо ефективності RandomizedSearch.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Інтерпретація коефіцієнтів через тематичні групи:**  \n",
    "- Мета: згрупувати найвпливовіші ознаки логістичної регресії за семантичними категоріями.  \n",
    "- Кроки:\n",
    "  1. Натренуйте TF-IDF + `LogisticRegression`, витягніть топ-100 позитивних і негативних ваг.\n",
    "  2. Здійсніть кластеризацію цих ознак через `KMeans` або тематичні словники (наприклад, емоції, політика, побут).\n",
    "  3. Підготуйте аналітичний звіт, у якому порівняйте вплив різних тематичних груп.\n",
    "- Підказки: використовуйте `nltk.corpus.wordnet` або ручні словники; збережіть результати як Markdown-таблицю.  \n",
    "- Перевірка: додайте інтерпретації до кожної групи та зробіть висновок про ключові теми сарказму.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Зважування прикладів за довірою:**  \n",
    "- Мета: перевірити, чи впливає вагове коригування прикладів (наприклад, за довжиною або емоційним тоном) на логістичну регресію.  \n",
    "- Кроки:\n",
    "  1. Обчисліть для кожного `comment` вагу (наприклад, `1 + 0.2 * кількість знаків оклику`) або на основі довжини.\n",
    "  2. Навчіть TF-IDF + `LogisticRegression` із параметром `sample_weight`.\n",
    "  3. Порівняйте метрики зі звичайним навчанням без ваг, проаналізуйте зміни в precision/recall.\n",
    "- Підказки: зберігайте ваги у зручному форматі (наприклад, `Series`); контрольно перевірте, щоб ваги не були надто великими.  \n",
    "- Перевірка: наведіть графік впливу ваг на F1 та коротко поясніть, чи варто застосовувати такий підхід.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – Порівняння з байєсівським логістичним підходом:**  \n",
    "- Мета: зіставити класичну логістичну регресію з байєсівською версією (наприклад, через `sklearn.linear_model.BayesianRidge` наближено або PyMC).  \n",
    "- Кроки:\n",
    "  1. Навчіть стандартну логістичну регресію на TF-IDF (1–2-грам).\n",
    "  2. Застосуйте байєсівський підхід (наприклад, `pymc` для моделювання коефіцієнтів із нормальними пріорами) на підмножині ознак (топ-500).\n",
    "  3. Порівняйте розподіли коефіцієнтів і якість класифікації.\n",
    "- Підказки: використовуйте NUTS або ADVI для байєсівської моделі; забезпечте відтворюваність через `random_seed`.  \n",
    "- Перевірка: підготуйте графіки постеріорних розподілів і таблицю метрик, поясніть різницю підходів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Аудит продуктивності після впровадження:**  \n",
    "- Мета: змоделювати процес аудиту логістичної регресії після умовного «розгортання» на нових даних.  \n",
    "- Кроки:\n",
    "  1. Розділіть дані на «історичні» (80%) та «нові» (20%), навчіть модель на історичних.\n",
    "  2. Застосуйте модель до «нових» даних, обчисліть метрики, побудуйте `Population Stability Index` (PSI) для оцінки дрейфу ознак.\n",
    "  3. Сформуйте рекомендації щодо ретренінгу або оновлення гіперпараметрів.\n",
    "- Підказки: використовуйте `pandas.qcut` для бінінгу PSI; задокументуйте кроки аудиту у Markdown.  \n",
    "- Перевірка: надайте висновки про стабільність моделі та поріг, при якому слід запускати ретренінг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fefd0178f43ce832031653be70f0a0e47f62cf4c"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.3. Інтерпретація та порівняння моделей</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:41:17.605160200Z",
     "start_time": "2023-11-21T14:41:17.590161300Z"
    },
    "_uuid": "247a13fd3ae4d5c015c0ca0489a9a95d72ad7e9f"
   },
   "outputs": [],
   "source": [
    "# Довідково: Нижче подамо матрицю невідповідностей\n",
    "def plot_confusion_matrix(\n",
    "    actual,\n",
    "    predicted,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=\"Confusion matrix\",\n",
    "    figsize=(7, 7),\n",
    "    cmap=plt.cm.Blues,\n",
    "    path_to_save_fig=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(actual, predicted).T\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"Predicted label\")\n",
    "    plt.xlabel(\"True label\")\n",
    "\n",
    "    if path_to_save_fig:\n",
    "        plt.savefig(path_to_save_fig, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 3</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Криві навчання з аналізом перенавчання:**  \n",
    "- Мета: побудувати криві навчання/валідації для логістичної регресії на `train-balanced-sarcasm.csv`, оцінити ступінь перенавчання та виявити оптимальний обсяг навчальної вибірки.  \n",
    "- Кроки:\n",
    "  1. Створіть пайплайн `TfidfVectorizer(ngram_range=(1,2), min_df=5, max_features=50000)` + `LogisticRegression(C=1, solver='lbfgs', max_iter=300)`.\n",
    "  2. Використайте `learning_curve` з `train_sizes=np.linspace(0.1, 1.0, 8)` та `cv=5`, обчисліть середні/стандартні відхилення точності для train і val наборів.\n",
    "  3. Побудуйте графік кривих, прокоментуйте області недонавчання/перенавчання, визначте поріг, після якого приріст якості мінімальний.\n",
    "- Підказки: використовуйте `ShuffleSplit` для стійких оцінок; збережіть результати у `DataFrame` для подальшого аналізу.  \n",
    "- Перевірка: додатково побудуйте confusion matrix на повному тренуванні, аби звірити висновки з фактичними помилками.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – Validation curve для параметра C з контролем перенавчання:**  \n",
    "- Мета: проаналізувати, як значення регуляризації `C` впливає на узагальнення моделі, поєднавши validation curve з аналізом кривих помилок.  \n",
    "- Кроки:\n",
    "  1. Використайте той самий пайплайн TF-IDF + логістична регресія.\n",
    "  2. Застосуйте `validation_curve` із `param_range=np.logspace(-2, 2, 10)`, оцінюйте F1 у 5-кратній валідації.\n",
    "  3. Визначте оптимальний `C`, порівняйте з навчальними кривими, додайте confusion matrix.\n",
    "- Підказки: візуалізуйте train/val криві; збережіть найкращу модель через `Pipeline.set_params`.  \n",
    "- Перевірка: наведіть розгорнутий звіт із метриками та рекомендаціями щодо вибору `C`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Odds ratios та інтерпретація:**  \n",
    "- Мета: обчислити співвідношення шансів для ключових ознак логістичної регресії й надати інтерпретацію у контексті сарказму.  \n",
    "- Кроки:\n",
    "  1. Натренуйте модель TF-IDF (1–2-грам) + `LogisticRegression(C=1, solver='liblinear')`.\n",
    "  2. Отримайте коефіцієнти, обчисліть `odds_ratio = np.exp(coef_)`, відіберіть топ-20 позитивних і негативних ознак.\n",
    "  3. Створіть таблицю з odds ratio та прикладами коментарів, де ці ознаки проявляються.\n",
    "- Підказки: приведіть коефіцієнти до pandas `Series`; додайте колонку з частотою ознаки.  \n",
    "- Перевірка: поясніть, як інтерпретація odds ratio допомагає покращити ознакі або preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – Аналіз важливості ознак і їхніх перетинів:**  \n",
    "- Мета: дослідити, як топ-ознаки перетинаються з категоріями (наприклад, пунктуація, емоції), та окреслити вплив на модель.  \n",
    "- Кроки:\n",
    "  1. Витягніть топ-50 ознак за абсолютним коефіцієнтом.\n",
    "  2. Згрупуйте їх за тематичними категоріями (пунктуація, саркастичні маркери, заперечення, позитивні/негативні слова).\n",
    "  3. Побудуйте кольорову матрицю перетинів (heatmap) та інтерпретуйте вплив кожної категорії на log-odds.\n",
    "- Підказки: використовуйте ручний словник або лексичні ресурси (`wordnet`); heatmap створіть через `seaborn`.  \n",
    "- Перевірка: підготуйте текстовий висновок зі спостереженнями щодо ключових категорій, що сигналізують сарказм.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Аналіз помилок через confusion matrix і приклади:**  \n",
    "- Мета: розібрати типові помилки логістичної регресії та розробити рекомендації щодо їх зменшення.  \n",
    "- Кроки:\n",
    "  1. Навчіть модель на 80% даних, протестуйте на 20%, побудуйте confusion matrix (`classification_report` + власна візуалізація).\n",
    "  2. Випишіть по 20 прикладів false positives/negatives, визначте спільні шаблони.\n",
    "  3. Запропонуйте 2–3 стратегії покращення (нові ознакі, очищення даних, налаштування порогу).\n",
    "- Підказки: використовуйте `precision_recall_curve` для аналізу порогів; логуванням фіксуйте ID помилкових записів.  \n",
    "- Перевірка: переобчисліть модель із однією з запропонованих змін і порівняйте метрики.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Probabilities calibration та оцінка:**  \n",
    "- Мета: оцінити, наскільки логістична регресія добре калібрована, і виконати калібрування при потребі.  \n",
    "- Кроки:\n",
    "  1. Натренуйте базову модель TF-IDF + `LogisticRegression`.\n",
    "  2. Застосуйте `calibration_curve` і `BrierScoreLoss` на валідаційному наборі, побудуйте reliability plot.\n",
    "  3. Якщо калібрування незадовільне, застосуйте `CalibratedClassifierCV` (Platt / isotonic), порівняйте результати.\n",
    "- Підказки: зменшення словника до 30 тис. ознак прискорить процес; використовуйте `sklearn.metrics.brier_score_loss`.  \n",
    "- Перевірка: вкажіть, як калібрування вплинуло на прийняття рішень (наприклад, вибір порога).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Аналіз чутливості до дисбалансу:**  \n",
    "- Мета: дослідити вплив зміни балансу класів на логістичну регресію шляхом підвибірки та oversampling.  \n",
    "- Кроки:\n",
    "  1. Створіть підвибірки датасету з різними співвідношеннями класів (наприклад, 1:1, 1:2, 1:3).\n",
    "  2. Навчіть модель на кожній підвибірці, збережіть метрики (F1, precision, recall).\n",
    "  3. Проведіть oversampling (`SMOTE`) і порівняйте з результатами підвибірок.\n",
    "- Підказки: використовуйте `imblearn.combine.SMOTEENN`; логуванням фіксуйте час і стабільність метрик.  \n",
    "- Перевірка: побудуйте графік «співвідношення класів vs F1» і надайте висновки щодо чутливості.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Криві ROC та PR у розрізі підгруп:**  \n",
    "- Мета: побудувати ROC і Precision-Recall криві для різних підгруп текстів (наприклад, короткі/довгі, з емодзі/без) та оцінити стабільність моделі.  \n",
    "- Кроки:\n",
    "  1. Розділіть дані за певними ознаками (довжина, наявність емодзі, CAPS).\n",
    "  2. Для кожної підгрупи побудуйте ROC/PR криві, обчисліть AUC.\n",
    "  3. Порівняйте відмінності та поясніть, які групи найпроблемніші.\n",
    "- Підказки: використовуйте `sklearn.metrics.roc_curve` та `precision_recall_curve`; збережіть графіки.  \n",
    "- Перевірка: запропонуйте зміни для груп із низьким AUC (наприклад, додаткові ознакі) і протестуйте на невеликій вибірці.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Пороги прийняття рішення:**  \n",
    "- Мета: оптимізувати поріг класифікації логістичної регресії під конкретну метрику (наприклад, F0.5 чи F2).  \n",
    "- Кроки:\n",
    "  1. Отримайте предикції ймовірностей на валідації.\n",
    "  2. Обчисліть F-метрику для різних порогів (0.1–0.9 із кроком 0.05).\n",
    "  3. Визначте оптимальний поріг і порівняйте з дефолтним 0.5, побудуйте confusion matrix для обох.\n",
    "- Підказки: використовуйте `sklearn.metrics.fbeta_score`; графік залежності F-метрики від порога допоможе візуалізувати вибір.  \n",
    "- Перевірка: опишіть, як змінився баланс precision/recall та коли варто застосовувати новий поріг.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – Порівняння пайплайнів підготовки тексту:**  \n",
    "- Мета: порівняти два-три пайплайни попередньої обробки (очищення спецсимволів, лематизація, сленг) і оцінити вплив на логістичну регресію.  \n",
    "- Кроки:\n",
    "  1. Реалізуйте три сценарії препроцесингу (мінімальний, зі сленгом, з лематизацією) у вигляді функцій.\n",
    "  2. Для кожного сценарію побудуйте TF-IDF + `LogisticRegression`, оцініть у 5-кратній валідації.\n",
    "  3. Порівняйте метрики, час виконання, стабільність ознак.\n",
    "- Підказки: кешуйте проміжні результати; для лематизації використовуйте `spacy` з `nlp.pipe`.  \n",
    "- Перевірка: сформуйте таблицю з результатами та рекомендацію, який сценарій доцільно використовувати.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – SHAP-аналіз найвпливовіших ознак:**  \n",
    "- Мета: використовуючи SHAP, виявити особливості впливу окремих ознак на рішення логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Навчіть модель на скороченому словнику (наприклад, top-5000 ознак).\n",
    "  2. Застосуйте `shap.LinearExplainer`, побудуйте `summary_plot` і `force_plot` для вибірки прикладів.\n",
    "  3. Інтерпретуйте, які ознаки найчастіше штовхають модель до передбачення сарказму.\n",
    "- Підказки: використовуйте `shap` версії ≥0.40 для підтримки розріджених матриць; обмежте кількість прикладів.  \n",
    "- Перевірка: підготуйте текстовий висновок із прикладами і пропозиціями щодо вдосконалення ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Порівняння з іншими лінійними моделями:**  \n",
    "- Мета: зіставити логістичну регресію з іншими лінійними моделями (SGD, LinearSVC із Platt scaling) з точки зору якості та інтерпретованості.  \n",
    "- Кроки:\n",
    "  1. Натренуйте TF-IDF + `SGDClassifier(loss='log_loss')` та TF-IDF + `LinearSVC` (із подальшим Platt scaling).\n",
    "  2. Порівняйте метрики з логістичною регресією, час навчання, стабільність ознак.\n",
    "  3. Проаналізуйте, у яких сценаріях альтернативи дають кращі результати.\n",
    "- Підказки: використовуйте однакові параметри TF-IDF; зберігайте ваги для порівняння.  \n",
    "- Перевірка: зведіть результати в таблицю, сформулюйте висновок щодо вибору моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – Регуляризація по групах ознак:**  \n",
    "- Мета: перевірити вплив групової регуляризації (Group Lasso) на відбір ознак у логістичній регресії.  \n",
    "- Кроки:\n",
    "  1. Розбийте словник на групи (наприклад, за частинами мови або тематичними токенами).\n",
    "  2. Використайте бібліотеки, що підтримують групову логістичну регресію (наприклад, `glmnet_python` або PyTorch).\n",
    "  3. Порівняйте метрики й кількість активних груп із класичною L2.\n",
    "- Підказки: оберіть невеликі групи для ефективності; використовуйте GPU, якщо можливо.  \n",
    "- Перевірка: підготуйте висновки щодо того, які групи ознак зберігаються і чому.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – Перехресна перевірка порогів для різних бізнес-метрик:**  \n",
    "- Мета: підібрати пороги для різних сценаріїв (мінімізувати false positives, максимізувати recall) і сформувати рекомендації.  \n",
    "- Кроки:\n",
    "  1. На валідації обчисліть precision-recall при різних порогах.\n",
    "  2. Для кожного сценарію (наприклад, «уникнути FP» та «максимізувати знаходження сарказму») знайдіть відповідний поріг.\n",
    "  3. Побудуйте таблицю, у якій зазначте вибрані пороги, відповідні метрики та бізнес-інтерпретацію.\n",
    "- Підказки: використовуйте `sklearn.metrics.precision_recall_curve` та `roc_curve`; збережіть пороги.  \n",
    "- Перевірка: перевірте обрані пороги на тестовому наборі та наведіть результати.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Виявлення дрейфу ознак:**  \n",
    "- Мета: дослідити, як дистрибуція топових ознак змінюється між різними часовими періодами.  \n",
    "- Кроки:\n",
    "  1. Розділіть датасет на часові сегменти (наприклад, квартали чи роки).\n",
    "  2. Для кожного сегменту обчисліть середнє TF-IDF для топ-ознак, побудуйте лінійні графіки динаміки.\n",
    "  3. Оцініть, чи потрібна адаптація моделі до нових трендів.\n",
    "- Підказки: використовуйте `pandas.Grouper` або власні бінінги; збережіть таблицю ознак × час.  \n",
    "- Перевірка: сформулюйте рекомендації щодо ретренінгу або оновлення словників.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Порівняння інтерпретацій моделей через LIME:**  \n",
    "- Мета: застосувати LIME для пояснення рішень логістичної регресії та зіставити з глобальними коефіцієнтами.  \n",
    "- Кроки:\n",
    "  1. Оберіть 20 випадкових прикладів (10 TP, 10 FN).\n",
    "  2. Застосуйте `lime.lime_text.LimeTextExplainer`, побудуйте локальні пояснення.\n",
    "  3. Порівняйте з глобальними коефіцієнтами, визначте узгодження/розбіжності.\n",
    "- Підказки: використовуйте той самий векторизатор, що й у моделі; обмежте кількість ознак у поясненні.  \n",
    "- Перевірка: підготуйте звіт із візуалізаціями LIME та висновками щодо пояснюваності.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Аналіз ложнопозитивних кейсів із контекстом:**  \n",
    "- Мета: глибоко дослідити ложнопозитивні передбачення, щоб визначити, які контекстні ознаки приводять до помилок.  \n",
    "- Кроки:\n",
    "  1. Визначте top-50 ложнопозитивних коментарів.\n",
    "  2. Для кожного проаналізуйте `parent_comment`, контекстні зв’язки, норми орфографії.\n",
    "  3. Запропонуйте зміни (наприклад, контекстні ознакі або правила постпроцесингу).\n",
    "- Підказки: використовуйте `spaCy` для витягування залежностей; складіть таблицю зі спільними патернами.  \n",
    "- Перевірка: імплементуйте одну з пропозицій і повторно оцініть модель.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Адаптивне вибіркове навчання (curriculum learning):**  \n",
    "- Мета: перевірити, чи допомагає поетапне навчання (від простих прикладів до складних) підвищити якість логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Визначте «простоту» прикладів (наприклад, короткі тексти без сленгу vs довгі з метафорами).\n",
    "  2. Навчіть модель поетапно: спочатку на простих, потім поступово додавайте складніші.\n",
    "  3. Порівняйте з моделлю, навченою відразу на всьому наборі.\n",
    "- Підказки: можна використовувати ваги `sample_weight`; логуванням відстежуйте якість на кожному етапі.  \n",
    "- Перевірка: підготуйте графік приросту метрик і обґрунтування ефективності підходу.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – Виявлення аномалій у ознаковому просторі:**  \n",
    "- Мета: ідентифікувати аномальні тексти, які могуть збивати логістичну регресію, та оцінити вплив їх видалення.  \n",
    "- Кроки:\n",
    "  1. Побудуйте TF-IDF матрицю, використайте `IsolationForest` або `OneClassSVM` для пошуку аномалій.\n",
    "  2. Видаліть топ-аномальні приклади, перевчіть модель.\n",
    "  3. Порівняйте метрики до/після, визначте, чи варто здійснювати очистку.\n",
    "- Підказки: задайте `contamination` ~0.02; контролюйте баланс класів після очищення.  \n",
    "- Перевірка: опишіть характер аномалій і поясніть, чому вони шкодять моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Моніторинг продуктивності після «розгортання»:**  \n",
    "- Мета: змоделювати виробничий моніторинг логістичної регресії та створити звіт про стабільність у часі.  \n",
    "- Кроки:\n",
    "  1. Розбийте дані на історичні (train) і «потокові» (імітація майбутніх) батчі.\n",
    "  2. Після навчання на історичних даних оцініть якість на кожному новому батчі: precision, recall, PSI для ключових ознак.\n",
    "  3. Сформуйте рекомендації щодо частоти ретренінгу та порогів сповіщень.\n",
    "- Підказки: збережіть результати у Markdown-таблиці; використовуйте `pandas.qcut` для PSI.  \n",
    "- Перевірка: підготуйте фінальний звіт зі списком метрик, графіками та висновками щодо стабільності моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5648f6ad7a14ef3a582909f7c0c72c4fc80204aa"
   },
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.4. Вдосконалення моделі логістичної регресії</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 4</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Дворівневе TF-IDF за частотою лексики:**  \n",
    "- Мета: побудувати логістичну регресію, що одночасно враховує “рідкісні” та “часті” слова, для виявлення сарказму в `train-balanced-sarcasm.csv`.  \n",
    "- Кроки:\n",
    "  1. Сформуйте два TF-IDF векторизатори: для рідкісних слів (`max_df=0.4`, `min_df=2`) і для частих слів (`max_df=1.0`, `min_df=30`), обидва з `ngram_range=(1,2)`.\n",
    "  2. Об’єднайте їх через `FeatureUnion`, додайте стандартизовані числові ознаки (довжина, частка CAPS, пунктуація).\n",
    "  3. Навчіть `LogisticRegression` (solver='lbfgs', C=1) і порівняйте з базовою моделлю, оцінюючи F1, precision, recall у 5-кратній `StratifiedKFold` валідації.\n",
    "- Підказки: використовуйте `Pipeline`, щоб уникнути витоків; кешуйте TF-IDF через `memory` параметр.  \n",
    "- Перевірка: підготуйте таблицю порівняння метрик “базова” vs “дворівнева” та наведіть приклади ознак із найбільшим приростом ваг.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – Інтенсивність CAPS та емоційна експресія:**  \n",
    "- Мета: дослідити, чи підвищений рівень капс-лок і повторів символів покращує логістичну регресію для сарказму.  \n",
    "- Кроки:\n",
    "  1. Витягніть кількість повністю великих слів, частку великих літер, довгі повтори (`r'(.)\\1{2,}'`) для `comment` і `parent_comment`.\n",
    "  2. Об’єднайте ці ознаки з TF-IDF (1–2-грам) у `ColumnTransformer`, стандартизуйте числовий блок.\n",
    "  3. Натренуйте `LogisticRegression` (solver='saga', penalty='l2'), порівняйте метрики з моделлю без нових ознак.\n",
    "- Підказки: використовуйте `FunctionTransformer` для швидкого підрахунку; логуванням збережіть статистику CAPS.  \n",
    "- Перевірка: побудуйте barplot коефіцієнтів для CAPS-ознак і поясніть їхній вплив.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Тональність і саркастичні контрасти:**  \n",
    "- Мета: інтегрувати полярність `comment` та різницю тональностей із `parent_comment` для логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Обчисліть полярність, суб’єктивність (`TextBlob` або `VADER`) для обох текстів, сформуйте похідні: різниця, модуль різниці, знак зміни.\n",
    "  2. Додайте ці ознаки до TF-IDF (ngram 1–3) через `ColumnTransformer`.\n",
    "  3. Навчіть `LogisticRegression(C=0.8)`, порівняйте AUC та F1 з базовою моделлю без тональності.\n",
    "- Підказки: нормуйте тональність у діапазоні [−1;1]; обробіть пропуски (для відсутніх parent_comment).  \n",
    "- Перевірка: наведіть приклади з сильним контрастом тональностей, де модель змінила прогноз.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – Ознаки пунктуаційної драматургії:**  \n",
    "- Мета: перевірити вплив складних пунктуаційних патернів (наприклад, “?!”, “...”) на логістичну регресію.  \n",
    "- Кроки:\n",
    "  1. Витягніть частоти одиночних знаків пунктуації та біграми символів (`!?`, `?!`, `...`, `!!!`).\n",
    "  2. Створіть бінарні ознаки наявності “довгих хвостів” (`len(re.findall(r'!{3,}', text))`).\n",
    "  3. Об’єднайте з TF-IDF (1–2-грам), навчіть `LogisticRegression(C=1.2)` та оцініть вплив на precision для сарказму.\n",
    "- Підказки: використовуйте `Series.str.count`; додавайте ознаки у `scipy.sparse.csr_matrix` для ефективності.  \n",
    "- Перевірка: проаналізуйте, які пунктуаційні патерни отримали найбільші коефіцієнти, і сформулюйте висновок.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Детектування мемних тригерів:**  \n",
    "- Мета: додати до логістичної регресії словник мемів/сленгу та перевірити їхній внесок у виявлення сарказму.  \n",
    "- Кроки:\n",
    "  1. Зберіть словник популярних саркастичних мемів/сленгу (наприклад, за відкритими списками).\n",
    "  2. Створіть бінарні/частотні ознаки за словником, інтегруйте їх із TF-IDF у `FeatureUnion`.\n",
    "  3. Навчіть логістичну регресію, порівняйте метрики й обчисліть SHAP для мемних ознак.\n",
    "- Підказки: використовуйте `DictVectorizer`; слідкуйте за кодуванням (lowercase).  \n",
    "- Перевірка: підготуйте топ-10 мемів із найвищими вагами та опишіть їхній вплив.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Багатомовні ознаки й мовні перемикачі:**  \n",
    "- Мета: обробити змішані мови та вплив мовних перемикань у саркастичних текстах.  \n",
    "- Кроки:\n",
    "  1. Визначте мову кожного речення (`langdetect`, `fasttext`), підрахуйте кількість мовних перемикань у межах коментаря.\n",
    "  2. Додайте до TF-IDF бінарні ознаки “змішана мова”, “домінантна мова”, “інший алфавіт”.\n",
    "  3. Навчіть логістичну регресію, порівняйте стабільність результатів за мовними підгрупами.\n",
    "- Підказки: кешуйте результати мовної детекції; використовуйте `Pipeline`, щоб уникнути дубляжу.  \n",
    "- Перевірка: наведіть криву ROC для англомовних vs змішаних коментарів, сформулюйте рекомендації.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Контекстні пари коментарів:**  \n",
    "- Мета: використати ознаки порівняння `comment` і `parent_comment` (відстань за n-грамами, TF-IDF косинус).  \n",
    "- Кроки:\n",
    "  1. Обчисліть косинусну схожість між TF-IDF `comment` та `parent_comment`, різницю довжин, наявність цитат.\n",
    "  2. Додайте ці показники до моделей як числові ознаки.\n",
    "  3. Навчіть логістичну регресію й оцініть, чи допомагає контекстна схожість виявляти сарказм.\n",
    "- Підказки: для косинусної схожості використовуйте `sklearn.metrics.pairwise.cosine_similarity`; нормуйте значення.  \n",
    "- Перевірка: подайте scatterplot “схожість vs саркастичність” і проаналізуйте.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Семантичні ембедінги + логістична регресія:**  \n",
    "- Мета: поєднати TF-IDF із контекстними ембедінгами (наприклад, `sentence-transformers`) для кращої класифікації.  \n",
    "- Кроки:\n",
    "  1. Згенеруйте sentence embeddings (наприклад, `all-MiniLM-L6-v2`) для `comment`.\n",
    "  2. Об’єднайте їх із TF-IDF матрицею через `FeatureUnion` (спочатку зменшивши розмірність ембедінгів PCA до 100).\n",
    "  3. Навчіть `LogisticRegression(C=0.5)` і порівняйте з чистим TF-IDF.\n",
    "- Підказки: зберігайте ембедінги як `np.float32`; використовуйте `StandardScaler` перед об’єднанням.  \n",
    "- Перевірка: оцініть приріст F1 і наведіть приклади, де ембедінги допомогли.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Стилометричні ознаки авторів:**  \n",
    "- Мета: додати стилометрію (середня довжина слова, різноманіття, типові суфікси) до логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Обчисліть лексичне різноманіття (`unique_tokens / total_tokens`), середню довжину слова, кількість емодзі.\n",
    "  2. Додайте ці ознаки до TF-IDF у `ColumnTransformer`.\n",
    "  3. Навчіть модель, оцініть вплив на precision та recall.\n",
    "- Підказки: обробляйте токенізацію `nltk.word_tokenize`; для емодзі використовуйте `regex`.  \n",
    "- Перевірка: продемонструйте кореляцію стилометричних ознак із сарказмом.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – Динамічний вибір порога за вартістю помилок:**  \n",
    "- Мета: оптимізувати поріг логістичної регресії з урахуванням різних “вартісних” сценаріїв.  \n",
    "- Кроки:\n",
    "  1. Визначте матрицю вартостей (наприклад, FP=5, FN=1).\n",
    "  2. На валідації переберіть пороги 0–1 з кроком 0.01, обчисліть очікувані витрати.\n",
    "  3. Оберіть поріг із мінімальними витратами та задокументуйте результати.\n",
    "- Підказки: використовуйте `numpy` для векторизованого підрахунку; побудуйте графік “поріг vs витрати”.  \n",
    "- Перевірка: порівняйте метрики при новому порозі зі стандартним 0.5.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – Feature hashing для масштабних експериментів:**  \n",
    "- Мета: прискорити обробку великого словника через `HashingVectorizer` та порівняти з класичним TF-IDF у логістичній регресії.  \n",
    "- Кроки:\n",
    "  1. Реалізуйте два пайплайни: TF-IDF + `LogisticRegression` і `HashingVectorizer` + `LogisticRegression`.\n",
    "  2. Проведіть заміри часу та пам’яті на 5-кратній валідації (наприклад, через `memory_profiler`).\n",
    "  3. Порівняйте якість і ресурси, сформулюйте висновки.\n",
    "- Підказки: у `HashingVectorizer` увімкніть `alternate_sign=False`; контролюйте колізії.  \n",
    "- Перевірка: оформіть таблицю “метрика/час/пам’ять” для обох підходів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Інкрементальне навчання для потокових даних:**  \n",
    "- Мета: реалізувати й оцінити інкрементальне оновлення логістичної регресії на потокових батчах.  \n",
    "- Кроки:\n",
    "  1. Розбийте дані на послідовні батчі (наприклад, кожні 50 тис. рядків), використайте `HashingVectorizer`.\n",
    "  2. По черзі застосовуйте `partial_fit` логістичної регресії (solver='saga') з підтримкою обробки потоків.\n",
    "  3. Порівняйте якість і час із повним навчанням на всьому наборі.\n",
    "- Підказки: збережіть стан моделі після кожного батча; використовуйте `StandardScaler` з `partial_fit`.  \n",
    "- Перевірка: побудуйте графік еволюції F1 після кожного батча.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – Крос-перевірка з перекриттям тематичних модулів:**  \n",
    "- Мета: дослідити, чи логістична регресія узагальнюється між різними тематичними групами (наприклад, політика, спорт).  \n",
    "- Кроки:\n",
    "  1. Розмітьте тексти тематичними тегами (можна за ключовими словами або кластеризацією TF-IDF + KMeans).\n",
    "  2. Проведіть cross-topic валідацію: тренуйте на одних темах, тестуйте на інших (наприклад, leave-one-topic-out).\n",
    "  3. Порівняйте результати з класичною крос-валідацією.\n",
    "- Підказки: використовуйте `GroupKFold`; збережіть розподіл тем.  \n",
    "- Перевірка: підготуйте висновок про теми, на яких модель узагальнюється найгірше, і запропонуйте покращення.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – Оцінка взаємодій через PolynomialFeatures:**  \n",
    "- Мета: інтегрувати нелінійні взаємодії між числовими ознаками (наприклад, тональність × CAPS) у логістичну регресію.  \n",
    "- Кроки:\n",
    "  1. Витягніть числові ознаки (тональність, пунктуація, CAPS).\n",
    "  2. Застосуйте `PolynomialFeatures(degree=2, interaction_only=True)` перед логістичною регресією.\n",
    "  3. Порівняйте метрики з моделлю без взаємодій, звертаючи увагу на стабільність.\n",
    "- Підказки: стандартизуйте ознаки перед поліноміальним розширенням; контролюйте кількість ознак.  \n",
    "- Перевірка: опишіть взаємодії з найбільшими коефіцієнтами та їхню роль у сарказмі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Адаптивна відсічка словника за інформативністю:**  \n",
    "- Мета: автоматично обмежити словник TF-IDF, відкидаючи малоінформативні ознаки за допомогою `SelectFromModel`.  \n",
    "- Кроки:\n",
    "  1. Побудуйте TF-IDF (1–3-грам) з великою кількістю ознак (до 100 тис.).\n",
    "  2. Навчіть проміжну логістичну регресію та застосуйте `SelectFromModel` за порогом ваг (наприклад, top 10%).\n",
    "  3. На відібраних ознаках навчіть фінальну модель, порівняйте якість та час.\n",
    "- Підказки: використовувати `max_iter=1000`, щоб модель сходилася; контролюйте sparse-формат.  \n",
    "- Перевірка: підготуйте графік залежності F1 від розміру словника.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Змішане кодування: TF-IDF + bag-of-entities:**  \n",
    "- Мета: додати до логістичної регресії інформацію про назви сутностей (імена, організації).  \n",
    "- Кроки:\n",
    "  1. Використайте `spaCy` або `stanza` для NER; створіть список сутностей у кожному коментарі.\n",
    "  2. Побудуйте bag-of-entities (OneHot/TF-IDF по сутностях) і додайте до словних TF-IDF.\n",
    "  3. Навчіть логістичну регресію й оцініть, чи допомагає знання сутностей.\n",
    "- Підказки: нормуйте сутності (lowercase), об’єднуйте рідкісні в категорію `other`.  \n",
    "- Перевірка: наведіть приклади, де згадки конкретних сутностей викликали саркастичні висловлювання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Аугментація даних через парафрази:**  \n",
    "- Мета: перевірити, чи покращує логістичну регресію аугментація саркастичних текстів парафразами.  \n",
    "- Кроки:\n",
    "  1. Згенеруйте парафрази саркастичних коментарів (наприклад, `back-translation` або `parrot paraphraser`), зберігши мітку.\n",
    "  2. Об’єднайте оригінальний і аугментований корпус, побудуйте TF-IDF + `LogisticRegression`.\n",
    "  3. Порівняйте F1, аналізуючи вплив на recall позитивного класу.\n",
    "- Підказки: контролюйте якість парафраз (видаляйте повтори, фільтруйте занадто короткі); обмежте кількість аугментованих прикладів.  \n",
    "- Перевірка: наведіть статистику оновленого корпусу та порівняння метрик до/після.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Фільтрація шумових користувачів:**  \n",
    "- Мета: виявити авторів із великою кількістю неоднозначних коментарів та дослідити, як їхнє виключення впливає на модель.  \n",
    "- Кроки:\n",
    "  1. Для кожного автора порахуйте частку саркастичних коментарів, кількість суперечливих (модель помилилася).\n",
    "  2. Визначте “шумових” авторів (наприклад, >70% помилок), видаліть їхні коментарі та перевчіть модель.\n",
    "  3. Порівняйте метрики та поясніть, чи варто застосовувати фільтрацію.\n",
    "- Підказки: авторів можна знайти через колонку `author`; зберігайте проміжні таблиці.  \n",
    "- Перевірка: опишіть зміну F1 та вплив на стабільність моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – Автоматичне формування словника сарказму:**  \n",
    "- Мета: побудувати власний словник саркастичних маркерів із моделі логістичної регресії та інтегрувати його в preprocessing.  \n",
    "- Кроки:\n",
    "  1. Навчіть базову логістичну регресію, отримайте топ-позитивні ознаки.\n",
    "  2. Збережіть їх як словник сарказму, використайте для створення додаткових бінарних ознак (“містить маркер”).\n",
    "  3. Додайте нові ознакі до пайплайна та перевчіть модель, порівнюючи метрики.\n",
    "- Підказки: очищайте словник від стоп-слів; додавайте нові ознаки через `FunctionTransformer`.  \n",
    "- Перевірка: продемонструйте список маркерів і зростання точності/recall.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Розгортання та моніторинг у вигляді звіту:**  \n",
    "- Мета: створити прототип “production-ready” пайплайна логістичної регресії з моніторингом якості.  \n",
    "- Кроки:\n",
    "  1. Побудуйте е2е-процес: preprocessing, навчання, серіалізація (`joblib`) і функція для передбачення нових текстів.\n",
    "  2. Створіть модуль моніторингу: збір метрик, логування помилок, обчислення PSI для ключових ознак.\n",
    "  3. Підготуйте Markdown-звіт із описом архітектури, метрик і плану ретренінгу.\n",
    "- Підказки: використовуйте `mlflow` або простий логер; забезпечте відтворюваність (`random_state`).  \n",
    "- Перевірка: продемонструйте приклад передбачення на нових даних і фрагмент лог-звіту з метриками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-4.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.5. Практичне застосування результатів інтелектуального аналізу даних</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:2em;\">Завдання 5</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Фінальна модель зі збагаченими ознаками:**  \n",
    "- Мета: зібрати всі попередньо створені ознаки (TF-IDF, пунктуація, CAPS, тональність, контекст) у єдиний пайплайн логістичної регресії та оцінити фінальну продуктивність.  \n",
    "- Кроки:\n",
    "  1. Об’єднайте підготовлені трансформери у `ColumnTransformer`/`FeatureUnion`, створіть повний `Pipeline`.\n",
    "  2. Розбийте `train-balanced-sarcasm.csv` на train/validation/test (60/20/20) з фіксованим `random_state`.\n",
    "  3. Навчіть `LogisticRegression` (solver='lbfgs', C=1), зафіксуйте метрики (F1, AUC, log-loss) на валідації, оберіть найкращі гіперпараметри та протестуйте.\n",
    "- Підказки: використовуйте `Pipeline` з іменованими кроками для зручності; логуванням (`mlflow` або `wandb`) відслідковуйте експерименти.  \n",
    "- Перевірка: складіть підсумковий звіт із метриками й порівнянням із базовою моделлю TF-IDF + логістична регресія.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – Аналіз незбалансованого набору:**  \n",
    "- Мета: перевірити поведінку фінальної моделі без балансування класів, оцінити необхідність ваг/ресемплінгу.  \n",
    "- Кроки:\n",
    "  1. Візьміть початковий (незбалансований) `train-balanced-sarcasm.csv`, збережіть індекси саркастичних/несаркастичних.\n",
    "  2. Навчіть модель у трьох сценаріях: без ваг, із `class_weight='balanced'`, із oversampling (SMOTE).\n",
    "  3. Порівняйте метрики (особливо recall для сарказму) та час навчання.\n",
    "- Підказки: використовуйте `imblearn.pipeline`; контролюйте витік даних (SMOTE лише на train).  \n",
    "- Перевірка: побудуйте таблицю порівняння й сформулюйте рекомендацію щодо балансування.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Продуктивна функція передбачення:**  \n",
    "- Мета: обгорнути фінальну логістичну регресію в сервісну функцію, що повертає ймовірність сарказму та пояснення ознак.  \n",
    "- Кроки:\n",
    "  1. Зробіть `Pipeline` серіалізованим (через `joblib`), реалізуйте функцію `predict_with_explanation(text)` із `predict_proba`.\n",
    "  2. Інтегруйте `LIME` або `SHAP` для локального пояснення уведеного тексту.\n",
    "  3. Додайте перевірку: на тестових прикладах забезпечте коректність, сформуйте приклади використання.\n",
    "- Підказки: для SHAP використовуйте `LinearExplainer`; обмежте кількість ознак у поясненні.  \n",
    "- Перевірка: задокументуйте API-функцію, наведіть 2–3 приклади з поясненнями.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – Довгі vs короткі тексти:**  \n",
    "- Мета: показати, як фінальна модель поводиться на різних довжинах коментарів.  \n",
    "- Кроки:\n",
    "  1. Розбийте дані на короткі (<50 символів), середні (50–120) та довгі (>120).\n",
    "  2. Виміряйте метрики (F1, precision, recall) на кожній підмножині.\n",
    "  3. Розгляньте можливість окремих моделей або зважування ознак для проблемних довжин.\n",
    "- Підказки: візуалізуйте результати bar chart; при потребі навчіть окремі пайплайни.  \n",
    "- Перевірка: сформулюйте рекомендації щодо обробки текстів різної довжини (наприклад, видалення шуму в коротких).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Класифікація за жанрами тем:**  \n",
    "- Мета: оцінити логістичну регресію в розрізі тематичних підмножин (політика, спорт, розваги).  \n",
    "- Кроки:\n",
    "  1. Змітьте теми ключовими словами або латентними темами (`NMF`/`LDA`).\n",
    "  2. Для кожної теми обчисліть метрики моделі, порівняйте стабільність.\n",
    "  3. Запропонуйте модифікації для тем, де якість падає.\n",
    "- Підказки: використовуйте `TopicModel` із TF-IDF; логуванням збережіть розподіл тем.  \n",
    "- Перевірка: створіть тему→метрика таблицю, додайте висновки щодо адаптації моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Експеримент з зовнішнім тестовим набором:**  \n",
    "- Мета: перевірити здатність логістичної регресії переноситися на зовнішні дані (інший сабреддіт, інша платформа).  \n",
    "- Кроки:\n",
    "  1. Підготуйте зовнішній тестовий набір (за можливості), очистіть його до одного формату.\n",
    "  2. Застосуйте фінальну модель без донавчання, порівняйте метрики.\n",
    "  3. Проаналізуйте, які ознаки втратили актуальність, запропонуйте адаптацію (наприклад, оновити словник).\n",
    "- Підказки: якщо немає зовнішнього набору, імітуйте його, відокремивши частину датасету з іншими темами.  \n",
    "- Перевірка: підготуйте порівняння внутрішній vs зовнішній набір і план дій при деградації.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Аналіз класифікаційних помилок з поясненнями:**  \n",
    "- Мета: зібрати найтиповіші помилки (FP, FN), пояснити причини й запропонувати покращення.  \n",
    "- Кроки:\n",
    "  1. Витягніть топ-50 FP/FN, застосуйте SHAP/LIME для пояснення.\n",
    "  2. Знайдіть повторювані патерни (наприклад, сарказм із позитивною тональністю).\n",
    "  3. Запропонуйте 2–3 конкретні покращення (нові ознакі, очищення, поріг).\n",
    "- Підказки: оформіть таблицю з колонками текст/проблема/рішення; дотримуйтесь анонімності.  \n",
    "- Перевірка: реалізуйте одну пропозицію й виміряйте зміну метрик.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Вплив feature dropout:**  \n",
    "- Мета: оцінити, наскільки критичними є різні групи ознак, послідовно відключаючи їх.  \n",
    "- Кроки:\n",
    "  1. Визначте групи ознак (TF-IDF, тональність, пунктуація, контекст).\n",
    "  2. Запустіть експерименти, кожного разу відключаючи одну групу, зафіксуйте метрики.\n",
    "  3. Побудуйте “waterfall chart” впливу ознак.\n",
    "- Підказки: реалізуйте параметр `include_features` у пайплайні; автоматизуйте через цикл.  \n",
    "- Перевірка: підготуйте звіт із впливом кожної групи та рекомендацією, які ознакі критичні.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Підбір порога під бізнес-метрику:**  \n",
    "- Мета: вибрати оптимальний поріг для різних сценаріїв (мінімізація FP, максимізація recall сарказму).  \n",
    "- Кроки:\n",
    "  1. Зберіть precision-recall та ROC-криві для валідації.\n",
    "  2. Для кожної бізнес-мети підберіть поріг (наприклад, maximize F0.5, maximize recall при precision≥0.6).\n",
    "  3. Протестуйте пороги на тестовому наборі.\n",
    "- Підказки: використовуйте `sklearn.metrics.precision_recall_curve`; створіть таблицю поріг→метрики.  \n",
    "- Перевірка: задокументуйте вибори порогів і наведені метрики на тесті.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – Порівняння логістичної регресії з ансамблями:**  \n",
    "- Мета: перевірити, чи перевершують фінальну логістичну модель ансамблі (наприклад, XGBoost, LightGBM) при використанні тих самих ознак.  \n",
    "- Кроки:\n",
    "  1. Побудуйте загальний `FeatureGenerator`, який повертає матрицю ознак.\n",
    "  2. Навчіть логістичну регресію й ансамблеві моделі на тих самих ознаках.\n",
    "  3. Порівняйте метрики та пояснюваність, час тренування.\n",
    "- Підказки: контролюйте гіперпараметри ансамблів; аналізуйте важливість ознак.  \n",
    "- Перевірка: сформулюйте висновок, коли варто залишатися на логістичній регресії, а коли — переходити до ансамблів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – Моніторинг в реальному часі:**  \n",
    "- Мета: розробити шаблон моніторингу продуктивності моделі в “продуктивному” середовищі.  \n",
    "- Кроки:\n",
    "  1. Створіть функції для обчислення метрик на ковзаючому вікні (наприклад, останні 1000 прогнозів).\n",
    "  2. Поставте сповіщення при падінні метрик нижче порогу.\n",
    "  3. Оцініть ефективність моніторингу через імітацію зниження якості.\n",
    "- Підказки: використовуйте `psutil`/`logging`; можна моделювати потоковий сценарій.  \n",
    "- Перевірка: задокументуйте “runbook” (що робити при деградації).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Сценарій ретренінгу:**  \n",
    "- Мета: прописати процедуру ретренінгу логістичної регресії при появі нових даних.  \n",
    "- Кроки:\n",
    "  1. Зберіть шаблон скрипту: завантаження нових даних, об’єднання з історичними, розділення.\n",
    "  2. Перенавчіть модель, порівняйте метрики старої й нової.\n",
    "  3. Передбачте rollback: як повернутися до попередньої версії при погіршенні.\n",
    "- Підказки: використовуйте керування версіями (Git, DVC); автоматизуйте через `Makefile`/CI.  \n",
    "- Перевірка: створіть тестовий кейс ретренінгу й опишіть результати.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – Документація моделі:**  \n",
    "- Мета: підготувати детальну документацію (Model Card) фінальної логістичної регресії.  \n",
    "- Кроки:\n",
    "  1. Описати data pipeline, ознакі, гіперпараметри, метрики.\n",
    "  2. Додати секції: “Обмеження”, “Етичні зауваги”, “Рекомендації користувачам”.\n",
    "  3. Оформити у вигляді Markdown- або PDF-звіту, додати графіки.\n",
    "- Підказки: використовуйте шаблони Model Card (Google); зберігайте статистику у вигляді таблиць.  \n",
    "- Перевірка: перевірте, чи документація охоплює всі аспекти відтворюваності й етики.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – Порівняння граничних випадків:**  \n",
    "- Мета: вручну зібрати “corner cases” (іронія без ключових слів, змішані мови) та перевірити модель.  \n",
    "- Кроки:\n",
    "  1. Створіть тестовий набір із 50 граничних прикладів (реальні/штучні).\n",
    "  2. Запустіть модель, проаналізуйте результати й пояснення.\n",
    "  3. Розробіть рекомендації щодо покращення для таких випадків.\n",
    "- Підказки: використовуйте ручний контроль якості; фіксуйте приклади у CSV.  \n",
    "- Перевірка: оновіть модель (за потреби) й повторно протестуйте corner cases.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Персоналізація для авторів:**  \n",
    "- Мета: перевірити, чи допомагає персоналізована логістична регресія (додаткові ознаки автора) для частих користувачів.  \n",
    "- Кроки:\n",
    "  1. Для авторів із ≥10 постів додайте бінарні ознаки “автор=Х” (one-hot або embedding).\n",
    "  2. Навчіть модель і порівняйте якість із базовою.\n",
    "  3. Оцініть ризики (overfitting, приватність).\n",
    "- Підказки: використовуйте `OneHotEncoder(handle_unknown='ignore')`; контролюйте розмір словника авторів.  \n",
    "- Перевірка: подайте результати й рекомендації, чи варто персоналізувати модель.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Функція батьківського контексту:**  \n",
    "- Мета: додати “функцію контрасту” між `comment` і `parent_comment` (наприклад, TF-IDF різниця) до фінальної моделі.  \n",
    "- Кроки:\n",
    "  1. Обчисліть різницю TF-IDF векторів або “протиріччя” (наприклад, cosine distance).\n",
    "  2. Додайте як числову ознаку до пайплайна.\n",
    "  3. Проаналізуйте, чи покращується розпізнавання сарказму, коли контраст великий.\n",
    "- Підказки: реалізуйте це як `FunctionTransformer`; нормуйте значення.  \n",
    "- Перевірка: опишіть вплив на метрики й покажіть приклади.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Self-training на невизначених даних:**  \n",
    "- Мета: апробувати напівконтрольоване навчання (self-training) для розширення даних.  \n",
    "- Кроки:\n",
    "  1. Візьміть unlabeled дані (якщо доступні) або використайте коментарі без мітки.\n",
    "  2. На першому колі навчіть модель, виберіть приклади з високою впевненістю, додайте їх до тренувального набору.\n",
    "  3. Повторіть 1–2 ітерації, порівняйте метрики.\n",
    "- Підказки: контролюйте пороги впевненості; уникайте накопичення помилок.  \n",
    "- Перевірка: оцініть приріст F1 і чи виправданий self-training.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Explainable dashboard:**  \n",
    "- Мета: створити інтерактивний дашборд (наприклад, `streamlit`), що демонструє можливості моделі.  \n",
    "- Кроки:\n",
    "  1. Побудуйте UI з полем вводу, виводом ймовірності та пояснення (SHAP/LIME).\n",
    "  2. Додайте вкладку з метриками й графіками (криві ROC, confusion matrix).\n",
    "  3. Забезпечте обробку помилок, логування введень.\n",
    "- Підказки: використовуйте кешування в streamlit; серіалізуйте модель перед тим.  \n",
    "- Перевірка: протестуйте дашборд на випадкових прикладах, переконайтеся, що пояснення коректні.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – Гібридний порівняльний аналіз:**  \n",
    "- Мета: порівняти логістичну регресію з нейромережевою моделлю (наприклад, простим BiLSTM) при однакових даних.  \n",
    "- Кроки:\n",
    "  1. Підготуйте однаковий набір ознак (словникові індекси чи ембедінги).\n",
    "  2. Навчіть логістичну регресію та BiLSTM, порівняйте метрики, час і пояснюваність.\n",
    "  3. Опишіть компроміси між підходами.\n",
    "- Підказки: використовуйте `Keras`/`PyTorch` для BiLSTM; контролюйте однаковий train/validation split.  \n",
    "- Перевірка: зведіть результати у таблицю, сформулюйте висновки для курсового проєкту.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Підсумкова презентація:**  \n",
    "- Мета: підготувати презентаційний матеріал (слайди або ноутбук) із ключовими висновками про логістичну регресію.  \n",
    "- Кроки:\n",
    "  1. Зібрати ключові результати: метрики, графіки, приклади помилок, рекомендації.\n",
    "  2. Оформити слайди (PowerPoint/Google Slides) або інтерактивний ноутбук (Jupyter).\n",
    "  3. Переконатися, що матеріал зрозумілий для аудиторії, яка не бачила код.\n",
    "- Підказки: використовуйте візуалізації (ROC, PR, SHAP); додайте розділ “Наступні кроки”.  \n",
    "- Перевірка: отримайте peer-review (колега/викладач) та внесіть правки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
