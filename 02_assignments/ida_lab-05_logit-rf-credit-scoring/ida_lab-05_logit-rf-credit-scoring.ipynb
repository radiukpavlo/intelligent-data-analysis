{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Лабораторна робота 5.</center></h1>\n",
    "<h2><center>Логістична регресія й випадковий ліс в задачі кредитного скорингу</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Виконав:** Прізвище І.П.\n",
    "\n",
    "**Варіант:** №__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зміст \n",
    "\n",
    "- [5.1. Завантаження навчальних даних](#lab-5.1)\n",
    "- [5.2. Бутстреп](#lab-5.2)  \n",
    "- [5.3. Підбір параметрів для ансамблевих моделей](#lab-5.3)\n",
    "- [5.4. Визначення впливу ознак](#lab-5.4)\n",
    "- [5.5. Створення ансамблевих моделей](#lab-5.5)\n",
    "- [5.6. Удосконалення ансамблевих моделей](#lab-5.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Головним завданням цієї лабораторної роботи є побудова моделі на основі машинного навчання (МН) для задачі кредитного скорингу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.1. Завантаження навчальних даних</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Змінна, значення якої прогнозується, $Y$:\n",
    "\n",
    " - SeriousDlqin2yrs – стан невиплати людиною кредиту впроводж наступних 90 днів; можливі значення: 1 – клієнт оплачує кредит із запізненням або не виплачує взагалі, 0 – клієнт оплачує кредит вчасно. \n",
    "\n",
    "Незалежні ознаки, $X$:\n",
    "\n",
    " - age – вік позичальника кредитних коштів; тип – integer;\n",
    " - NumberOfTime30-59DaysPastDueNotWorse – кількість прострочених виплат інших кредитів більше 30-59 днів тому, але не більше впродовж останніх двох років; тип – integer;\n",
    " - DebtRatio – відношення щомісячного відрахування щодо заборгованості (кредити, аліменти тощо) до сукупного місячного доходу (percentage); тип – real;\n",
    " - MonthlyIncome – місячний дохід в доларах; тип – real;\n",
    " - NumberOfTimes90DaysLate – кількість прострочених виплат інших кредитів більше 90 днів; тип – integer;\n",
    " - NumberOfTime60-89DaysPastDueNotWorse – кількість прострочених виплат інших кредитів більш 60-89 днів, але не більше впродовж останніх двох років; тип – integer;\n",
    " - NumberOfDependents – кількість членів сім'ї позичальника; тип – integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:45.092082500Z",
     "start_time": "2023-11-21T16:17:45.062974400Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Відключимо попередження Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:45.111708Z",
     "start_time": "2023-11-21T16:17:45.093589800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Задамо графіки у форматі .svg, щоби вони мали кращу чіткість\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "# Підвищимо розмір графіків за замовчуванням\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 7, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Створимо функцію, яка буде замінювати NaN значення на медіану в кожному стовпчику таблиці:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:45.138011300Z",
     "start_time": "2023-11-21T16:17:45.111708Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_nan(table):\n",
    "    for col in table.columns:\n",
    "        table[col]= table[col].fillna(table[col].median())\n",
    "    return table   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантажуємо дані для подальшого навчання:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.111524200Z",
     "start_time": "2023-11-21T16:17:45.124492400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8158.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>3870.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6666.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10500.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.271820</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  age  NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  \\\n",
       "0                 0   64                                     0     0.249908   \n",
       "1                 0   58                                     0  3870.000000   \n",
       "2                 0   41                                     0     0.456127   \n",
       "3                 0   43                                     0     0.000190   \n",
       "4                 1   49                                     0     0.271820   \n",
       "\n",
       "   NumberOfTimes90DaysLate  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                        0                                     0   \n",
       "1                        0                                     0   \n",
       "2                        0                                     0   \n",
       "3                        0                                     0   \n",
       "4                        0                                     0   \n",
       "\n",
       "   MonthlyIncome  NumberOfDependents  \n",
       "0         8158.0                 0.0  \n",
       "1            NaN                 0.0  \n",
       "2         6666.0                 0.0  \n",
       "3        10500.0                 2.0  \n",
       "4          400.0                 0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_scoring_url = 'https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/refs/heads/main/02_assignments/ida_lab-05_logit-rf-credit-scoring/credit_scoring_sample.csv'\n",
    "\n",
    "data = pd.read_csv(credit_scoring_url, sep =';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розглянемо типи завантажених даних:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.143103400Z",
     "start_time": "2023-11-21T16:17:46.107527900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeriousDlqin2yrs                          int64\n",
       "age                                       int64\n",
       "NumberOfTime30-59DaysPastDueNotWorse      int64\n",
       "DebtRatio                               float64\n",
       "NumberOfTimes90DaysLate                   int64\n",
       "NumberOfTime60-89DaysPastDueNotWorse      int64\n",
       "MonthlyIncome                           float64\n",
       "NumberOfDependents                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переглянемо розподіл класів у цільовій змінні:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.450060800Z",
     "start_time": "2023-11-21T16:17:46.125097600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of target\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SeriousDlqin2yrs\n",
       "0    0.777511\n",
       "1    0.222489\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"443.850625pt\" height=\"341.167375pt\" viewBox=\"0 0 443.850625 341.167375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-11-13T18:24:09.845017</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.10.5, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 341.167375 \n",
       "L 443.850625 341.167375 \n",
       "L 443.850625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 46.050625 301.34175 \n",
       "L 436.650625 301.34175 \n",
       "L 436.650625 24.14175 \n",
       "L 46.050625 24.14175 \n",
       "z\n",
       "\" style=\"fill: #e5e5e5\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 46.050625 301.34175 \n",
       "L 46.050625 24.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m76d1015992\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76d1015992\" x=\"46.050625\" y=\"301.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(42.869375 315.940188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 99.137362 301.34175 \n",
       "L 99.137362 24.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76d1015992\" x=\"99.137362\" y=\"301.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 5000 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(86.412362 315.940188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(190.869141 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 152.224099 301.34175 \n",
       "L 152.224099 24.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76d1015992\" x=\"152.224099\" y=\"301.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 10000 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(136.317849 315.940188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(190.869141 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(254.492188 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 205.310836 301.34175 \n",
       "L 205.310836 24.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76d1015992\" x=\"205.310836\" y=\"301.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 15000 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(189.404586 315.940188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(190.869141 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(254.492188 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 258.397573 301.34175 \n",
       "L 258.397573 24.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76d1015992\" x=\"258.397573\" y=\"301.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 20000 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(242.491323 315.940188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(190.869141 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(254.492188 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 311.484309 301.34175 \n",
       "L 311.484309 24.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76d1015992\" x=\"311.484309\" y=\"301.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 25000 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(295.578059 315.940188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(190.869141 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(254.492188 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 364.571046 301.34175 \n",
       "L 364.571046 24.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76d1015992\" x=\"364.571046\" y=\"301.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 30000 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(348.664796 315.940188) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(190.869141 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(254.492188 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 417.657783 301.34175 \n",
       "L 417.657783 24.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m76d1015992\" x=\"417.657783\" y=\"301.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 35000 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(401.751533 315.940188) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(190.869141 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(254.492188 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- number_of_observations -->\n",
       "     <g style=\"fill: #555555\" transform=\"translate(167.539375 331.138) scale(0.12 -0.12)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(63.378906 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" transform=\"translate(126.757812 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" transform=\"translate(224.169922 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(287.646484 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(349.169922 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" transform=\"translate(390.283203 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(440.283203 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-66\" transform=\"translate(501.464844 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" transform=\"translate(536.669922 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(586.669922 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" transform=\"translate(647.851562 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(711.328125 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(763.427734 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(824.951172 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" transform=\"translate(866.064453 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(925.244141 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(986.523438 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(1025.732422 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(1053.515625 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(1114.697266 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(1178.076172 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 46.050625 288.74175 \n",
       "L 436.650625 288.74175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <defs>\n",
       "       <path id=\"m209849d7e7\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m209849d7e7\" x=\"46.050625\" y=\"288.74175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.0 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(23.1475 292.540969) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 46.050625 238.34175 \n",
       "L 436.650625 238.34175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m209849d7e7\" x=\"46.050625\" y=\"238.34175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.2 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(23.1475 242.140969) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 46.050625 187.94175 \n",
       "L 436.650625 187.94175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m209849d7e7\" x=\"46.050625\" y=\"187.94175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.4 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(23.1475 191.740969) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path d=\"M 46.050625 137.54175 \n",
       "L 436.650625 137.54175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m209849d7e7\" x=\"46.050625\" y=\"137.54175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.6 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(23.1475 141.340969) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path d=\"M 46.050625 87.14175 \n",
       "L 436.650625 87.14175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m209849d7e7\" x=\"46.050625\" y=\"87.14175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.8 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(23.1475 90.940969) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path d=\"M 46.050625 36.74175 \n",
       "L 436.650625 36.74175 \n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_28\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m209849d7e7\" x=\"46.050625\" y=\"36.74175\" style=\"fill: #555555; stroke: #555555; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 1.0 -->\n",
       "      <g style=\"fill: #555555\" transform=\"translate(23.1475 40.540969) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- unique_value -->\n",
       "     <g style=\"fill: #555555\" transform=\"translate(16.318125 202.706438) rotate(-90) scale(0.12 -0.12)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-71\" d=\"M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "M 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 -1331 \n",
       "L 2906 -1331 \n",
       "L 2906 525 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-75\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(63.378906 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(126.757812 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-71\" transform=\"translate(154.541016 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(218.017578 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(281.396484 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" transform=\"translate(342.919922 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-76\" transform=\"translate(392.919922 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(452.099609 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(513.378906 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(541.162109 0)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(604.541016 0)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 46.050625 288.74175 \n",
       "L 418.050625 288.74175 \n",
       "L 418.050625 263.54175 \n",
       "L 46.050625 263.54175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 46.050625 263.54175 \n",
       "L 46.050625 263.54175 \n",
       "L 46.050625 238.34175 \n",
       "L 46.050625 238.34175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 46.050625 238.34175 \n",
       "L 46.050625 238.34175 \n",
       "L 46.050625 213.14175 \n",
       "L 46.050625 213.14175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 46.050625 213.14175 \n",
       "L 46.050625 213.14175 \n",
       "L 46.050625 187.94175 \n",
       "L 46.050625 187.94175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 46.050625 187.94175 \n",
       "L 46.050625 187.94175 \n",
       "L 46.050625 162.74175 \n",
       "L 46.050625 162.74175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 46.050625 162.74175 \n",
       "L 46.050625 162.74175 \n",
       "L 46.050625 137.54175 \n",
       "L 46.050625 137.54175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 46.050625 137.54175 \n",
       "L 46.050625 137.54175 \n",
       "L 46.050625 112.34175 \n",
       "L 46.050625 112.34175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 46.050625 112.34175 \n",
       "L 46.050625 112.34175 \n",
       "L 46.050625 87.14175 \n",
       "L 46.050625 87.14175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 46.050625 87.14175 \n",
       "L 46.050625 87.14175 \n",
       "L 46.050625 61.94175 \n",
       "L 46.050625 61.94175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 46.050625 61.94175 \n",
       "L 152.50015 61.94175 \n",
       "L 152.50015 36.74175 \n",
       "L 46.050625 36.74175 \n",
       "z\n",
       "\" clip-path=\"url(#pcc5139d9ab)\" style=\"fill: #ff0000\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 46.050625 301.34175 \n",
       "L 46.050625 24.14175 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 436.650625 301.34175 \n",
       "L 436.650625 24.14175 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 46.050625 301.34175 \n",
       "L 436.650625 301.34175 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 46.050625 24.14175 \n",
       "L 436.650625 24.14175 \n",
       "\" style=\"fill: none; stroke: #ffffff; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- Target distribution -->\n",
       "    <g transform=\"translate(175.753 18.14175) scale(0.144 -0.144)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(44.583984 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(105.863281 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-67\" transform=\"translate(145.226562 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(208.703125 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(270.226562 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(309.435547 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(341.222656 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(404.699219 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-73\" transform=\"translate(432.482422 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(484.582031 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(523.791016 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(564.904297 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" transform=\"translate(592.6875 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(656.164062 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(719.542969 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(758.751953 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(786.535156 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(847.716797 0)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pcc5139d9ab\">\n",
       "   <rect x=\"46.050625\" y=\"24.14175\" width=\"390.6\" height=\"277.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = data['SeriousDlqin2yrs'].hist(orientation='horizontal', color='red')\n",
    "ax.set_xlabel(\"number_of_observations\")\n",
    "ax.set_ylabel(\"unique_value\")\n",
    "ax.set_title(\"Target distribution\")\n",
    "\n",
    "print('Distribution of target')\n",
    "data['SeriousDlqin2yrs'].value_counts()/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі виберемо назви всіх ознак з таблиці, крім прогнозованого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.479399800Z",
     "start_time": "2023-11-21T16:17:46.452061700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'NumberOfTime30-59DaysPastDueNotWorse',\n",
       " 'DebtRatio',\n",
       " 'NumberOfTimes90DaysLate',\n",
       " 'NumberOfTime60-89DaysPastDueNotWorse',\n",
       " 'MonthlyIncome',\n",
       " 'NumberOfDependents']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_columns_names = data.columns.values\n",
    "independent_columns_names = [x for x in data if x != 'SeriousDlqin2yrs']\n",
    "independent_columns_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Застосовуємо функцію, яка замінює всі NaN значення на медіанне значення відповідного стовпця."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.511519100Z",
     "start_time": "2023-11-21T16:17:46.468893500Z"
    }
   },
   "outputs": [],
   "source": [
    "table = delete_nan(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розділяємо підготовлений набір даних на незалежні ознаки ($X$) та цільову ознаку ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.528310200Z",
     "start_time": "2023-11-21T16:17:46.484920900Z"
    }
   },
   "outputs": [],
   "source": [
    "X = table[independent_columns_names]\n",
    "y = table['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.2. Бутстреп</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спершу обрахуйте інтервальну оцінку середнього віку (age) для надійних клієнтів та тих клієнтів, що прострочили виплату кредиту з \"впевненістю\" >= 90% . Використайте приклад з лекції, поставте `np.random.seed (0)`, як це зроблено в огляді."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.529309700Z",
     "start_time": "2023-11-21T16:17:46.499427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Допоміжна функція для генерації підвибірок за допомогою бутстрепу\n",
    "def get_bootstrap_samples(data, n_samples):\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[indices]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 1</span>\n",
    "\n",
    "У контексті проекту \"Бустрем\" (моніторингової платформи кредитного портфеля) на основі `credit_scoring_sample.csv` побудуйте серію варіантів. Усі варіанти починаються з розрахунку 90% довірчого інтервалу середнього віку (`age`) для надійних клієнтів (`SeriousDlqin2yrs == 0`) та тих, хто прострочив кредит (`SeriousDlqin2yrs == 1`) з довірчою ймовірністю >= 90% за прикладом з лекції, фіксуючи `np.random.seed(0)` перед кожною вибіркою. Далі \"Бустрем\" очікує різні підходи до доповнення цих базових оцінок: поглиблена інженерія, візуалізація, підгрупи та комунікація висновків.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Базова інтервальна оцінка:**\n",
    "\n",
    "- Мета: відтворити лекційний приклад і отримати 90% довірчі інтервали віку для обох груп.  \n",
    "- Кроки:  \n",
    "  1. Відфільтруйте `SeriousDlqin2yrs` і встановіть `np.random.seed(0)` перед відбором (наприклад, по 500 записів на групу).  \n",
    "  2. Обчисліть `mean`, `sem` і використайте `stats.t.ppf(0.95, df)` для меж інтервалу.  \n",
    "  3. Запишіть середні й межі в таблицю.  \n",
    "- Підказки: `scipy.stats.sem`, `stats.t.ppf`, переконайтеся, що `df = n - 1`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – Підтвердження стабільності seed:**\n",
    "\n",
    "- Мета: показати, що інтервали з `np.random.seed(0)` відтворювані та не змінюються без зміни seed.  \n",
    "- Кроки:  \n",
    "  1. Для надійних і дефолтних клієнтів фіксуйте `np.random.seed(0)` і відбирайте 300 записів із `np.random.choice` без заміни.  \n",
    "  2. Побудуйте 90% інтервали та повторіть ще двічі, щоб побачити, що значення не змінюються.  \n",
    "  3. Занотуйте, скільки разів межі ідентичні та коли змінився формат даних.  \n",
    "- Підказки: фіксуйте `np.random.seed(0)` перед кожним `choice`, зберігайте маску й результати.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Бутстрепівський інтервал:**\n",
    "\n",
    "- Мета: отримати 90% інтервал віку через бутстреп, що описує додаткову невизначеність.  \n",
    "- Кроки:  \n",
    "  1. За `np.random.seed(0)` згенеруйте 2000 бутстреп-реплік середнього віку для кожної групи (з поверненням).  \n",
    "  2. Порахуйтe 5-й і 95-й перцентили з масиву `boot_means`.  \n",
    "  3. Оцініть, наскільки ширші бутстреп-інтервали від класичних.  \n",
    "- Підказки: використайте `np.random.choice` для індексів та `np.percentile`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – Візуалізація інтервалів:**\n",
    "\n",
    "- Мета: створити графік із 90% інтервалами для кожної групи та пояснити, як \"Бустрем\" читає його.  \n",
    "- Кроки:  \n",
    "  1. З `np.random.seed(0)` виконайте звичайну вибірку, знайдіть інтервали.  \n",
    "  2. Зобразіть на `matplotlib` або `seaborn` середні з errorbar.  \n",
    "  3. Додайте пояснення, які вікові межі сигналізують про підвищений контроль.  \n",
    "- Підказки: `plt.errorbar`, горизонтальні лінії, додавайте текст з `ax.text`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Інтервали за квантилями віку:**\n",
    "\n",
    "- Мета: представити 90% інтервали як відрізки між 5-м і 95-м перцентилями.  \n",
    "- Кроки:  \n",
    "  1. Встановіть seed і збережіть вибірку, знайдіть 5-й та 95-й перцентили для `age`.  \n",
    "  2. Порівняйте ширину перцентильних інтервалів між групами.  \n",
    "  3. Додайте опис, чи перцентильний метод дає ті ж висновки.  \n",
    "- Підказки: `np.percentile`, `pd.Interval`, фіксуйте `n`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Вплив розміру вибірки:**\n",
    "\n",
    "- Мета: дослідити, як зростання `n` із 200 до 800 впливає на ширину 90% інтервалу.  \n",
    "- Кроки:  \n",
    "  1. З `np.random.seed(0)` зробіть вибірки 200, 400, 800 записів для кожної групи.  \n",
    "  2. Для кожного `n` підрахуйте margin of error і ширину інтервалу.  \n",
    "  3. Зобразіть залежність ширини від `n`.  \n",
    "- Підказки: `stats.sem`, `np.sqrt(n)`, запишіть формулу width = 2 * margin.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Імовірнісний нагляд \"Бустрем\":**\n",
    "\n",
    "- Мета: визначити, які вікові спостереження виходять за межі 90% інтервалів і можуть слугувати тригером.  \n",
    "- Кроки:  \n",
    "  1. Отримайте інтервали для обох груп із `np.random.seed(0)` та знайдіть порушників.  \n",
    "  2. Знайдіть клієнтів, вік яких менший або більший за межі (виведіть кількість).  \n",
    "  3. Пропишіть умову тригера для \"Бустрем\".  \n",
    "- Підказки: `np.where`, `DataFrame.query`, зберігайте `interval_lower`/`upper`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Логарифмічний масштаб віку:**\n",
    "\n",
    "- Мета: оцінити 90% інтервали після трансформації `np.log(age + 1)` і повернути вік назад.  \n",
    "- Кроки:  \n",
    "  1. З `np.random.seed(0)` зчитайте `log_age` та обчисліть інтервали для обох груп.  \n",
    "  2. Інтервали зворотніх перетворень (exponentiate).  \n",
    "  3. Порівняйте з класичними інтервалами і напишіть, чому трансформація допомагає.  \n",
    "- Підказки: `np.log1p`, `np.expm1`, зверніть увагу на зміщення смуги.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Інтервали для клієнтів із DebtRatio < 0.3:**\n",
    "\n",
    "- Мета: розрахувати 90% інтервали віку для груп з низьким борговим навантаженням.  \n",
    "- Кроки:  \n",
    "  1. Відфільтруйте за `DebtRatio < 0.3`, застосуйте `np.random.seed(0)` і вибірку.  \n",
    "  2. Обчисліть середні та інтервали, порівняйте з загальними.  \n",
    "  3. Додайте коментар, чи \"Бустрем\" може знижувати контроль для такої категорії.  \n",
    "- Підказки: `df.query`, `np.percentile`, `df['DebtRatio']`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – Утриманці та інтервали:**\n",
    "  \n",
    "- Мета: надати 90% інтервали віку для клієнтів з і без утриманців окремо.  \n",
    "- Кроки:  \n",
    "  1. Розбийте дані на `NumberOfDependents == 0` та `>= 1` для кожної групи при `np.random.seed(0)`.  \n",
    "  2. Обчисліть інтервали й порівняйте: де ширше.  \n",
    "  3. Додайте висновок для \"Бустрем\" щодо сімейного навантаження.  \n",
    "- Підказки: `groupby`, `stats.t.ppf`, зберігайте `dependents_group`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – Рівні довіри 85%, 90%, 95%:**\n",
    "\n",
    "- Мета: показати, як розширення довіри змінює інтервал віку.  \n",
    "- Кроки:  \n",
    "  1. Для `confidence` у списку `[0.85, 0.9, 0.95]` з `np.random.seed(0)` обчисліть margin.  \n",
    "  2. Побудуйте таблицю з шириною інтервалу для обох груп.  \n",
    "  3. Поясніть, який рівень рекомендує \"Бустрем\" для оперативних рішень.  \n",
    "- Підказки: `stats.t.ppf((1+conf)/2, df)`, `pd.DataFrame`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Сегментування за віком:**\n",
    "\n",
    "- Мета: окремо розрахувати 90% інтервали для клієнтів старших/молодших 60 років.  \n",
    "- Кроки:  \n",
    "  1. Додайте колонку `age_group = age > 60`, задай seed перед вибіркою.  \n",
    "  2. Для кожного сегмента та `SeriousDlqin2yrs` знайдіть інтервали.  \n",
    "  3. Чи відрізняються межі між сегментами, і що це каже \"Бустрем\".  \n",
    "- Підказки: `df.assign`, `groupby(['age_group','SeriousDlqin2yrs']).apply(...)`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – 100 бутстреп-сценаріїв:**\n",
    "\n",
    "- Мета: проілюструвати варіацію середнього віку через багатократний бутстреп.  \n",
    "- Кроки:  \n",
    "  1. За seed = 0 виконати 100 бутстрепів з `np.random.choice` (з заміною).  \n",
    "  2. Зберегти 90% інтервали кожного разу та посортувати за шириною.  \n",
    "  3. Показати, як часто інтервали перекриваються між групами.  \n",
    "- Підказки: використайте `np.random.seed(0)` лише на початку, а далі для відтворення можна зберегти `np.random.RandomState`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – Аналітика старших дефолтерів:**\n",
    "\n",
    "- Мета: знайти 5 найстарших дефолтерів і перевірити, чи вкладаються вони в 90% інтервал.  \n",
    "- Кроки:  \n",
    "  1. З `np.random.seed(0)` відберіть інтервали, знайдіть `nlargest(5)` за `age`.  \n",
    "  2. Порівняйте їхній вік із межами інтервалу, помітні аномалії.  \n",
    "  3. Підготуйте замітку для \"Бустрем\": які старші клієнти викликають підозру.  \n",
    "- Підказки: `nlargest`, `interval_lower/upper`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Інтервали для клієнтів з доходом понад медіану:**\n",
    "\n",
    "- Мета: оцінити 90% інтервал віку для клієнтів з доходом > median у кожній групі.  \n",
    "- Кроки:  \n",
    "  1. При `np.random.seed(0)` відфільтруйте записі з високим доходом.  \n",
    "  2. Обчисліть інтервали, порівняйте з загальною вибіркою.  \n",
    "  3. Запропонуйте, чи \"Бустрем\" може вважати таких клієнтів більш надійними.  \n",
    "- Підказки: `df['MonthlyIncome'].median()`, `mask`, `stats.t.ppf`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Сезонність і інтервали:**\n",
    "\n",
    "- Мета: подивитися, чи клієнти, які звертаються в першій половині року, мають відмінні 90% інтервали віку.  \n",
    "- Кроки:  \n",
    "  1. З `np.random.seed(0)` додайте колонку `month = np.random.randint(1, 13, size=len(df))`.  \n",
    "  2. Побудуйте інтервали для `month <= 6` та `>= 7` по обох групах.  \n",
    "  3. Опишіть, як \"Бустрем\" може реагувати на сезонні дисбаланси.  \n",
    "- Підказки: `np.random.randint`, `groupby`, `np.random.seed`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Інтервали за біном DebtRatio:**\n",
    "\n",
    "- Мета: для кожного біну `DebtRatio` ([0,0.3), [0.3,0.6), [0.6,2]) розрахувати 90% інтервали віку.  \n",
    "- Кроки:  \n",
    "  1. Створіть `debt_bin = pd.cut(DebtRatio, bins=[-0.01,0.3,0.6,2], labels=['low','medium','high'])`.  \n",
    "  2. Для кожного біна і `SeriousDlqin2yrs` з `np.random.seed(0)` обчисліть інтервали.  \n",
    "  3. Поясніть, як \"Бустрем\" може використовувати ці розбивки для рівнів контролю.  \n",
    "- Підказки: `groupby`, `agg`, `stats.t.ppf`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Інтервали на тестовій частині:**\n",
    "\n",
    "- Мета: розділити дані на `train/test`, побудувати 90% інтервали віку саме на `test`.  \n",
    "- Кроки:  \n",
    "  1. `train_test_split` (70/30) зі `stratify`, `random_state=0`.  \n",
    "  2. На test-set для обох груп з `np.random.seed(0)` розрахуйте інтервали.  \n",
    "  3. Порівняйте з інтервалами train і повідомте, чи вони стабільні.  \n",
    "- Підказки: `train_test_split`, `stratify=y`, `np.random.seed(0)` перед `sample`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – Інтервали для DebtRatio > 1:**\n",
    "\n",
    "- Мета: оцінити, чи клієнти з високим `DebtRatio` мають ширші 90% інтервали віку.  \n",
    "- Кроки:  \n",
    "  1. Відфільтруйте на `DebtRatio > 1` і, встановивши `np.random.seed(0)`, зробіть вибірку.  \n",
    "  2. Обчисліть інтервали для надійних/дефолтних клієнтів.  \n",
    "  3. Подайте висновки для \"Бустрем\": чи потрібно посилити контроль.  \n",
    "- Підказки: `df.loc`, `np.random.choice`, `stats.sem`.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Комунікація висновків:**\n",
    "\n",
    "- Мета: сформулювати короткий бриф для \"Бустрем\" зі 90% інтервалами і практичними рекомендаціями.  \n",
    "- Кроки:  \n",
    "  1. Отримайте інтервали за допомогою `np.random.seed(0)`.  \n",
    "  2. Складіть таблицю (mean, lower, upper, ширина) для обох груп.  \n",
    "  3. Додайте три ключові рекомендації: кого додатково перевіряти, кого залишити.  \n",
    "- Підказки: `pd.DataFrame`, `np.random.seed`, вказуйте `n` і `confidence`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-5.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.3. Підбір параметрів для ансамблевих моделей</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однією з важливих метрик якості моделі є значення площі під [ROC-кривої](https://uk.wikipedia.org/wiki/ROC-%D0%BA%D1%80%D0%B8%D0%B2%D0%B0) (AUC). Значення ROC-AUC лежить від 0 до 1. Чим ближче значення метрики ROC-AUC до 1, тим якісніше відбувається класифікація моделлю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.701699Z",
     "start_time": "2023-11-21T16:17:46.515032500Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Використовуємо модуль [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) для побудови логістичної регресії. Через незбалансованість класів в цільовій функції додаємо параметр балансування. Також додамо параметр `random_state=5` для відтворюваності результатів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.729754900Z",
     "start_time": "2023-11-21T16:17:46.705719500Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=5, class_weight= 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер спробуємо підібрати найкращий коефіцієнт регуляризації (коефіцієнт C в логістичній регресії) для моделі логістичної регрессії. Найкраще значення параметра С забезпечить нам оптимальність моделі, яка буде добре прогнозувати значення цільової функції і водночас не буде перенавчатися. Інші параметри залишаємо за замовчуванням."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.742773400Z",
     "start_time": "2023-11-21T16:17:46.721237200Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'C': (0.0001, 0.001, 0.01, 0.1, 1, 10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоби підібрати коефіцієнт регуляризації переглянемо значення `ROC-AUC` на Stratified крос-валідації з 5 фолдів для кожного можливого значення коефіцієнта регуляризації за допомогою класу [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.764666Z",
     "start_time": "2023-11-21T16:17:46.735272400Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі для розв'язку наступного завдання використайте [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) з метрикою `ROC-AUC` за параметром C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 2</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – глибина дерев у бэггінгу:**  \n",
    "- Мета: знайти значення max_depth для базових дерев, яке забезпечує найкращий баланс між відтворюваністю та якістю виявлення ризикованих позичальників на credit_scoring_sample.csv.  \n",
    "- Кроки:  \n",
    "  1. Завантажте таблицю, замініть пропущені MonthlyIncome на медіану, відберіть числові ознаки (age, DebtRatio, MonthlyIncome, NumberOfTimes90DaysLate, NumberOfDependents, NumberOfTime30-59DaysPastDueNotWorse, NumberOfTime60-89DaysPastDueNotWorse).  \n",
    "  2. Оцініть базову LogisticRegression(random_state=5, class_weight='balanced') через StratifiedKFold(n_splits=5, shuffle=True, random_state=5) і зафіксуйте ROC-AUC.  \n",
    "  3. Створіть BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=100, bootstrap=True, max_samples=0.8) і застосуйте GridSearchCV зі scoring='roc_auc' для значень max_depth = [3, 5, 7, 9, 12].  \n",
    "  4. Побудуйте графік середнього ROC-AUC від глибини дерева та порівняйте з базовою логістичною регресією.  \n",
    "- Підказки: скористайтеся `np.median` для імпутації, `StratifiedKFold` гарантує баланс класів, а `classification_report` запропонує додаткову перевірку на `test_size=0.2`.  \n",
    "- Порівняння з логістичною регресією: покажіть, чи Bagging з оптимальною глибиною підвищує ROC-AUC, і поясніть вплив max_depth.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – кількість дерев у бэггінгу:**  \n",
    "- Мета: виявити, скільки базових дерев (`n_estimators`) потрібно для стабільної ROC-AUC, враховуючи `credit_scoring_sample.csv` і незначний дисбаланс.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте ознаки та цільову змінну, замість `MonthlyIncome` використайте стандартизований залишок `MonthlyIncome / age`.  \n",
    "  2. Збережіть результати ROC-AUC з базової логістичної регресії та використайте `StratifiedKFold`.  \n",
    "  3. Візьміть `BaggingClassifier` із `DecisionTreeClassifier(max_depth=6, random_state=5)` і запустіть `GridSearchCV` на `n_estimators = [50, 100, 200, 400]` при `max_samples=0.75`.  \n",
    "  4. Оцініть швидкість навчання й дисперсію крос-валідації, виберіть найкращий `n_estimators`.  \n",
    "- Підказки: `GridSearchCV` з `return_train_score=True` дозволить побачити перенавчання, а `np.log1p(DebtRatio)` зменшить вплив великих значень.  \n",
    "- Порівняння з логістичною регресією: покажіть, чи додаткові дерева дають ефект у ROC-AUC і наскільки довше обчислення.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – мінімальна кількість зразків для поділу:**  \n",
    "- Мета: налаштувати min_samples_split, щоб уникнути глибоких дефектних вузлів, створивши додаткові ознаки, які описують історію прострочень.  \n",
    "- Кроки:  \n",
    "  1. Сформуйте ознаку payment_history = NumberOfTimes90DaysLate + NumberOfTime30-59DaysPastDueNotWorse, нормалізуйте її через MinMaxScaler на рівні (0, 1), а також додайте individual_debt_ratio = DebtRatio / (NumberOfDependents + 1).  \n",
    "  2. Програйте LogisticRegression(random_state=5, class_weight='balanced') як базову модель, збережіть метрику `roc_auc_score` на `StratifiedKFold(n_splits=5)`.  \n",
    "  3. Побудуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=120, bootstrap=True) і в GridSearchCV перевірте min_samples_split у [2, 4, 6, 8, 10].  \n",
    "  4. Проаналізуйте, чи зростання min_samples_split зменшує варіанс та наскільки це співвідноситься із новою ознакою payment_history.  \n",
    "- Підказки: використайте Pipeline або окрему трансформацію, щоб MinMaxScaler не зашумлював решту ознак, а scoring='roc_auc' дозволить зберегти фокус на якості класифікації.  \n",
    "- Порівняння з логістичною регресією: покажіть, як зміна min_samples_split впливає на ROC-AUC порівняно з базовим алгоритмом.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – кількість ознак в кожному дереві:**  \n",
    "- Мета: підібрати max_features, щоб кожне дерево бачило достатньо, але не надто багато ознак, у моделі на credit_scoring_sample.csv.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте набір стандартних ознак, включно зі скольованим DebtRatio, MonthlyIncome та агрегатами NumberOfTime60-89DaysPastDueNotWorse і NumberOfTimes90DaysLate.  \n",
    "  2. Оцініть LogisticRegression(random_state=5, class_weight='balanced') як опорну лінію.  \n",
    "  3. Побудуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=8, random_state=5), bootstrap=True, n_estimators=150) і застосуйте GridSearchCV для max_features = ['sqrt', 'log2', 0.5, 0.75, 1.0].  \n",
    "  4. Візуалізуйте ROC-AUC щодо max_features і виберіть найстабільніший варіант.  \n",
    "- Підказки: поєднуйте шукач з `StratifiedKFold`, використовуйте `ColumnTransformer` для числового скейлінгу, а `return_train_score=True` допоможе помітити перенавчання.  \n",
    "- Порівняння з логістичною регресією: опишіть, чи зменшення max_features підвищує узагальнення порівняно з одиночним логістичним класифікатором.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – випадкова вибірка ознак:**  \n",
    "- Мета: визначити, як вмикання/вимикання bootstrap_features змінює якість ансамблю при роботі з дисбалансною вибіркою.  \n",
    "- Кроки:  \n",
    "  1. Зберіть ознаки, додайте логарифм `np.log1p(MonthlyIncome)` та `DebtRatio`, а пропуски заповніть медіаною.  \n",
    "  2. Зафіксуйте LogisticRegression(random_state=5, class_weight='balanced') як референс.  \n",
    "  3. Ініціалізуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=7, random_state=5), n_estimators=120, max_samples=0.8) і проведіть GridSearchCV по bootstrap_features = [True, False] і max_features = [0.5, 0.7, 'sqrt'].  \n",
    "  4. Порівняйте ROC-AUC, precision, `recall` та оцініть, наскільки часто bootstrap_features=True допомагає уникнути однотипних дерев.  \n",
    "- Підказки: bootstrap_features контролює, чи змінюються ознаки для кожного дерева, тому поєднання з різними max_features дає інсайти щодо «сирої» або «збалансованої» інформації.  \n",
    "- Порівняння з логістичною регресією: наочно покажіть, в яких ділянках простору ознак bootstrap_features=True дає перевагу.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – частка вибірки для кожного дерева:**  \n",
    "- Мета: підібрати max_samples, щоб баланс між різноманітністю та інформацією про кредитну історію давав найвищий ROC-AUC.  \n",
    "- Кроки:  \n",
    "  1. Завантажте дані, заповніть MonthlyIncome медіаною та додайте credit_age = age / (NumberOfDependents + 1) як нову ознаку.  \n",
    "  2. Засвойте базовий LogisticRegression(random_state=5, class_weight='balanced') і порівняйте ROC-AUC.  \n",
    "  3. Візьміть BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=7, random_state=5), n_estimators=200, bootstrap=True) та проведіть GridSearchCV з max_samples = [0.4, 0.6, 0.8, 1.0] при max_features=0.7.  \n",
    "  4. Оцініть, чи обмеження вибірки призводить до стабільнішої оцінки ROC-AUC, і порівняйте з результатами логістичної регресії.  \n",
    "- Підказки: max_samples можна вказати як частку (`float`) або абсолютну кількість; зберігайте однаковий `random_state` для реплікованості.  \n",
    "- Порівняння з логістичною регресією: як зміна max_samples впливає на розрив між Bagging та логістичною моделлю?  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – оцінка позавибірковими зразками:**  \n",
    "- Мета: використати oob_score=True, щоб підібрати max_depth і порівнювати оцінку з ROC-AUC на крос-валідації.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте дані, імпутуйте MonthlyIncome, а DebtRatio розбийте на квантилі за допомогою pd.qcut.  \n",
    "  2. Змоделюйте LogisticRegression(random_state=5, class_weight='balanced'), зареєструйте середній ROC-AUC.  \n",
    "  3. Створіть BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=150, bootstrap=True, oob_score=True) і за допомогою GridSearchCV переберіть max_depth = [5, 7, 9, 11].  \n",
    "  4. Для кожного значення проаналізуйте oob_score_ та cv_results_['mean_test_score'], поясніть розбіжності.  \n",
    "- Підказки: oob_score за умов bootstrap=True надає «безкоштовну» оцінку, але її варто порівнювати з StratifiedKFold.  \n",
    "- Порівняння з логістичною регресією: покажіть, чи oob_score дає стимул до кращого ROC-AUC порівняно з логістичною межею.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – кількість зразків у листі:**  \n",
    "- Мета: обмежити min_samples_leaf, щоб запобігти надто дрібним листам і одночасно зберегти знання про клієнтів із високим DebtRatio.  \n",
    "- Кроки:  \n",
    "  1. Додайте ознаки DebtRatio_log = np.log1p(DebtRatio) і late_payments = NumberOfTimes90DaysLate + NumberOfTime60-89DaysPastDueNotWorse, заповніть пропуски MonthlyIncome.  \n",
    "  2. Порівняйте ROC-AUC базової LogisticRegression(random_state=5, class_weight='balanced') із первинною метрикою.  \n",
    "  3. Встановіть BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=140, bootstrap=True) та проведіть GridSearchCV з min_samples_leaf = [1, 3, 5, 7, 10] і max_depth=9.  \n",
    "  4. Продемонструйте, як min_samples_leaf впливає на кількість листових кінців та ROC-AUC.  \n",
    "- Підказки: для візуалізації скористайтесь pd.Series(grid.cv_results_['mean_test_score'], index=grid.param_grid['min_samples_leaf']), а std_test_score підкаже про стабільність.  \n",
    "- Порівняння з логістичною регресією: поясніть різницю в перевагах між регуляризацією в логістиці та контрольованим листом у деревах.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – вибір критерію поділу:**  \n",
    "- Мета: з'ясувати, який критерій (gini або entropy) краще показує ризик дефолту, та чи варто поєднати це з min_samples_split.  \n",
    "- Кроки:  \n",
    "  1. Сформуйте ознаки, додайте late_rate = NumberOfTimes90DaysLate / (age + 1) і \n",
    "p.log1p(DebtRatio), імпутуйте MonthlyIncome.  \n",
    "  2. Отримайте базовий ROC-AUC від LogisticRegression(random_state=5, class_weight='balanced').  \n",
    "  3. Ініціалізуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=130, bootstrap=True) та при GridSearchCV(scoring='roc_auc') переберіть base_estimator__criterion = ['gini', 'entropy'] та base_estimator__min_samples_split = [2, 4, 6].  \n",
    "  4. Проаналізуйте, чи entropy скорочує сумарну невизначеність більше, ніж gini, і як це впливає на ROC-AUC.  \n",
    "- Підказки: BaggingClassifier передає параметри базовому естиматору через base_estimator__, тож саме так формуються param_grid.  \n",
    "- Порівняння з логістичною регресією: порівняйте, наскільки зміна критерію підсилює перевагу ансамблю над логістикою.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – випадковий пошук глибини та кількості дерев:**  \n",
    "- Мета: вирішити, чи швидкий RandomizedSearchCV дає порівнянні або кращі результати, ніж повний перебір, підбираючи одночасно \n",
    "_estimators і max_depth.  \n",
    "- Кроки:  \n",
    "  1. Завантажте дані, імпутуйте MonthlyIncome, відкиньте NumberOfDependents із NaN, а DebtRatio зробіть \n",
    "p.log1p.  \n",
    "  2. Оцініть базову LogisticRegression(random_state=5, class_weight='balanced') за \n",
    "oc_auc_score.  \n",
    "  3. Ініціалізуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), bootstrap=True) і запустіть RandomizedSearchCV (n_iter=20, cv=StratifiedKFold(5)) для max_depth у [3, 5, 7, 9, 11, 13] та \n",
    "_estimators у [50, 100, 150, 200, 250, 300].  \n",
    "  4. Порівняйте best_score_ з базовою логістичною регресією та зафіксуйте час пошуку.  \n",
    "- Підказки: встановіть \n",
    "p.random.seed(5) перед RandomizedSearchCV, а також \n",
    "eturn_train_score=True, щоби бачити розрив між навчанням і валідацією.  \n",
    "- Порівняння з логістичною регресією: прикріпіть ROC-AUC від обох моделей та прокоментуйте, чи випадковий пошук обійшов детермінований перебір.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – пайплайн із масштабуванням та поліномами:**  \n",
    "- Мета: додати трансформації до ознак (MonthlyIncome, DebtRatio) перед ансамблем та підібрати одночасно degree у PolynomialFeatures і max_depth дерев.  \n",
    "- Кроки:  \n",
    "  1. Створіть ColumnTransformer, який масштабує числові ознаки через StandardScaler, а до DebtRatio і MonthlyIncome додає PolynomialFeatures(include_bias=False).  \n",
    "  2. Порівняйте з базовою LogisticRegression(random_state=5, class_weight='balanced') для контрольної точки ROC-AUC.  \n",
    "  3. Вбудуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), bootstrap=True) в Pipeline і застосуйте GridSearchCV з параметрами polynomial__degree = [1, 2] і bagging__max_depth = [5, 7, 9].  \n",
    "  4. Проаналізуйте, як підвищення degree змінює ROC-AUC та порівняйте час навчання з логістичною регресією.  \n",
    "- Підказки: використовуйте префікси pipeline.set_params(polynomial__degree=...) у GridSearchCV, StratifiedKFold гарантує стабільну оцінку, а Pipeline захищає від витоку даних.  \n",
    "- Порівняння з логістичною регресією: чи трансформація ознак додає ефект порівняно з простою логістичною регресією?  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – обмеження кількості листових вузлів:**  \n",
    "- Мета: підібрати max_leaf_nodes для DecisionTreeClassifier у складі BaggingClassifier, щоби уникнути дуже розгалужених дерев.  \n",
    "- Кроки:  \n",
    "  1. Створіть ознаки Debt_per_dependant = DebtRatio / (NumberOfDependents + 1) та income_per_age = MonthlyIncome / (age + 1), імпутуйте MonthlyIncome.  \n",
    "  2. Зафіксуйте ROC-AUC базової LogisticRegression(random_state=5, class_weight='balanced').  \n",
    "  3. Запустіть GridSearchCV(scoring='roc_auc') для BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), bootstrap=True, n_estimators=150) із max_leaf_nodes = [10, 20, 40, 80, 160, None].  \n",
    "  4. Порівняйте mean_test_score для кожного max_leaf_nodes і опишіть, де модель стає стабільною.  \n",
    "- Підказки: max_leaf_nodes=None дозволяє дереву розростатися, тому у GridSearchCV слід подивитися на сумісний std_test_score.  \n",
    "- Порівняння з логістичною регресією: розкажіть, де контроль кількості листів дає перевагу над регуляризацією логістичного оцінювання.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – стабільність випадковості:**  \n",
    "- Мета: підібрати \n",
    "andom_state, який забезпечує стабільний ROC-AUC, водночас налаштувавши \n",
    "_estimators.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте базу ознак, імпутуючи MonthlyIncome, і зафіксуйте LogisticRegression(random_state=5, class_weight='balanced').  \n",
    "  2. Запустіть GridSearchCV над BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=8), bootstrap=True, n_estimators=150) із параметрами \n",
    "andom_state = [0, 5, 42, 99] та \n",
    "_estimators = [100, 150, 200], scoring='roc_auc'.  \n",
    "  3. Додатково проведіть повтори cross_val_score для найкращої комбінації з різними \n",
    "andom_state, щоби оцінити розкид.  \n",
    "  4. Оцініть, чи більш стабільна модель із певним \n",
    "andom_state має меншу дисперсію, ніж логістична регресія.  \n",
    "- Підказки: grid.best_params_ допоможе відтворити найкращий \n",
    "andom_state, а \n",
    "p.random.seed гарантує, що ви порівнюєте однакові інтерпретації.  \n",
    "- Порівняння з логістичною регресією: покажіть, яка модель менше реагує на зміну \n",
    "andom_state.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – поступове додавання дерев через warm_start:**  \n",
    "- Мета: використати warm_start=True, щоби поетапно додавати дерева й знайти момент стабілізації ROC-AUC.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте дані, імпутуйте пропуски, масштабовуйте MonthlyIncome та DebtRatio.  \n",
    "  2. Ініціалізуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=8, random_state=5), bootstrap=True, warm_start=True) і прописуйте цикл, де кожна ітерація додає по 20 дерев (\n",
    "_estimators += 20).  \n",
    "  3. Після кожного кроку вимірюйте ROC-AUC на StratifiedKFold(n_splits=5) і порівняйте з LogisticRegression(random_state=5, class_weight='balanced').  \n",
    "  4. Змоніторте точку насичення (коли додавання дерев дає <0.005 приросту) та збережіть найкращу конфігурацію.  \n",
    "- Підказки: warm_start дозволяє не навчати заново всі дерева, але пам'ятайте про oob_score=False, інакше доведеться перезапускати.  \n",
    "- Порівняння з логістичною регресією: опишіть, чи поступовий додавання дерев прямує до ROC-AUC, близького чи кращого за логістичну модель.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – зразки з повторенням або без:**  \n",
    "- Мета: порівняти bootstrap=True та False, а також підібрати max_samples, аби зрозуміти, чи заміна зразків допомагає боротися з дисбалансом.  \n",
    "- Кроки:  \n",
    "  1. Створіть ознаки has_dependents = (NumberOfDependents > 0).astype(int) і debt_to_income = DebtRatio * 100, імпутуйте MonthlyIncome.  \n",
    "  2. Порахуйте ROC-AUC для LogisticRegression(random_state=5, class_weight='balanced').  \n",
    "  3. Встановіть BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=150) і в GridSearchCV переберіть bootstrap = [True, False] разом із max_samples = [0.6, 0.8, 1.0].  \n",
    "  4. Поясніть, чи відмова від повторної вибірки дає вищу чи нижчу дисперсію, порівняно з логістичною регресією.  \n",
    "- Підказки: коли bootstrap=False, max_samples має бути 1.0 або відкидати, натомість bootstrap=True дозволяє зменшити вплив екстремальних записів.  \n",
    "- Порівняння з логістичною регресією: хто краще справляється з новими складними зразками – Bagging із bootstrap=True чи логістична модель?  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – мінімальна фракція ваги в листі:**  \n",
    "- Мета: настроїти min_weight_fraction_leaf, щоб врахувати, що клієнти з вищими доходами мають більший внесок у загальний ризик.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте sample_weight = np.where(MonthlyIncome.isna(), 0.5, 1.0) і доповніть її ознакою income_bucket = pd.qcut(MonthlyIncome.fillna(MonthlyIncome.median()), 4, labels=False).  \n",
    "  2. Перевірте LogisticRegression з class_weight='balanced' як базу.  \n",
    "  3. Вбудуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=150, bootstrap=True) і здійсніть GridSearchCV з min_weight_fraction_leaf = [0.0, 0.01, 0.02, 0.05], передаючи sample_weight у it.  \n",
    "  4. Порівняйте ROC-AUC при різних фракціях, опишіть, як це впливає на клієнтів із низьким і високим доходом.  \n",
    "- Підказки: min_weight_fraction_leaf потрібен, коли приклади мають неоднакову важливість; передавайте sample_weight до it, щоби DecisionTreeClassifier зміг використати їх.  \n",
    "- Порівняння з логістичною регресією: покажіть, чи градаційне відсіювання листів дає кращу якість, ніж class_weight='balanced'.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – мінімальне зменшення варіабельності:**  \n",
    "- Мета: налаштувати min_impurity_decrease, щоб дерево розгалужувалося лише тоді, коли спостерігається достатній приріст інформації.  \n",
    "- Кроки:  \n",
    "  1. Створіть нові ознаки late_ratio = NumberOfTimes90DaysLate / (NumberOfTime30-59DaysPastDueNotWorse + 1) та income_to_debt = MonthlyIncome / (DebtRatio + 0.01).  \n",
    "  2. Оцініть LogisticRegression(random_state=5, class_weight='balanced') з \n",
    "oc_auc_score.  \n",
    "  3. Застосуйте GridSearchCV до BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=140, bootstrap=True) для min_impurity_decrease = [0.0, 0.0001, 0.0005, 0.001, 0.005].  \n",
    "  4. Відстежте, як зростання min_impurity_decrease змінює mean_test_score, і поясніть, чи модель починає пропускати важливі розбивки.  \n",
    "- Підказки: min_impurity_decrease пропорційно зменшує різницю між перед і після розбиття; GridSearchCV покаже, де ROC-AUC найвищий без надмірного розгалуження.  \n",
    "- Порівняння з логістичною регресією: порівняйте, наскільки min_impurity_decrease знижує варіацію порівняно з лінійною моделлю.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – баггінг логістичних регресій:**  \n",
    "- Мета: обкатати BaggingClassifier із base_estimator=LogisticRegression і підібрати параметри регуляризації (C) одночасно з \n",
    "_estimators.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте дані зі стандартизованими MonthlyIncome та DebtRatio, замініть NaN усіх числових ознак на медіану і додайте has_dependents.  \n",
    "  2. Виміряйте ROC-AUC для одиночної LogisticRegression(random_state=5, class_weight='balanced') із C=1.0.  \n",
    "  3. Ініціалізуйте BaggingClassifier(base_estimator=LogisticRegression(random_state=5, solver='lbfgs', max_iter=200), bootstrap=True) і проведіть GridSearchCV для \n",
    "_estimators = [50, 100, 150] та base_estimator__C = [0.01, 0.1, 1, 10].  \n",
    "  4. Порівняйте результати з базовою логістичною моделлю та з’ясуйте, чи ансамбль з ремодульованою регуляризацією дає вищий ROC-AUC.  \n",
    "- Підказки: BaggingClassifier передає параметри до base_estimator через префікс base_estimator__, а sample_weight можна зберігати, щоб врахувати вклад високодохідних клієнтів.  \n",
    "- Порівняння з логістичною регресією: покажіть, як ансамбль логістичних регресій змінив ROC-AUC та precision, і чим це відрізняється від одиночної моделі.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – стекінг баггінгів та логістики:**  \n",
    "- Мета: побудувати StackingClassifier, де рівень-0 складається з двох налаштованих BaggingClassifier, а мета-естиматор — LogisticRegression, і підібрати ваги/параметри для базових моделей.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте дані із заповненням пропусків, стандартним скейлінгом та додатковою ознакою debt_income_ratio.  \n",
    "  2. Побудуйте два базових BaggingClassifier: перший із max_depth=6, другий із max_features=0.6, обидва з \n",
    "_estimators=100.  \n",
    "  3. Ініціалізуйте StackingClassifier(estimators=[('bag1', bag1), ('bag2', bag2)], final_estimator=LogisticRegression(class_weight='balanced')) і застосуйте GridSearchCV до параметрів bag1__n_estimators, bag2__max_depth та inal_estimator__C.  \n",
    "  4. Порівняйте ROC-AUC стеку з окремими BaggingClassifier та базовою логістичною моделлю.  \n",
    "- Підказки: використайте passthrough=True, щоби логістика бачила вихідні ознаки, а cv=StratifiedKFold(5) з shuffle=True гарантує стабільність.  \n",
    "- Порівняння з логістичною регресією: поясніть, наскільки стек покращив метрику порівняно з одиночним логістичним мета-естиматором.  \n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – багатокритерійна оптимізація:**  \n",
    "- Мета: одночасно контролювати ROC-AUC та Recall через GridSearchCV з декількома скорінгами і підібрати max_depth та max_samples.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте ознаки, імпутуючи MonthlyIncome та NumberOfDependents, а також додайте \n",
    "ecent_miss_rate = NumberOfTime30-59DaysPastDueNotWorse / (age + 1).  \n",
    "  2. Виміряйте ROC-AUC для базової LogisticRegression(random_state=5, class_weight='balanced').  \n",
    "  3. Налаштуйте BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=5), n_estimators=150, bootstrap=True) і проведіть GridSearchCV із scoring={'roc_auc':'roc_auc', 'recall':'recall'}, refit='roc_auc' для max_depth=[5,7,9] та max_samples=[0.6,0.8,1.0].  \n",
    "  4. Проаналізуйте trade-off між \n",
    "oc_auc та \n",
    "ecall, поясніть, чому \n",
    "efit='roc_auc' залишає Recall другим критерієм у звіті.  \n",
    "- Підказки: grid.cv_results_ містить окремі метрики (mean_test_recall, mean_test_roc_auc), їх можна відобразити графічно.  \n",
    "- Порівняння з логістичною регресією: покажіть, чи Bagging дозволяє підвищити Recall, не жертвуючи ROC-AUC.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-5.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.4. Визначення впливу ознак</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У цьому завданні ми попрацюємо з важливістю ознак. Пам'ятаємо, що важливість ознаки визначається **абсолютним значенням її коефіцієнта**. Крім того, потрібно заздалегідь нормалізувати всі ознаки, щоб їх правильно порівняти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.813337800Z",
     "start_time": "2023-11-21T16:17:46.755158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.001, class_weight=&#x27;balanced&#x27;, random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;balanced&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight='balanced', random_state=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lr = LogisticRegression(C=0.001,random_state=5, class_weight='balanced')\n",
    "scal = StandardScaler()\n",
    "lr.fit(scal.fit_transform(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрахуємо важливість ознак:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.828858600Z",
     "start_time": "2023-11-21T16:17:46.819349800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NumberOfTime30-59DaysPastDueNotWorse</td>\n",
       "      <td>0.723421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NumberOfTimes90DaysLate</td>\n",
       "      <td>0.516656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NumberOfTime60-89DaysPastDueNotWorse</td>\n",
       "      <td>0.195486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NumberOfDependents</td>\n",
       "      <td>0.101722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DebtRatio</td>\n",
       "      <td>-0.024068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MonthlyIncome</td>\n",
       "      <td>-0.163015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>-0.417115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feat      coef\n",
       "1  NumberOfTime30-59DaysPastDueNotWorse  0.723421\n",
       "3               NumberOfTimes90DaysLate  0.516656\n",
       "4  NumberOfTime60-89DaysPastDueNotWorse  0.195486\n",
       "6                    NumberOfDependents  0.101722\n",
       "2                             DebtRatio -0.024068\n",
       "5                         MonthlyIncome -0.163015\n",
       "0                                   age -0.417115"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'feat': independent_columns_names,\n",
    "              'coef': lr.coef_.flatten().tolist()}).sort_values(by='coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 3</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Еластичність логіт-коефіцієнтів за віком і DebtRatio:**  \n",
    "- Мета: порівняти, наскільки чутливо змінюються шанси прострочення при модифікації `age` та `DebtRatio` у стандартизованій логістичній регресії.  \n",
    "- Кроки:  \n",
    "  1. Очистіть `credit_scoring_sample.csv`, замініть пропуски медіанами та стандартизуйте числові стовпці.  \n",
    "  2. Навчіть `LogisticRegression(penalty='none', class_weight='balanced')`, зафіксуйте коефіцієнти та довірчі інтервали через бутстреп ≥1000 ітерацій.  \n",
    "  3. Обчисліть еластичність `∂logit/∂feature × feature/odds` для `age` та `DebtRatio` у всій вибірці й окремо для клієнтів із `MonthlyIncome` нижче та вище медіани.  \n",
    "- Підказки: `StandardScaler`, `sklearn.utils.resample`, логіт = log(p/(1-p)). Візуалізуйте еластичності бокс-плотами.  \n",
    "- Інтерпретація: поясніть, який вік/рівень боргу дає найбільший приріст odds прострочення та чому.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – SHAP-карти взаємодій RandomForest:**  \n",
    "- Мета: дослідити спільний вплив `MonthlyIncome` та `NumberOfTimes90DaysLate` у моделі `RandomForestClassifier`.  \n",
    "- Кроки:  \n",
    "  1. Збалансуйте вибірку SMOTE або undersampling’ом, навчіть `RandomForestClassifier(n_estimators=500, max_depth=None, random_state=42)`.  \n",
    "  2. Обчисліть `TreeExplainer` SHAP-значення й побудуйте interaction plot для двох ознак.  \n",
    "  3. Розбийте клієнтів на квартилі доходу та опишіть, як взаємодія з частотою 90-day late змінює прогноз усередині кожного сегмента.  \n",
    "- Підказки: `shap.dependence_plot(feature, shap_values, X, interaction_index=...)`. Перед обчисленням SHAP усуньте корельовані змінні (>0.85).  \n",
    "- Перевірка: переконайтеся, що глобальна важливість SHAP узгоджується з `feature_importances_` (кореляція >0.7).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Перестановочна важливість зі стабільністю:**  \n",
    "- Мета: виміряти, наскільки стабільний вплив ознак при 30 перезапусках k-fold для двох моделей (логістичної регресії й випадкового лісу).  \n",
    "- Кроки:  \n",
    "  1. Використайте `StratifiedKFold(n_splits=5, shuffle=True)` та для кожного спліту тренуйте обидві моделі на тих самих даних.  \n",
    "  2. Для кожного запуску рахуйте permutation importance (`sklearn.inspection.permutation_importance`, scoring='roc_auc').  \n",
    "  3. Побудуйте ранжування ознак за медіаною важливості та коефіцієнтом варіації. Порівняйте топ-5 у двох моделях через коефіцієнт Спірмена.  \n",
    "- Підказки: зберігайте результати у `long`-таблиці (model, feature, importance, run).  \n",
    "- Інтерпретація: поясніть, де саме RandomForest знаходить додаткову інформацію порівняно з лінійною моделлю.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – 2D partial dependence + ICE для боргу й віку:**  \n",
    "- Мета: дослідити поверхню ризику прострочення залежно від `DebtRatio` та `age`.  \n",
    "- Кроки:  \n",
    "  1. Тренуйте `GradientBoostingClassifier` як додаткову еталонну модель, але основний аналіз робіть на `RandomForest`.  \n",
    "  2. Побудуйте `PartialDependenceDisplay.from_estimator(..., ['DebtRatio', 'age'])` та 20 випадкових ICE-кривих для `DebtRatio`.  \n",
    "  3. Виділіть області, де PD і ICE розходяться >5 п.п. й дайте пояснення (наприклад, різні сегменти доходу).  \n",
    "- Підказки: масштабування не обов’язкове для дерев, але обріжте екстремальні `DebtRatio` (Winzorization 1%).  \n",
    "- Візуалізація: додайте контурні лінії ризику та маркери середніх значень сегментів.  \n",
    "- Інтерпретація: сформулюйте бізнес-правила (наприклад, “високий вік + високий DebtRatio = зона високого ризику”).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Кумулятивний локальний ефект (ALE) для `MonthlyIncome`:**  \n",
    "- Мета: зрозуміти, як зміна доходу впливає на прогноз логістичної регресії без припущення про лінійність.  \n",
    "- Кроки:  \n",
    "  1. Побудуйте `Pipeline(StandardScaler → LogisticRegression(C=0.1, penalty='l2'))`.  \n",
    "  2. Використайте бібліотеку `alibi` або власну імплементацію для 2-го порядку ALE (`alibi.explainers.ALE`).  \n",
    "  3. Порівняйте ALE-криві для всієї вибірки та підвибірки клієнтів з `NumberOfDependents ≥ 3`.  \n",
    "- Підказки: попередньо зріжте доходи до 99 перцентиля, аби уникнути шуму.  \n",
    "- Інтерпретація: опишіть дохідні пороги, після яких приріст ризику згладжується.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Контрфактичні пояснення “мінімальний крок до безпеки”:**  \n",
    "- Мета: для 50 клієнтів з високою прогнозованою ймовірністю прострочення знайти найменші зміни в ознаках, що переводять їх нижче порога 0.3.  \n",
    "- Кроки:  \n",
    "  1. Навчіть `LogisticRegression` та `RandomForest`; виберіть клієнтів, де обидві моделі прогнозують p>0.6.  \n",
    "  2. За допомогою `dice-ml` або власного градієнтного пошуку побудуйте контрфакти з обмеженнями: `age` не змінюється, `NumberOfTimes...` тільки зменшується, `MonthlyIncome` може зрости максимум на 40%.  \n",
    "  3. Оцініть, які ознаки найчастіше потрібно коригувати, та середню “ціну” інтервенції.  \n",
    "- Підказки: нормуйте ознаки в [0,1], щоб пошук сходився.  \n",
    "- Інтерпретація: сформулюйте три приклади політик скорингу, що базуються на виявлених корекціях.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Локальні лінійні пояснення для помилок RF:**  \n",
    "- Мета: з’ясувати, які ознаки спричиняють хибно-негативні рішення `RandomForestClassifier`.  \n",
    "- Кроки:  \n",
    "  1. Збережіть 100 FN випадків (реальний дефолт, прогноз “0”).  \n",
    "  2. Запустіть `LIME` або `shap.KernelExplainer` для кожного FN та побудуйте розподіл локальних ваг.  \n",
    "  3. Порівняйте топ-3 ваги з глобальними коефіцієнтами логістичної регресії; виміряйте відстань Дженсена–Шеннона між розподілами впливу.  \n",
    "- Підказки: для стабільності LIME використовуйте kernel_width ~= 0.75 × sqrt(n_features).  \n",
    "- Інтерпретація: які патерни призводять до пропуску проблемних клієнтів?\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Траєкторії коефіцієнтів при зміні регуляризації:**  \n",
    "- Мета: простежити, які ознаки залишаються важливими в логістичній регресії при зміні `C` від 1e-3 до 1e2.  \n",
    "- Кроки:  \n",
    "  1. Побудуйте сітку з 15 логарифмічних значень `C`, навчаючи модель на тих самих тренувальних даних (з фіксованим random_state).  \n",
    "  2. Для кожного `C` зберігайте коефіцієнти та AUC на валідації.  \n",
    "  3. Накладіть “path plot” (ось x – log10(C), y – коефіцієнт); виділіть точки, де знак коефіцієнта змінюється.  \n",
    "- Підказки: використайте `sklearn.linear_model.LogisticRegression(solver='liblinear')` для малих `C`.  \n",
    "- Інтерпретація: поясніть, які ознаки є “ядром” моделі, а які запалюються лише при слабкій регуляризації.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Ієрархічне групування ознак за впливом:**  \n",
    "- Мета: знайти кластери ознак, що мають подібні профілі важливості між моделями.  \n",
    "- Кроки:  \n",
    "  1. Обчисліть три типи важливостей: коефіцієнти логістичної регресії, Gini importance з лісу та permutation importance.  \n",
    "  2. Нормалізуйте кожний тип у діапазон [0,1] та сформуйте матрицю (feature × importance_type).  \n",
    "  3. Побудуйте дендрограму (метод Уорда) та виділіть щонайменше 3 кластери; для кожного кластеру охарактеризуйте тематику ознак.  \n",
    "- Підказки: `scipy.cluster.hierarchy`.  \n",
    "- Висновок: запропонуйте, як можна спростити модель, об’єднавши кластери або відкинувши надлишкові ознаки.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – Сегментована permutation importance:**  \n",
    "- Мета: перевірити, чи однаковий вплив ознак для молодих (<35), середніх (35–55) і старших (>55) клієнтів.  \n",
    "- Кроки:  \n",
    "  1. Для кожного сегмента окремо перенавчіть логістичну регресію та випадковий ліс (з однаковими гіперпараметрами).  \n",
    "  2. Порахуйте permutation importance та AUC у межах сегмента.  \n",
    "  3. Побудуйте “importance heatmap” (ось x – сегмент, y – ознака, колір – важливість).  \n",
    "- Підказки: слідкуйте, щоб у кожному сегменті було ≥500 записів; за потреби використайте stratifed subsampling.  \n",
    "- Інтерпретація: чи змінюється критичність `MonthlyIncome` або `NumberOfDependents` з віком?\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – Чутливість до шуму у вхідних ознаках:**  \n",
    "- Мета: оцінити, як випадкове додавання шуму до `MonthlyIncome` і `DebtRatio` впливає на коефіцієнти й важливості.  \n",
    "- Кроки:  \n",
    "  1. Згенеруйте 5 рівнів шуму (σ від 0 до 0.3 std ознаки) та для кожного перенавчіть моделі.  \n",
    "  2. Виміряйте зміщення коефіцієнтів логістичної регресії та зміну `feature_importances_` лісу.  \n",
    "  3. Побудуйте графік “noise vs drop in AUC/importance”.  \n",
    "- Підказки: додавайте шум тільки до тренувальної частини, щоб оцінити робастність.  \n",
    "- Інтерпретація: які ознаки найвразливіші до вимірювальних помилок?\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Порівняння методів відбору ознак:**  \n",
    "- Мета: з’ясувати, чи змінюється вплив ознак після вибору k-best (`f_classif`) порівняно з L1-штрафом.  \n",
    "- Кроки:  \n",
    "  1. Побудуйте три пайплайни: (а) без відбору, (б) `SelectKBest(k=8)`, (в) `LogisticRegression(penalty='l1', C=0.5)`.  \n",
    "  2. Для кожного обчисліть коефіцієнти та permutation importance після повторного тренування на відібраних фічах.  \n",
    "  3. Зведіть результат у таблицю, де для кожної ознаки вказано “залишилась/відкинута” та її вплив.  \n",
    "- Підказки: використайте `Pipeline` та `ColumnTransformer` для акуратного препроцесингу.  \n",
    "- Висновок: які методи відбору краще зберігають критичні ознаки для задачі скорингу?\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – Аналіз взаємодій через H-statistic:**  \n",
    "- Мета: кількісно виміряти взаємодію `NumberOfTime30-59DaysPastDueNotWorse` з іншими ознаками у випадковому лісі.  \n",
    "- Кроки:  \n",
    "  1. Навчіть ліс та використайте `sklearn.inspection.partial_dependence` для отримання PD по комбінаціях.  \n",
    "  2. Обчисліть H-statistic (Greenwell 2018) для кожної пари з цільовою ознакою.  \n",
    "  3. Побудуйте топ-5 взаємодій та інтерпретуйте їх у контексті кредитного ризику.  \n",
    "- Підказки: реалізуйте H-statistic вручну або скористайтесь `pyALE`.  \n",
    "- Інтерпретація: які комбінації поведінкових показників створюють найвищий ризик?\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – SHAP waterfall для конкретних позичальників:**  \n",
    "- Мета: проілюструвати, як окремі ознаки ведуть до позитивного/негативного прогнозу для двох клієнтів з протилежними рішеннями.  \n",
    "- Кроки:  \n",
    "  1. Виберіть одного клієнта зі статусом дефолту та одного надійного платника, що знаходяться близько до порога рішення.  \n",
    "  2. Побудуйте `shap.waterfall_plot` для наочного пояснення впливу кожної ознаки.  \n",
    "  3. Опишіть, які відмінності у `NumberOfDependents`, `DebtRatio`, `age` найбільше впливають на результат.  \n",
    "- Підказки: використайте однакову модель (наприклад, найкращий RandomForest).  \n",
    "- Висновок: сформулюйте рекомендації для кредитних аналітиків, як трактувати подібні кейси.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Моніторинг дрейфу важливостей:**  \n",
    "- Мета: змоделювати сценарій, коли дані надходять партіями, та відстежити, як змінюються важливості ознак.  \n",
    "- Кроки:  \n",
    "  1. Розбийте дані на 6 часових “батчів” за `age` (як проксі поколінь) або випадково.  \n",
    "  2. Для кожного батчу перенавчіть моделі та збережіть важливості.  \n",
    "  3. Побудуйте часові ряди важливостей і перевірте тренди (тест Кендалла).  \n",
    "- Підказки: використайте `pandas.Categorical` для маркування батчів.  \n",
    "- Інтерпретація: чи зростає роль поведінкових ознак у молодших поколінь?\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Порівняння глобальних та групових SHAP:**  \n",
    "- Мета: зрозуміти, чи відрізняється вплив ознак між клієнтами з різною кількістю утриманців.  \n",
    "- Кроки:  \n",
    "  1. Порахуйте глобальні SHAP values для всього датасету.  \n",
    "  2. Окремо усередніть SHAP для груп `NumberOfDependents = 0`, `1-2`, `≥3`.  \n",
    "  3. Побудуйте barplot різниць та сформулюйте висновки.  \n",
    "- Підказки: зберігайте shap_values у форматі `DataFrame`, щоб легко групувати.  \n",
    "- Висновок: чи справді багатодітність різко змінює внесок доходу або історії прострочень?\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Логіт-моделювання ймовірності приросту прострочень:**  \n",
    "- Мета: оцінити, у скільки разів зростає odds прострочення при збільшенні `NumberOfTime60-89DaysPastDueNotWorse` на 1, враховуючи взаємодію з `age`.  \n",
    "- Кроки:  \n",
    "  1. Створіть взаємодійну ознаку `age × NumberOfTime60-89...` та додайте її до логістичної регресії.  \n",
    "  2. Оцініть коефіцієнти, розрахуйте маржинальні ефекти при `age` = 30, 45, 60 років.  \n",
    "  3. Перевірте значущість взаємодії через тест Вальда.  \n",
    "- Підказки: використайте `statsmodels.discrete.discrete_model.Logit` для отримання стандартних похибок.  \n",
    "- Інтерпретація: сформулюйте, чи підвищується чутливість до прострочень із віком.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Важливість ознак у задачі скорингу з перекалібруванням:**  \n",
    "- Мета: перевірити, як калібрування (Platt scaling або isotonic) змінює сприйняття важливості.  \n",
    "- Кроки:  \n",
    "  1. Тренуйте RandomForest, потім застосуйте `CalibratedClassifierCV` з двома методами калібрування.  \n",
    "  2. На каліброваних прогнозах знову рахуйте permutation importance.  \n",
    "  3. Порівняйте ранги до та після калібрування (tau Спірмена).  \n",
    "- Підказки: використайте 3-fold внутрішньої калібровки.  \n",
    "- Інтерпретація: поясніть, які ознаки “втрачають” вагу після покращення калібрування.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – Порівняння глобальних коефіцієнтів і локальних градієнтів:**  \n",
    "- Мета: перевірити, чи збігаються знаки глобальних коефіцієнтів логістичної регресії з середніми локальними градієнтами втрати.  \n",
    "- Кроки:  \n",
    "  1. Навчіть логістичну регресію та обчисліть градієнти `∂loss/∂x_i` на валідації.  \n",
    "  2. Усередніть градієнти за клієнтами, порівняйте зі знаками коефіцієнтів.  \n",
    "  3. Визначте ознаки, де знак не збігається, та дослідіть причини (нелінійність, взаємодії).  \n",
    "- Підказки: градієнт для логіта можна отримати зі `predict_proba` та формули для бінарної логістичної втрати.  \n",
    "- Висновок: що робити з ознаками, де глобальні й локальні впливи не збігаються?\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Вплив ознак на прийняття кредиту vs. прострочення:**  \n",
    "- Мета: моделювати двоступеневий процес: імовірність схвалення кредиту та імовірність прострочення, щоб порівняти вплив тих самих ознак.  \n",
    "- Кроки:  \n",
    "  1. Змоделюйте додаткову бінарну ціль “схвалений” (імітуйте через випадковий відбір або наявні прапорці).  \n",
    "  2. Навчіть дві логістичні регресії з однаковими фічами.  \n",
    "  3. Порівняйте коефіцієнти/важливості та побудуйте діаграму, що показує, які ознаки штовхають у різні боки два рішення.  \n",
    "- Підказки: для другої задачі можна використати псевдо-мітки (наприклад, top-30% клієнтів за доходом як “схвалені”) — поясніть обмеження.  \n",
    "- Інтерпретація: як поєднати вплив ознак, щоб уникнути прийняття ризикових клієнтів?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:46.876995300Z",
     "start_time": "2023-11-21T16:17:46.829862400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ваш код тут"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Примітка 1:*** Приклад реалізації функції softmax можете підчитати [тут](https://machinelearningmastery.com/softmax-activation-function-with-python/).\n",
    "\n",
    "***Примітка 2:***\n",
    "\n",
    "Формула, за якою обраховують у скільки разів збільшаться шанси при підвищенні знчення ознаки:\n",
    "\n",
    "$$ \\large \\exp^{\\beta\\delta}, $$\n",
    "\n",
    "де\n",
    "\n",
    "$ \\beta $ – значення ознаки; $ \\delta $ – на скільки робимо приріст.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-5.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.5. Створення ансамблевих моделей</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 4</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Для всіх варіантів:**\n",
    "\n",
    "Використовуйте очищений датафрейм, сформований на основі `credit_scoring_sample.csv`, і не змінюйте стратифіковану схему розбиття, ініціалізовану у змінній `skf`. Базовими моделями для всіх підходів є регуляризована `LogisticRegression` (із стандартизацією числових полів) та `RandomForestClassifier`, над якими будується відповідний ансамблевий шар. Фіксуйте `random_state`, зберігайте результати кожного фолду, а підсумкові метрики звітуйте як `mean ± std` для AUC, KS, log-loss та бізнес-показників (expected profit, recall@precision).\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Out-of-fold стекінг логіт + RF:**\n",
    "- Мета: Побудувати трирівневий ансамбль, у якому логістична регресія та випадковий ліс генерують out-of-fold (OOF) прогнози, а метакласифікатор-логіт уточнює ризик дефолту.\n",
    "\n",
    "- Кроки:  \n",
    "  1. На кожному фолді `skf` тренуйте окремі пайплайни `StandardScaler -> LogisticRegression(C, penalty='l2')` та `RandomForestClassifier(max_depth, max_features, n_estimators)`; зберігайте OOF-імовірності через `cross_val_predict(..., method='predict_proba')`.  \n",
    "  2. Зберіть мета-набір з колонок `p_logit`, `p_rf`, їх добутку/різниці, а також ключових ознак (`age`, `DebtRatio`, `MonthlyIncome`). Нормалізуйте й натренуйте метамодель-логіт з elastic-net регуляризацією.  \n",
    "  3. Оцініть ансамбль на відкладеній вибірці та побудуйте криву KS для базових моделей і стеку на одному графіку.  \n",
    "- Підказки: Використовуйте `FunctionTransformer` для конструювання нових мета-ознак, а для порівняння з базами — `sklearn.metrics.roc_auc_score` і `scipy.stats.ks_2samp`.  \n",
    "- Оцінювання: Вкажіть виграш ансамблю над окремими моделями за AUC і Brier-score; проаналізуйте фолди з найбільшим/найменшим ефектом.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – Ваговий софт-вотинг під бізнес-метрику:**\n",
    "\n",
    "\n",
    "- Мета: Знайти оптимальні ваги soft voting для логіта та випадкового лісу, максимізуючи очікуваний прибуток кредитора.  \n",
    "- Кроки:  \n",
    "  1. Налаштуйте гіперпараметри базових моделей на внутрішній сітці (наприклад, `C ∈ {0.1, 1, 10}`, `max_depth ∈ {6, 10, 14}`) із використанням `skf`.  \n",
    "  2. Для кожного фолду сформуйте сітку ваг `w_logit ∈ [0.1, 0.9]` з кроком 0.1, обчисліть комбіновану ймовірність `w_logit * p_logit + (1 - w_logit) * p_rf` і підрахуйте очікуваний прибуток `tp*gain - fp*cost`.  \n",
    "  3. Оберіть ваги, що стабільно дають найвищий прибуток, зафіксуйте їх і повторно переоцініть ансамбль на повному `skf`.  \n",
    "- Підказки: Використовуйте `numpy.linspace` для генерації ваг та `sklearn.metrics.make_scorer` для кастомної функції прибутку; параметри `gain` і `cost` обґрунтуйте згідно з бізнес-кейсом.  \n",
    "- Оцінювання: Наведіть таблицю ваг + AUC/expected profit для кожного фолду й покажіть, як зміни ваг впливають на recall найризиковіших 5% клієнтів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Блендинг із відкладеним шаром калібрації:**\n",
    "\n",
    "\n",
    "- Мета: Порівняти класичний стекінг з блендингом, коли мета-рівень навчається на окремому стратифікованому холдауті та поєднує калібровані прогнози базових моделей.  \n",
    "- Кроки:  \n",
    "  1. Відокремте 15% тренувальних даних у стратифікований `blend_holdout`, решту використайте для навчання логіта та випадкового лісу (з попереднім тюнінгом).  \n",
    "  2. Згенеруйте прогнози на `blend_holdout` та виконайте калібрування: для логіта — метод Платта, для випадкового лісу — ізотонічна регресія через `CalibratedClassifierCV(cv=skf, method='isotonic')`.  \n",
    "  3. Навчіть мета-логіт на каліброваних питомих ймовірностях, добутку та різниці прогнозів; зафіксуйте модель і протестуйте її в крос-валідації та на незалежному тесті.  \n",
    "- Підказки: Для відділення холдауту використайте `StratifiedShuffleSplit`; простежте, щоб `blend_holdout` не використовувався при тюнінгу гіперпараметрів.  \n",
    "- Оцінювання: Порівняйте криві калібрації (Reliability curve) до та після блендингу й вкажіть зміни в Brier-score.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – Сегментований ансамбль за віковими групами:**\n",
    "\n",
    "- Мета: Побудувати ансамбль, що враховує різну поведінку боржників у вікових сегментах та комбінує спеціалізовані моделі.  \n",
    "- Кроки:  \n",
    "  1. Розбийте клієнтів на сегменти (`<=35`, `36–55`, `>55`) через `pd.cut`; у кожному сегменті окремо налаштуйте логіт з `class_weight='balanced'` та випадковий ліс з оптимальним `min_samples_leaf`.  \n",
    "  2. Згенеруйте для кожного клієнта вектор з трьох пар прогнозів (по сегментах) та протягом `skf` навчіть гейтингову модель (ще один логіт), яка комбінує релевантні пари залежно від належності до сегмента.  \n",
    "  3. Реалізуйте інференс, що автоматично обирає \"свій\" сегмент і формує фінальну ймовірність як зважену суму спеціалізованих моделей.  \n",
    "- Підказки: Для зручності використайте `ColumnTransformer` з `OneHotEncoder` для сегментів, а також кешуйте треновані моделі сегментів, щоб не перенавчати їх на кожному фолді.  \n",
    "- Оцінювання: Побудуйте таблицю якості (AUC/recall) по сегментах та покажіть, наскільки сегментований ансамбль переважає глобальні моделі.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Бутстрепний логіт + стохастичний RF:**\n",
    "\n",
    "- Мета: Підвищити стійкість ансамблю завдяки багатократному бутстрепінгу логістичної регресії та агрегації зі стохастичним випадковим лісом.  \n",
    "- Кроки:  \n",
    "  1. Для кожного фолду генеруйте щонайменше 25 бутстрепних вибірок, тренуйте на них логіти з L1-регуляризацією та усереднюйте ймовірності (bagging логіта).  \n",
    "  2. Налаштуйте випадковий ліс із високою стохастичністю (`max_features='sqrt'`, `max_samples=0.7`, `min_samples_leaf>=5`) і отримаєте прогнози на тих самих фолдах.  \n",
    "  3. Об'єднайте середні ймовірності логіта з виходом лісу через простий soft voting або невеликий мета-логіт; порівняйте варіативність прогнозів між бутстрепами.  \n",
    "- Підказки: Скористайтеся `sklearn.ensemble.BaggingClassifier` з `base_estimator=LogisticRegression(...)` для спрощення коду; тримайте контроль над `n_jobs` для пришвидшення.  \n",
    "- Оцінювання: Проаналізуйте розкид AUC між бутстрепами, наведіть boxplot log-loss і зробіть висновки щодо стабільності ансамблю.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Ансамбль для дисбалансного класу:**\n",
    "\n",
    "- Мета: Побудувати ансамбль, що поєднує засоби балансування: логіт з адаптивними вагами класів та `BalancedRandomForestClassifier`.  \n",
    "- Кроки:  \n",
    "  1. Дослідіть співвідношення класів, обчисліть коефіцієнт дисбалансу та задайте `class_weight={0:w0,1:w1}` для логіта; додатково протестуйте `SMOTE`/`RandomUnderSampler` у пайплайні.  \n",
    "  2. Натренуйте `BalancedRandomForestClassifier` (або `RandomForestClassifier` з `class_weight='balanced_subsample'`) на тих самих фолдах, підібравши `n_estimators`, `max_depth`.  \n",
    "  3. Комбінуйте моделі через soft voting або стекінг, приділивши увагу recall та precision у міноритарному класі; додайте cost-sensitive логіку у виборі порогу.  \n",
    "- Підказки: Скористайтесь `imblearn.pipeline.Pipeline` для спільного навчання ресемплера і моделі; метрики рахуйте через `classification_report` та `precision_recall_curve`.  \n",
    "- Оцінювання: Звірте `PR-AUC` базових моделей й ансамблю, наведіть зсув у F1-score та частці виявлених дефолтів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Калібрований ансамбль імовірностей:**\n",
    "\n",
    "- Мета: Отримати добре калібровані PD, поєднавши логіт і випадковий ліс після незалежної калібрації.  \n",
    "- Кроки:  \n",
    "  1. Для кожної базової моделі застосуйте `CalibratedClassifierCV` з `cv=skf`: для логіта оберіть `method='sigmoid'`, для випадкового лісу — `method='isotonic'`.  \n",
    "  2. Після калібрації створіть ансамблевий прогноз як середнє каліброваних PD або як вагову суму з коефіцієнтами, що мінімізують Brier-score.  \n",
    "  3. Побудуйте графіки надійності (`CalibrationDisplay`) та порівняйте їх з некаліброваними версіями.  \n",
    "- Підказки: Уникайте \"витоку\" — калібрування має відбуватися всередині кожного фолду; для пошуку ваг використайте `scipy.optimize.minimize`.  \n",
    "- Оцінювання: Звітируйте Brier-score, Expected Calibration Error та відсоток клієнтів, для яких PD у [0.2, 0.4] змінилась більш ніж на 5 п.п.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Контрольований поріг при обмеженні FPR:**\n",
    "\n",
    "- Мета: Створити ансамбль, що дотримується бізнес-вимоги `FPR <= 5%`, але максимізує recall дефолтів.  \n",
    "- Кроки:  \n",
    "  1. Підготуйте базові прогнози логіта й RF; сформуйте комбіновану ймовірність (наприклад, середнє або стекінг).  \n",
    "  2. На кожному фолді побудуйте криву ROC та знайдіть поріг, де `FPR` не перевищує 0.05; перевірте стабільність порогу між фолдами.  \n",
    "  3. Реалізуйте адаптивний пороговий механізм: якщо FPR вищий, коригуйте ваги ансамблю (зменшуйте вагу RF, якщо він більш агресивний).  \n",
    "- Підказки: Використовуйте `sklearn.metrics.roc_curve` для пошуку порогів; для автоматизації можна написати невелику функцію, що повертає оптимальний `threshold`.  \n",
    "- Оцінювання: Наведіть таблицю `threshold/FPR/TPR` по фолдах та поясніть, як ансамбль впорається з найризиковішими 1% заявок.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Відбір ознак через важливості RF:**\n",
    "\n",
    "- Мета: Скоротити простір ознак для логіта, залишивши тільки найінформативніші за оцінкою випадкового лісу, та знову поєднати моделі.  \n",
    "- Кроки:  \n",
    "  1. Тренуйте випадковий ліс на повному наборі, збережіть `feature_importances_` для кожного фолду й усередніть їх.  \n",
    "  2. Відіберіть топ-N ознак (наприклад, 5, 7, 9) і навчіть на них логістичну регресію, порівнюючи її з базовою версією.  \n",
    "  3. Побудуйте ансамбль (soft voting або стекінг), у якому RF працює на повному наборі, а логіт — на скороченому; дослідіть вплив відбору на стабільність коефіцієнтів.  \n",
    "- Підказки: Для вибору `N` використайте `SequentialFeatureSelector` або ручний перебір; візуалізуйте важливості через горизонтальні bar-чарти.  \n",
    "- Оцінювання: Звітируйте зміну AUC/log-loss при різних `N` та поясніть, які змінні найчастіше потрапляють у топ.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – Стекінг через StackingClassifier з пошуком гіперпараметрів:**\n",
    "\n",
    "- Мета: Реалізувати офіційний `StackingClassifier`, де базові оцінювачі (логіт і RF) тюнінгуються разом із метамоделлю.  \n",
    "- Кроки:  \n",
    "  1. Створіть пайплайни базових моделей (логіт зі стандартизацією, RF з налаштовуваними глибиною/кількістю дерев).  \n",
    "  2. Сконфігуруйте `StackingClassifier` із мета-логітом та режимом `passthrough=True`, щоб метамодель бачила як базові прогнози, так і сирі ознаки.  \n",
    "  3. Запустіть `GridSearchCV` по параметрах усіх рівнів (наприклад, `C`, `max_depth`, `max_features`, `use_proba`), контролюючи час виконання за рахунок невеликих сіток.  \n",
    "- Підказки: Збережіть найкращу конфігурацію через `grid_search.best_params_`; для прискорення встановіть `n_jobs=-1` і використайте `verbose`.  \n",
    "- Оцінювання: Покажіть приріст AUC від `passthrough=True`, вкажіть усі обрані гіперпараметри та перевірте модель на відкладених даних.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – Динамічні ваги за якістю фолду:**\n",
    "\n",
    "- Мета: Побудувати ансамбль, що на кожному фолді перераховує ваги логіта й RF відповідно до їхньої поточної якості.  \n",
    "- Кроки:  \n",
    "  1. Під час `skf` рахуйте для кожної базової моделі AUC і log-loss; збережіть їх у таблиці.  \n",
    "  2. Перетворіть якість на ваги (наприклад, `w = (1 / logloss) / Σ(1 / logloss)` або пропорційно AUC) і за цими вагами агрегуйте ймовірності.  \n",
    "  3. Оцініть стабільність ваг між фолдами, а також вплив на загальну дисперсію прогнозів.  \n",
    "- Підказки: Автоматизуйте обчислення ваг у колбеку або власній функції; не забудьте зафіксувати мінімальне/максимальне значення ваг, щоб уникати вироджених випадків.  \n",
    "- Оцінювання: Побудуйте heatmap ваг (фолди × моделі) та графік, що показує кореляцію між вагою і покращенням метрик.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Гібрид PCA + сирі ознаки:**\n",
    "\n",
    "- Мета: Дослідити, чи покращить ансамбль ситуацію, якщо логіт працюватиме на компактному просторі головних компонент, а випадковий ліс — на сирих ознаках.  \n",
    "- Кроки:  \n",
    "  1. Побудуйте пайплайн `StandardScaler -> PCA(n_components=k) -> LogisticRegression`, де `k` підбирається за часткою поясненої дисперсії (наприклад, ≥0.9).  \n",
    "  2. На тих самих фолдах навчіть випадковий ліс без зменшення розмірності, але з tuned `n_estimators`, `max_depth`.  \n",
    "  3. Об'єднайте прогнози через soft voting або стекінг, додавши до мета-рівня ще й ознаку \"залишкова дисперсія\" для кожного спостереження.  \n",
    "- Підказки: Для аналізу вибору `k` побудуйте Scree-plot; перевірте, чи не втрачаються важливі delinquency-фічі.  \n",
    "- Оцінювання: Порівняйте AUC логіта до/після PCA та вплив гібриду на recall топ-10% ризикових клієнтів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – Прибуткоорієнтований ансамбль:**\n",
    "\n",
    "- Мета: Налаштувати ансамбль, що максимізує очікуваний NPV портфеля, враховуючи вартість капіталу та втрату від дефолту.  \n",
    "- Кроки:  \n",
    "  1. Визначте бізнес-параметри: `gain_tp` (збережений дохід від виявленого дефолту), `cost_fp` (втрата від неправомірної відмови), `loss_fn` (кредит, виданий дефолтному клієнту).  \n",
    "  2. Для кожного фолду комбінуйте логіт і RF, але підбирайте ваги/поріг так, щоб максимізувати очікуваний грошовий результат; реалізуйте власну метрику через `make_scorer(greater_is_better=True)`.  \n",
    "  3. Проведіть чутливісний аналіз: як змінюється рішення при ±20% зміні `gain/cost`.  \n",
    "- Підказки: Використовуйте `pandas.DataFrame.eval` для швидких розрахунків прибутку; візуалізуйте профіль прибутку проти порогу.  \n",
    "- Оцінювання: Наведіть таблицю прибутків по фолдах для базових моделей і ансамблю; зробіть висновок про стійкість.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – Ансамбль зі справедливістю за кількістю утриманців:**\n",
    "\n",
    "- Мета: Забезпечити близькі показники recall для клієнтів без утриманців і з ≥1 утриманцем, не втрачаючи загальну якість.  \n",
    "- Кроки:  \n",
    "  1. Створіть бінарну ознаку `has_dependents` та відстежуйте метрики окремо по двох групах у кожному фолді.  \n",
    "  2. Налаштуйте логіт і RF (можна з вагами), а потім коригуйте ваги ансамблю або пороги так, щоб |recall_group0 - recall_group1| ≤ 0.05.  \n",
    "  3. Якщо умова не виконується, додайте до мета-рівня штраф за порушення (наприклад, через множник у функції втрат) і повторіть навчання.  \n",
    "- Підказки: Застосуйте `sklearn.metrics.recall_score` із параметром `sample_weight`, щоб порівняти групи; для штрафів використайте `Optuna` або простий grid search.  \n",
    "- Оцінювання: Наведіть таблицю метрик за групами, графік різниці recall між ними та поясніть, як впливає баланс на бізнес-ризики.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Різноманітність моделей і pruning:**\n",
    "\n",
    "- Мета: Підвищити узгодженість ансамблю шляхом вимірювання різноманітності між логітом і RF та видалення \"надто схожих\" конфігурацій.  \n",
    "- Кроки:  \n",
    "  1. Для кількох наборів гіперпараметрів базових моделей зберіть їх OOF-прогнози й обчисліть Q-статистику, коефіцієнт кореляції та double-fault.  \n",
    "  2. Оберіть пари моделей, що мають достатню різноманітність (наприклад, Q < 0.8), і лише їх використайте в ансамблі.  \n",
    "  3. Зафіксуйте кінцевий ансамбль (2–3 логіти з різними `C`, 2 RF з різними глибинами) та агрегуйте прогнози середнім або лінійною регресією.  \n",
    "- Підказки: Для розрахунку різноманітності створіть окремі утиліти; уникайте перевантаження ансамблю надто близькими моделями.  \n",
    "- Оцінювання: Покажіть таблицю \"різноманітність → якість\" і поясніть, чому pruned-ансамбль працює краще/стабільніше.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Листовий embedding від випадкового лісу:**\n",
    "\n",
    "- Мета: Перетворити індекси листків випадкового лісу на sparse-ознаки для логістичної регресії та побудувати стек, що поєднує структурні шаблони RF з лінійною інтерпретацією.  \n",
    "- Кроки:  \n",
    "  1. Натренуйте RF з обмеженою глибиною; збережіть результати `forest.apply(X)` та перетворіть їх на one-hot через `OneHotEncoder(handle_unknown='ignore')`.  \n",
    "  2. Побудуйте логістичну регресію на згенерованих leaf-features, за потреби застосувавши `TruncatedSVD` для зменшення розмірності.  \n",
    "  3. Поєднайте вихід базового RF та \"leaf-logit\" у стекінгу й оцініть виграш.  \n",
    "- Підказки: Для економії пам'яті використовуйте `scipy.sparse.csr_matrix`; pipeline можна скласти з `FeatureUnion`.  \n",
    "- Оцінювання: Порівняйте коефіцієнти leaf-логіта з важливостями RF та поясніть, які листки найбільше сигналізують дефолт.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Резидуальне коригування прогнозів:**\n",
    "\n",
    "- Мета: Побудувати ансамбль, у якому випадковий ліс моделює залишкову помилку логістичної регресії.  \n",
    "- Кроки:  \n",
    "  1. Навчіть логіт на всіх фолдах, отримаєте `p_logit` та обчисліть залишки `residual = y - p_logit`.  \n",
    "  2. Тренуйте RF на тих самих ознаках, але з таргетом `residual`; обмежте предикцію в діапазоні [-1, 1].  \n",
    "  3. Фінальний ризик розрахуйте як `clip(p_logit + correction_rf, 0, 1)`; перевірте, чи зменшився log-loss.  \n",
    "- Підказки: Для стабільності використовуйте менший `max_depth`; візуалізуйте розподіл залишків до/після корекції.  \n",
    "- Оцінювання: Звірте RMSE залишків і проаналізуйте, в яких діапазонах `DebtRatio` резидуальне моделювання дає найбільший ефект.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Монотонні обмеження для логіта + RF-корекція:**\n",
    "\n",
    "- Мета: Забезпечити монотонне зростання PD зі збільшенням показників прострочок, комбінуючи обмежений логіт і гнучкий RF.  \n",
    "- Кроки:  \n",
    "  1. Складіть перелік ознак, для яких потрібна монотонність (`NumberOfTimes90DaysLate`, `NumberOfTime60-89DaysPastDueNotWorse`, `NumberOfTime30-59DaysPastDueNotWorse`).  \n",
    "  2. Навчіть логіт з L-BFGS, фіксуючи знак коефіцієнтів (наприклад, через регуляризацію та post-processing), щоби коефіцієнти були ненегативні.  \n",
    "  3. Додайте RF без обмежень, що моделює залишкові взаємодії; під час ансамблювання переконайтеся, що фінальна функція все ще монотонна (за потреби застосуйте ізотонічну регресію).  \n",
    "- Підказки: Використайте `sklearn.linear_model.SGDClassifier` із `penalty='l2'` і кастомними вагами або оптимізуйте через `cvxpy`.  \n",
    "- Оцінювання: Побудуйте часткові залежності (PDP) для показників прострочок та доведіть, що умова монотонності виконується.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – SHAP-кероване коригування ваг:**\n",
    "\n",
    "- Мета: Поєднати моделі так, щоб фінальне рішення узгоджувалося з інтерпретацією SHAP і не суперечило ключовим драйверам ризику.  \n",
    "- Кроки:  \n",
    "  1. Обчисліть SHAP-значення для логіта (`LinearExplainer`) та RF (`TreeExplainer`) на валідаційних фолдах; визначте топ-5 ознак за абсолютним SHAP.  \n",
    "  2. Побудуйте вагову функцію, яка збільшує внесок моделі, чиї SHAP-пояснення краще співпадають із бізнес-гіпотезами (наприклад, більші коефіцієнти для моделей, де `DebtRatio` посідає топ-3).  \n",
    "  3. Переналаштуйте ансамбль, використовуючи знайдені ваги, і перевірте, чи покращились як метрики, так і пояснюваність (узгодженість ознак у топі).  \n",
    "- Підказки: Зберігайте пояснення у `pandas`-таблицях та будуйте bar-чарти впливу; автоматизуйте перевірку \"узгодженості\" правил.  \n",
    "- Оцінювання: Наведіть топ-5 SHAP-ознак до/після ансамблю та поясніть, як змінилися коефіцієнти ваг.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Автоматизований енкодинг через ColumnTransformer:**\n",
    "\n",
    "- Мета: Побудувати єдиний пайплайн, де логіт та RF підключаються до спільного `ColumnTransformer`, а ансамбль збирається автоматично.  \n",
    "- Кроки:  \n",
    "  1. Створіть `ColumnTransformer`, що паралельно масштабує числові поля для логіта та подає сирі (або bucketized) значення до випадкового лісу; розгляньте додавання похідних ознак (DebtRatio × NumberOfDependents).  \n",
    "  2. Обгорніть трансформер двома пайплайнами (`logit_pipe`, `rf_pipe`) та підключіть їх до `VotingClassifier` або `StackingClassifier`.  \n",
    "  3. Налаштуйте гіперпараметри трансформера (кількість бінів, спосіб обробки пропусків) і базових моделей у спільному `GridSearchCV`.  \n",
    "- Підказки: Використовуйте `Pipeline(memory=...)` для кешування; для нових ознак скористайтесь `FunctionTransformer`.  \n",
    "- Оцінювання: Покажіть, що повністю автоматизований пайплайн зменшує технічний борг: порівняйте час навчання, кількість рядків коду та якість із ручною реалізацією."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-5.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.6. Удосконалення ансамблевих моделей</span>\n",
    "\n",
    "[Повернутися до змісту](#lab-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наступне завдання полягатиме у навчанні та удосконаленні ансамблевих моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red; font-size:1.5em;\">Завдання 5</span>\n",
    "\n",
    "---\n",
    "\n",
    "**Для всіх варіантів:**\n",
    "\n",
    "Необхідно використати ті ж самі тренувальні та валідаційні вибірки, що й у попередніх підрозділах файлу `ida_lab-05_logit-rf-credit-scoring.ipynb`, використовуючи дані з `credit_scoring_sample.csv`. Базовий стек складають пайплайн `StandardScaler -> LogisticRegression` та `RandomForestClassifier`, навчені в режимі stratified k-fold (змінна `skf`). Усі покращення необхідно оцінювати мінімум за AUC, log-loss, KS, precision/recall на бізнес-порозі та очікуваним прибутком. Результати подайте у вигляді таблиць `mean ± std` по фолдах і коротких інтерпретацій.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 1 – Багатоцільова оптимізація через Optuna:**\n",
    "\n",
    "- Мета: Одночасно зменшити log-loss та максимізувати прибуток ансамблю, налаштовуючи гіперпараметри логіта й випадкового лісу за допомогою TPE-семплера.  \n",
    "- Кроки:  \n",
    "  1. Визначте простір пошуку (`C`, `penalty`, `max_depth`, `max_features`, `min_samples_leaf`) і налаштуйте `Optuna study` з двома цілями.  \n",
    "  2. На кожній пробі запускайте повний `skf`, зберігайте пари (log-loss, -expected_profit) та будуйте Парето-фронт.  \n",
    "  3. Оберіть компромісну конфігурацію (наприклад, найнижчий log-loss серед топ-3 за прибутком) і перевалідуйте її на hold-out.  \n",
    "- Підказки: Використовуйте `Optuna.integration.SkoptSampler` для прискорення та кешуйте результати через `RDBStorage`.  \n",
    "- Оцінювання: Покажіть еволюцію Парето-фронту і порівняйте обрану точку з базовими моделями.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 2 – Розширений блендинг із градієнтним бустингом:**\n",
    "\n",
    "- Мета: Додати до ансамблю третю модель (наприклад, `HistGradientBoostingClassifier`) і налаштувати ваговий блендинг, що мінімізує log-loss.  \n",
    "- Кроки:  \n",
    "  1. Навчіть кожну базову модель у `skf`, зберігши out-of-fold ймовірності.  \n",
    "  2. Складіть оптимізаційну задачу на ваги `w_logit + w_rf + w_hgb = 1`, знаходячи мінімум log-loss (використайте `scipy.optimize`).  \n",
    "  3. Зафіксуйте ваги, повторно натренуйте моделі на всіх даних і протестуйте ансамбль на відкладених спостереженнях.  \n",
    "- Підказки: Нормуйте прогнози перед блендингом (наприклад, через логіти), щоб уникнути домінування однієї моделі.  \n",
    "- Оцінювання: Наведіть таблицю метрик для кожної моделі та фінального бленду, а також heatmap ваг.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 3 – Дистиляція ансамблю в пояснювану модель:**\n",
    "\n",
    "- Мета: Сконденсувати поведінку складного ансамблю у регуляризований логіт, навчаючи його на soft-labels від стеку.  \n",
    "- Кроки:  \n",
    "  1. Згенеруйте прогнози ансамблю для всіх фолдів та повного train, зберігаючи температурно згладжені ймовірності (`T=2–5`).  \n",
    "  2. Навчіть `LogisticRegression` на вихідних ознаках із таргетом `p_teacher`, додайте важливі взаємодії (DebtRatio×Dependents).  \n",
    "  3. Порівняйте точність дистильованої моделі з учительським ансамблем і базовим логітом.  \n",
    "- Підказки: Використовуйте комбіновану функцію втрат `α * BCE(y, p_student) + (1-α) * KL(p_teacher, p_student)`.  \n",
    "- Оцінювання: Продемонструйте, як дистиляція впливає на інтерпретацію коефіцієнтів та бізнес-поріг.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 4 – Автоадаптивні пороги для різних сегментів:**\n",
    "\n",
    "- Мета: Навчити ансамбль, що підбирає власний поріг прийняття рішення для кожної групи клієнтів (наприклад, за віком або залежними).  \n",
    "- Кроки:  \n",
    "  1. Створіть сегменти (`age <=35`, `36–55`, `>55`, `has_dependents`) та зберіть прогнози ансамблю по кожному сегменту в `skf`.  \n",
    "  2. Для кожного сегмента підберіть поріг, що максимізує `expected_profit` при обмеженні `FPR <= 7%`.  \n",
    "  3. Реалізуйте inference, який автоматично застосовує відповідний поріг, і проаналізуйте стабільність порогів між фолдами.  \n",
    "- Підказки: Використайте `sklearn.metrics.roc_curve` + власну функцію прибутку; пороги можна згладити через експоненційне середнє.  \n",
    "- Оцінювання: Таблиця порогів та прибутку по сегментах; пояснення, чи варто уніфікувати пороги.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 5 – Адаптація до дрейфу через вагове ресемплювання:**\n",
    "\n",
    "- Мета: Підвищити стійкість ансамблю до потенційного дрейфу (зміни розподілу ознак) за допомогою ваг, розрахованих за щільностями.  \n",
    "- Кроки:  \n",
    "  1. Проведіть `adversarial validation`: навчіть логіт, що відрізняє train від hold-out, та збережіть його ймовірності як міру зсуву.  \n",
    "  2. Перетворіть ці ймовірності на ваги (`w = 1 / (p_shift + ε)`) і використайте їх у `fit(X, y, sample_weight=w)` для логіта й RF.  \n",
    "  3. Перевірте, чи зменшилась різниця метрик між фолдами та на відкладеній вибірці.  \n",
    "- Підказки: Для випадкового лісу використайте `BalancedRandomForestClassifier` або параметр `sample_weight`.  \n",
    "- Оцінювання: Звіт про зміну AUC/log-loss до і після ресемплювання, плюс графік важливості ознак дрейфу.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 6 – Псевдо-лейблинг відхилених заявок:**\n",
    "\n",
    "- Мета: Розширити тренувальний набір через напівавтоматичне маркування клієнтів, для яких модель має високу впевненість.  \n",
    "- Кроки:  \n",
    "  1. Визначте два пороги впевненості (наприклад, <0.05 та >0.95) і сформуйте псевдо-лейбли з прогнозів ансамблю для невикористаних заявок.  \n",
    "  2. Додайте ці записи до тренувальної вибірки з пониженими вагами (`sample_weight=0.3–0.5`) та перенавчіть логіт+RF.  \n",
    "  3. Перевірте, чи покращилися метрики на валідації й наскільки стабільні результати по фолдах.  \n",
    "- Підказки: Використовуйте `sklearn.utils.resample` для балансування псевдо-набору; ведіть окремий лог файлів із джерелом кожного запису.  \n",
    "- Оцінювання: Порівняйте AUC та recall до/після; вкажіть, скільки псевдо-міток додано і їхній вплив.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 7 – Призначення ваг ознакам через target encoding:**\n",
    "\n",
    "- Мета: Поліпшити взаємодію логіта й RF за рахунок впровадження регуляризованого target encoding для дискретних полів (наприклад, `NumberOfDependents`).  \n",
    "- Кроки:  \n",
    "  1. Реалізуйте згортання крос-валідаційних target-енкодерів (KFoldTargetEncoder) із шумом, щоб уникнути витоків.  \n",
    "  2. Додайте закодовані ознаки в пайплайни обох моделей та повторно натренуйте ансамбль.  \n",
    "  3. Проаналізуйте вплив енкодера на важливості ознак і на стабільність коефіцієнтів логіта.  \n",
    "- Підказки: Готові реалізації можна знайти в `category_encoders`; стежте, щоб `skf` у енкодері збігався з основною валідацією.  \n",
    "- Оцінювання: Таблиця метрик до/після, графіки залежності PD для закодованих ознак.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 8 – Сегментація за поведінковими кластерами та локальні ансамблі:**\n",
    "\n",
    "- Мета: Побудувати KMeans/miniBatch кластеризацію клієнтів і навчити для кожного кластера власний міні-ансамбль.  \n",
    "- Кроки:  \n",
    "  1. Масштабуйте ознаки й оберіть кількість кластерів (Silhouette/Calinski).  \n",
    "  2. У кожному кластері виконайте швидкий `skf`, налаштовуючи невеликі ансамблі (логіт + shallow RF), й збережіть їх як експертів.  \n",
    "  3. На інференсі обирайте експерта за найближчим кластером; додатково порівняйте з глобальним ансамблем.  \n",
    "- Підказки: Використовуйте `NearestCentroid` для швидкого призначення до кластера; кешуйте моделі експертів.  \n",
    "- Оцінювання: Таблиця кластерів (N, AUC, KS) та висновки про сегменти, де локальні моделі виграють.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 9 – Усунення перекосу через reweighing fairness:**\n",
    "\n",
    "- Мета: Забезпечити близький рівень recall для клієнтів із різною кількістю прострочок (`NumberOfTimes90DaysLate` ≤1 vs >1), не втрачаючи загальну якість.  \n",
    "- Кроки:  \n",
    "  1. Обчисліть справедливі ваги за методикою `reweighing` (Kamiran & Calders) і застосуйте їх під час навчання логіта й RF.  \n",
    "  2. Побудуйте ансамбль (stacking/voting) із цими вагами та протестуйте різницю показників між групами.  \n",
    "  3. За потреби додайте штраф у функцію втрат метамоделі за порушення `|recall_A - recall_B| > 0.05`.  \n",
    "- Підказки: Можна використати `aif360` для розрахунку ваг, але переконайтесь у сумісності з pandas.  \n",
    "- Оцінювання: Таблиця fairness-метрик (recall, precision) для обох груп та висновки щодо бізнес-ризиків.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 10 – KS-орієнтоване налаштування порогів і ваг:**\n",
    "\n",
    "- Мета: Підібрати ваги ансамблю та робочий поріг, які максимізують статистику Колмогорова–Смірнова між класами.  \n",
    "- Кроки:  \n",
    "  1. Побудуйте грід по вагам логіт/RF і для кожної комбінації обчисліть KS на OOF-прогнозах.  \n",
    "  2. Для комбінації-переможця знайдіть поріг із максимальним KS та перевірте його стабільність між фолдами.  \n",
    "  3. Застосуйте поріг до відкладеної вибірки, порівнявши зі стандартним (наприклад, 0.5).  \n",
    "- Підказки: KS можна швидко рахувати через `scipy.stats.ks_2samp`; збережіть розподіли оцінок для візуалізації.  \n",
    "- Оцінювання: Репорт із KS по фолдах, графік CDF та аналіз зміни TP/TN.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 11 – Спрямовані часткові залежності (PDP+ICE) та ручні правки:**\n",
    "\n",
    "- Мета: Використати PDP/ICE графіки для виявлення нелінійностей, після чого вручну додати відповідні поліноміальні/interaction-ознаки в логіт чи RF.  \n",
    "- Кроки:  \n",
    "  1. Побудуйте PDP/ICE для ключових ознак (`DebtRatio`, `MonthlyIncome`, `NumberOfDependents`).  \n",
    "  2. На основі спостережень створіть додаткові ознаки (наприклад, piecewise buckets), додайте їх до моделей і перенавчіть ансамбль.  \n",
    "  3. Перевірте, чи зменшився log-loss та чи покращився KS.  \n",
    "- Підказки: Використовуйте `sklearn.inspection.partial_dependence`; для piecewise можна застосувати `KBinsDiscretizer`.  \n",
    "- Оцінювання: Порівняння PDP до/після, зокрема чи згладилися \"сплески\".\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 12 – Стратегія «ensemble pruning» за Парето:**\n",
    "\n",
    "- Мета: Побудувати велику бібліотеку моделей (різні гіперпараметри логіта та RF) і залишити лише ті, що лежать на Парето-фронті (AUC vs стабільність).  \n",
    "- Кроки:  \n",
    "  1. Згенеруйте ≥15 конфігурацій, збережіть їх OOF-метрики та дисперсію результатів між фолдами.  \n",
    "  2. Визначте Парето-набір і лише з нього сформуйте soft-voting/stacking ансамбль.  \n",
    "  3. Порівняйте продуктивність pruned-ансамблю з ансамблем із усіх моделей.  \n",
    "- Підказки: Вимірюйте стабільність як std AUC або max-min між фолдами; автоматизація можлива через `pandas.DataFrame.query`.  \n",
    "- Оцінювання: Таблиця Парето-моделей і відсічені конфігурації, графік AUC vs std.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 13 – Гіперпараметричні метамоделі (surrogate modeling):**\n",
    "\n",
    "- Мета: Навчити метамодель (наприклад, Gaussian Process/RandomForestRegressor), що передбачає AUC logіта/RF за гіперпараметрами, щоб прискорити пошук.  \n",
    "- Кроки:  \n",
    "  1. Запишіть результати кількох десятків запусків `skf` для різних гіперпараметрів.  \n",
    "  2. Навчіть сурогат, який за `C`, `max_depth`, `min_samples_leaf` прогнозує AUC, та знайдіть максимум сурогату.  \n",
    "  3. Перевірте знайдені конфігурації на реальному `skf` і порівняйте з brute-force grid.  \n",
    "- Підказки: Для Gaussian Process використовуйте `sklearn.gaussian_process`; нормуйте параметри перед навчанням.  \n",
    "- Оцінювання: Час пошуку vs якість; помилка сурогату (MAE) на відкладених точках.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 14 – Стабілізація через cross-fitting та double validation:**\n",
    "\n",
    "- Мета: Зменшити витік інформації у стекінгу, застосувавши cross-fitting (дві незалежні схеми k-fold).  \n",
    "- Кроки:  \n",
    "  1. Розбийте `skf` на дві групи фолдів: перша генерує OOF для метарівня, друга використовується для навчання метамоделі.  \n",
    "  2. Переставте ролі та усередніть результати (double machine learning).  \n",
    "  3. Порівняйте з класичним стекінгом за AUC і варіативністю.  \n",
    "- Підказки: Реалізуйте власний cross-fitting або скористайтесь `sklearn.model_selection.StratifiedKFold` з різними `random_state`.  \n",
    "- Оцінювання: Таблиця метрик і графік розподілу прогнозів, щоб показати зменшення перенавчання.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 15 – Підвищення чутливості на хвості ризику (top-k focus):**\n",
    "\n",
    "- Мета: Оптимізувати ансамбль на виявлення найризиковіших 5% клієнтів (top-k recall).  \n",
    "- Кроки:  \n",
    "  1. Визначте метрику `recall@k`, де `k = 0.05 * N`, та інтегруйте її як скейлер у функцію втрат (наприклад, через `sklearn.metrics.make_scorer`).  \n",
    "  2. Налаштуйте ваги/пороги ансамблю, щоб максимізувати `recall@k`, контролюючи `precision@k`.  \n",
    "  3. Перевірте результат на тесті й побудуйте gain-curve.  \n",
    "- Підказки: Зберігайте сортування балів для кожного фолду; для оптимізації можна використати `Optuna` чи ручний пошук.  \n",
    "- Оцінювання: Порівняння gain-curve та `recall@k` базових моделей vs ансамблю.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 16 – Локальна експлорація гіперпараметрів через Hyperband:**\n",
    "\n",
    "- Мета: Використати Hyperband/Successive Halving для швидкого пошуку найкращих конфігурацій ансамблю.  \n",
    "- Кроки:  \n",
    "  1. Сформуйте простір пошуку для логіта та RF (20–30 конфігурацій) і налаштуйте `HalvingRandomSearchCV`.  \n",
    "  2. На кожному раунді збільшуйте кількість фолдів/дерев лише для найкращих моделей.  \n",
    "  3. Перевалідуйте фіналістів повним `skf` і побудуйте ансамбль з топ-3.  \n",
    "- Підказки: Контролюйте `resource='n_estimators'` для RF та `max_iter` для логіта; використовуйте `n_jobs=-1`.  \n",
    "- Оцінювання: Порівняйте час пошуку та AUC з класичним grid/random search.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 17 – Резидуальний бустинг поверх стеку:**\n",
    "\n",
    "- Мета: Додати легку модель (наприклад, `GradientBoostingRegressor`), що моделює резидуали ансамблю та коригує ймовірності.  \n",
    "- Кроки:  \n",
    "  1. Обчисліть залишок `residual = y - p_stack`.  \n",
    "  2. Навчіть регресор на тих самих ознаках, передбачаючи `residual`; обмежте вихід через `clip`.  \n",
    "  3. Додайте поправку до прогнозу (`p_final = clip(p_stack + residual_hat, 0, 1)`) і перевірте метрики.  \n",
    "- Підказки: Для стабільності використовуйте малу глибину дерев та learning_rate ≤ 0.05.  \n",
    "- Оцінювання: Зміна log-loss та розподіл залишків до/після.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 18 – Виявлення та обробка аномалій перед ансамблем:**\n",
    "\n",
    "- Мета: Зменшити вплив аномальних записів за допомогою ізоляційного лісу/LOF перед навчанням основних моделей.  \n",
    "- Кроки:  \n",
    "  1. Запустіть `IsolationForest` на тренувальних даних, виявивши top-1% аномалій; збережіть індекси.  \n",
    "  2. Порівняйте три підходи: (а) видалити аномалії, (б) зменшити їх ваги, (в) ввести бінарну ознаку `is_outlier`.  \n",
    "  3. Перенавчіть ансамбль і виберіть найкращу стратегію за AUC/log-loss.  \n",
    "- Підказки: Не застосовуйте аномалійний фільтр до тесту; зберігайте насіння генератора.  \n",
    "- Оцінювання: Вплив на метрики та бізнес-прибуток, список основних ознак аномальних кейсів.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 19 – SHAP-контроль якості покращень:**\n",
    "\n",
    "- Мета: Валідувати, що удосконалений ансамбль не суперечить бізнес-логіці, аналізуючи зміну SHAP-розподілів до/після.  \n",
    "- Кроки:  \n",
    "  1. Обчисліть SHAP для базового ансамблю та для покращеної версії (логіт: `LinearExplainer`, RF: `TreeExplainer`).  \n",
    "  2. Порівняйте топ-10 ознак, побудуйте beeswarm/summary-графіки.  \n",
    "  3. Якщо з’явилися неочікувані драйвери, виконайте коригування (наприклад, обмеження коефіцієнтів).  \n",
    "- Підказки: Використовуйте підвибірку (до 10 тис. рядків) для пришвидшення; кешуйте SHAP-значення.  \n",
    "- Оцінювання: Репорт із поясненнями змін у важливостях та впливу на метрики.\n",
    "\n",
    "---\n",
    "\n",
    "**Варіант 20 – Повний MLOps-контур для вдосконаленого ансамблю:**\n",
    "\n",
    "- Мета: Описати та частково реалізувати pipeline від навчання до деплойменту з автоматичним моніторингом якості ансамблю.  \n",
    "- Кроки:  \n",
    "  1. Сформулюйте навчальний скрипт (CLI) із параметрами для гіперпараметрів і збереженням артефактів (моделі, scaler, пороги).  \n",
    "  2. Реалізуйте evaluation-ноутбук/скрипт, що обчислює зазначені метрики та порівнює їх з базовими.  \n",
    "  3. Додайте модуль моніторингу (напр., простий дашборд у `mlflow` або `wandb`) для відстеження дрейфу та попереджень.  \n",
    "- Підказки: Структуруйте проєкт (папки `data/`, `models/`, `reports/`), логування ведіть через `mlflow.log_metric`.  \n",
    "- Оцінювання: Представте список артефактів, шаблон звіту та приклад alert-правила (метрика < порогу).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:17:47.015916700Z",
     "start_time": "2023-11-21T16:17:46.987867300Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
