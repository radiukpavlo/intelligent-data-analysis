{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Теми курсового проєкта</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тема 1: Інтелектуальна інформаційна система для прогнозування серцевих захворювань\n",
    "\n",
    "**Набір даних:** Heart Disease UCI (Версія Kaggle: [https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data))\n",
    "\n",
    "**Мета:** **Прогнозувати наявність серцевих захворювань** у пацієнтів на основі клінічних ознак (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Попереднє Оброблення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Використовуйте `pandas` для завантаження CSV файлу.\n",
    "    *   1.2. *Оброблення пропущених значень:* Ідентифікуйте та обробіть значення '?' (наприклад, заміною на `NaN`, а потім імпутацією середнім/медіаною або видаленням).\n",
    "    *   1.3. *Дослідницький аналіз даних (EDA):*\n",
    "        *   Візуалізуйте розподіли ключових ознак (наприклад, 'age', 'trestbps', 'chol') за допомогою гістограм та коробкових діаграм.\n",
    "        *   Проаналізуйте кореляції між ознаками за допомогою кореляційної теплової карти (`heatmap`).\n",
    "        *   Дослідіть взаємозв'язок між ознаками та цільовою змінною ('target') за допомогою групованих діаграм.\n",
    "    *   1.4. *Кодування категоріальних ознак:* Перетворіть категоріальні ознаки (наприклад, 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal') на числові за допомогою технік, як-от One-Hot Encoding (`pd.get_dummies`).\n",
    "    *   1.5. *Масштабування ознак:* Застосуйте `StandardScaler` до числових ознак для стандартизації їх діапазону.\n",
    "    *   1.6. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (`train_test_split`).\n",
    "\n",
    "2.  **Побудова Базових Моделей Класифікації:**\n",
    "    *   2.1. *Реалізація моделей:* Створіть екземпляри базових класифікаторів: Logistic Regression та K-Nearest Neighbors (K-NN).\n",
    "    *   2.2. *Навчання та Оцінка:*\n",
    "        *   Навчіть моделі на тренувальних даних.\n",
    "        *   Оцініть їх продуктивність на тестових даних, використовуючи метрики: Accuracy, Precision, Recall, $F_1$-score.\n",
    "    *   2.3. *Крос-валідація:* Застосуйте *крос-валідацію* (наприклад, `StratifiedKFold` з `cross_val_score`) для отримання більш надійної оцінки продуктивності моделей. Обчисліть середні значення метрик та AUC-ROC.\n",
    "    *   2.4. *Аналіз результатів:* Порівняйте продуктивність базових моделей. Візуалізуйте матриці плутанини та ROC-криві.\n",
    "\n",
    "3.  **Розширене Моделювання та Створення Інтерфейсу:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Створіть, навчіть та оцініть *просунуті класифікатори*, такі як Support Vector Machine (SVM), Random Forest та XGBoost.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Використовуйте техніки, як-от `GridSearchCV` або `RandomizedSearchCV`, для пошуку оптимальних гіперпараметрів для просунутих моделей.\n",
    "    *   3.3. *Порівняння моделей:* Систематично порівняйте продуктивність усіх моделей (базових та просунутих) за ключовими метриками. Визначте найкращу модель.\n",
    "    *   3.4. *Інтерпретація моделі:* Проаналізуйте **важливість ознак** (feature importance) для кращих моделей (наприклад, Random Forest, XGBoost), щоб зрозуміти, які фактори найбільше впливають на прогноз.\n",
    "    *   3.5. *Збереження моделі:* Збережіть найкращу навчену модель та необхідні об'єкти (наприклад, скейлер) за допомогою `joblib` або `pickle`.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Кінцевим результатом цього проєкту є **реалізований простий вебзастосунок**, створений за допомогою `streamlit`, `gradio` або розгорнутий на платформі `replit`. Застосунок повинен дозволяти користувачеві вводити клінічні дані та отримувати **прогноз ризику серцевого захворювання** на основі найкращої навченої моделі.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* Створіть Python-скрипт (наприклад, `app.py`).\n",
    "    2.  *Імпорт бібліотек:* Імпортуйте `streamlit` (або `gradio`), `pandas`, `numpy`, `joblib` (або `pickle`).\n",
    "    3.  *Завантаження моделі:* Завантажте збережену модель та скейлер (`joblib.load()`).\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   (`streamlit`) Використовуйте `st.title`, `st.sidebar.slider`, `st.sidebar.selectbox` тощо для створення елементів введення даних користувачем (відповідно до ознак моделі).\n",
    "        *   (`gradio`) Використовуйте `gr.Interface`, визначаючи функцію прогнозування та типи вхідних/вихідних даних (`gr.inputs`, `gr.outputs`).\n",
    "    5.  *Оброблення вводу:* Отримайте дані від користувача, перетворіть їх у формат, сумісний з моделлю (наприклад, DataFrame `pandas`). **Не забудьте застосувати збережений скейлер** до введених числових даних.\n",
    "    6.  *Прогнозування:* Використайте завантажену модель для отримання прогнозу (`model.predict()` та/або `model.predict_proba()`).\n",
    "    7.  *Відображення результату:* Покажіть прогноз користувачеві (наприклад, \"Ризик захворювання: Високий/Низький\" або ймовірність).\n",
    "    8.  *Запуск:* Запустіть локально (`streamlit run app.py` або `python app.py` для `gradio`).\n",
    "    9.  *Розгортання (опціонально):* Використовуйте `Streamlit Community Cloud`, `Hugging Face Spaces` (добре інтегрується з `gradio`), `Heroku`, `PythonAnywhere` або подібні сервіси для публічного доступу.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть новий Python Repl на [replit.com](https://replit.com/).\n",
    "    2.  *Завантаження файлів:* Завантажте ваш скрипт застосунку (`app.py`), збережену модель (`.pkl` або `.joblib`) та файл `requirements.txt` (що містить `streamlit`/`gradio`, `pandas`, `scikit-learn`, `xgboost` тощо).\n",
    "    3.  *Налаштування запуску:* У файлі `.replit` (або через Shell) вкажіть команду для запуску вашого застосунку (наприклад, `streamlit run app.py`).\n",
    "    4.  *Запуск:* Натисніть кнопку \"Run\". `Replit` автоматично встановить залежності та запустить вебсервер.\n",
    "    5.  *Доступ:* `Replit` надасть публічне посилання на ваш запущений вебзастосунок.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost` (опціонально)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Гістограми, коробкові діаграми (box plots), кореляційна теплова карта (heatmap), парні діаграми (pair plots), матриця плутанини (confusion matrix), ROC-крива, діаграми важливості ознак.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score, AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Використовуйте `StandardScaler` для _масштабування ознак_.\n",
    "*   **Зосередьтеся на інтерпретації результатів моделі** (наприклад, важливість ознак).\n",
    "*   Переконайтеся, що вебзастосунок є *зручним для користувача* та надає зрозумілі результати.\n",
    "*   Ретельно документуйте процес оброблення даних та вибору моделі.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 2: Інтелектуальна інформаційна система для прогнозування цін на вживані автомобілі\n",
    "\n",
    "**Набір даних:** Used Cars Price Prediction ([https://www.kaggle.com/datasets/avikasliwal/used-cars-price-prediction](https://www.kaggle.com/datasets/avikasliwal/used-cars-price-prediction))\n",
    "\n",
    "**Мета:** **Прогнозувати ціну** вживаних автомобілів на основі їхніх атрибутів (_Регресія_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та Інженерія Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте дані за допомогою `pandas`.\n",
    "    *   1.2. *Попередній аналіз:* Огляньте типи даних, кількість унікальних значень, наявність пропусків (`df.info()`, `df.describe()`, `df.isnull().sum()`).\n",
    "    *   1.3. *Очищення та парсинг ознак:*\n",
    "        *   Обробіть пропущені значення (видалення, імпутація середнім/медіаною/модою).\n",
    "        *   ***Очистіть та перетворіть рядкові ознаки, що представляють числові значення***: 'Mileage' (видалити 'km/kg'), 'Engine' (видалити 'CC'), 'Power' (видалити 'bhp', обробити 'null bhp'). Конвертуйте їх у числовий тип.\n",
    "        *   Витягніть *марку та модель* автомобіля з ознаки 'Name'.\n",
    "    *   1.4. *Інженерія ознак:*\n",
    "        *   Створіть нову ознаку 'car\\_age' на основі року випуску ('Year').\n",
    "        *   Розгляньте можливість створення інших ознак (наприклад, співвідношення потужності до об'єму двигуна).\n",
    "    *   1.5. *Кодування категоріальних ознак:* Закодуйте 'Location', 'Fuel\\_Type', 'Transmission', 'Owner\\_Type', а також створені ознаки марки/моделі (використовуйте One-Hot Encoding, Target Encoding або інші методи, враховуючи кардинальність).\n",
    "    *   1.6. *EDA:*\n",
    "        *   Візуалізуйте розподіл ціни ('Price'). Розгляньте **логарифмічне перетворення ціни**, якщо розподіл сильно асиметричний.\n",
    "        *   Дослідіть взаємозв'язки між ключовими ознаками ('car\\_age', 'Kilometers\\_Driven', 'Engine', 'Power') та ціною за допомогою діаграм розсіювання та кореляційної матриці.\n",
    "\n",
    "2.  **Побудова Базових Моделей Регресії:**\n",
    "    *   2.1. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки. Застосуйте масштабування (`StandardScaler` або `MinMaxScaler`) до числових ознак (після розділення).\n",
    "    *   2.2. *Реалізація моделей:* Створіть екземпляри базових регресійних моделей: Linear Regression, Ridge, Lasso.\n",
    "    *   2.3. *Навчання та Оцінка:* Навчіть моделі на тренувальних даних. Оцініть їх продуктивність на тестових даних за допомогою метрик MAE, MSE, RMSE та R-squared.\n",
    "    *   2.4. *Аналіз залишків:* ***Візуалізуйте залишки*** (різницю між прогнозованими та фактичними значеннями) для перевірки припущень лінійної регресії (наприклад, графік залишків проти прогнозованих значень).\n",
    "\n",
    "3.  **Розширена Регресія та Розгортання:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Створіть, навчіть та оцініть *просунуті регресійні моделі*: Random Forest Regressor, XGBoost Regressor.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Використовуйте `GridSearchCV` або `RandomizedSearchCV` для оптимізації гіперпараметрів просунутих моделей.\n",
    "    *   3.3. *Порівняння моделей:* Порівняйте продуктивність усіх моделей за обраними метриками. Розгляньте використання **RMSLE**, якщо застосовували логарифмічне перетворення ціни.\n",
    "    *   3.4. *Аналіз важливості ознак:* Визначте **ключові фактори**, що впливають на ціну автомобіля, за допомогою аналізу важливості ознак для моделей Random Forest або XGBoost.\n",
    "    *   3.5. *Збереження моделі:* Збережіть найкращу модель та об'єкти для попереднього оброблення (скейлер, кодувальники).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Результатом є **вебзастосунок для оцінки ціни вживаного автомобіля**, розроблений на `streamlit`, `gradio` або `replit`. Користувач повинен мати змогу ввести характеристики автомобіля (вік, пробіг, тип палива, трансмісія тощо) і отримати прогнозовану ціну.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`.\n",
    "    3.  *Завантаження моделі:* Завантажте збережену модель, скейлер та інші необхідні об'єкти попереднього оброблення.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи вводу для користувача (слайдери для віку/пробігу, випадаючі списки для типу палива/трансмісії тощо). Важливо відтворити **всі ознаки**, що використовувалися моделлю.\n",
    "    5.  *Оброблення вводу:* Зберіть дані користувача. Застосуйте **ту саму попередню обробку**, що й під час тренування (кодування категоріальних ознак, масштабування числових). Зверніть увагу на обробку ознак марки/моделі, якщо вони використовувались. Можливо, знадобиться зберегти список відомих марок/моделей.\n",
    "    6.  *Прогнозування:* Зробіть прогноз за допомогою завантаженої моделі. Якщо ціна логарифмувалася, **не забудьте застосувати обернене перетворення** (`np.expm1()`) до прогнозу.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану ціну користувачеві.\n",
    "    8.  *Запуск та Розгортання:* Як у Теми 1 (локально, потім `Streamlit Community Cloud`, `Hugging Face Spaces` тощо).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель (`.pkl`/`.joblib`), скейлер, кодувальники, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Як у Теми 1.\n",
    "    4.  *Доступ:* `Replit` надасть публічне посилання.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл цін (розгляньте *логарифмічне перетворення*), діаграми розсіювання ('Mileage'/'Year'/'Power' проти 'Price'), кореляційна теплова карта, діаграма важливості ознак, діаграми залишків.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, R-squared. Розгляньте _**RMSLE**_, якщо логарифмуєте ціну.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Приділіть *особливу увагу* очищенню рядкових стовпців, що представляють числові значення ('Mileage', 'Engine', 'Power').\n",
    "*   **Інженерія ознак є ключовою** (наприклад, 'car\\_age').\n",
    "*   Обережно обробляйте категоріальні ознаки з високою кардинальністю (наприклад, 'Name' -> марка/модель). Розгляньте обмеження кількості категорій або використання Target Encoding.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 3: Інтелектуальна інформаційна система для прогнозування попиту на товари в магазині\n",
    "\n",
    "**Набір даних:** Store Item Demand Forecasting Challenge ([https://www.kaggle.com/c/demand-forecasting-kernels-only/data](https://www.kaggle.com/c/demand-forecasting-kernels-only/data)) - _Потрібен акаунт Kaggle для доступу до даних змагання_.\n",
    "\n",
    "**Мета:** **Прогнозувати щоденні продажі** для 50 різних товарів у 10 різних магазинах (_Прогнозування часових рядів_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Підготовка Часових Рядів:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `train.csv` та `test.csv`.\n",
    "    *   1.2. *Оброблення дат:* Перетворіть стовпець 'date' на тип datetime та встановіть його як індекс (`pd.to_datetime`, `df.set_index`).\n",
    "    *   1.3. *Вибір та Агрегація:*\n",
    "        *   Розгляньте можливість почати з *підмножини* даних (наприклад, один магазин або кілька товарів), щоб прискорити розробку.\n",
    "        *   Створіть унікальний ідентифікатор для кожного часового ряду (комбінація 'store' та 'item').\n",
    "    *   1.4. *EDA Часових Рядів:*\n",
    "        *   Візуалізуйте кілька окремих часових рядів (продажі товару в магазині).\n",
    "        *   Дослідіть **тренди, сезонність** (річну, тижневу) та стаціонарність візуально та за допомогою статистичних тестів (наприклад, тест Дікі-Фуллера - ADF test з `statsmodels.tsa.stattools`).\n",
    "        *   Побудуйте графіки автокореляційної (ACF) та часткової автокореляційної (PACF) функцій для дослідження залежностей у рядах.\n",
    "    *   1.5. *Інженерія Часових Ознак:*\n",
    "        *   Створіть ознаки на основі дати: *день тижня, місяць, квартал, рік, день року*.\n",
    "        *   Створіть **лагові ознаки** (значення продажів за попередні дні).\n",
    "        *   Створіть **ознаки ковзного вікна** (наприклад, середнє/медіана/стандартне відхилення продажів за останні 7, 14, 30 днів).\n",
    "\n",
    "2.  **Побудова Базових Моделей Прогнозування:**\n",
    "    *   2.1. *Вибір Рядів:* Виберіть кілька репрезентативних часових рядів для побудови базових моделей.\n",
    "    *   2.2. *Реалізація Статистичних Моделей:*\n",
    "        *   Реалізуйте прості базові моделі: Naive forecast (прогноз = останнє відоме значення), Moving Average.\n",
    "        *   Реалізуйте класичні статистичні моделі, такі як ARIMA/SARIMA (`statsmodels.tsa.arima.model.ARIMA`) та експоненційне згладжування (наприклад, Holt-Winters, `statsmodels.tsa.holtwinters.ExponentialSmoothing`). Визначте порядки моделей (p, d, q) та сезонні компоненти (P, D, Q, s) на основі аналізу ACF/PACF та сіткового пошуку.\n",
    "    *   2.3. *Оцінка:* Оцініть продуктивність моделей на відкладеній частині даних (validation set), використовуючи метрики MAE, RMSE, MAPE (або SMAPE, якщо використовується в змаганні).\n",
    "\n",
    "3.  **Прогнозування за допомогою Машинного Навчання та Інтерфейс:**\n",
    "    *   3.1. *Підготовка Даних для ML:* Сформуйте табличний набір даних, де кожен рядок представляє дату, магазин, товар, а стовпці - створені часові/лагові ознаки та цільову змінну 'sales'.\n",
    "    *   3.2. *Реалізація ML Моделей:* Навчіть моделі машинного навчання (наприклад, Random Forest Regressor, XGBoost Regressor, LightGBM) на створеному табличному наборі даних.\n",
    "    *   3.3. *Реалізація Моделей Глибокого Навчання (Опціонально):* Розгляньте використання рекурентних нейронних мереж (LSTM, GRU) для моделювання послідовностей. Це потребує підготовки даних у вигляді послідовностей.\n",
    "    *   3.4. *Порівняння Продуктивності:* ***Порівняйте продуктивність*** статистичних моделей, ML-моделей та (якщо реалізовано) DL-моделей на валідаційній вибірці.\n",
    "    *   3.5. *Прогноз на Тестовому Наборі:* Використовуючи найкращу модель, згенеруйте прогнози для даних у `test.csv`. Формат подання залежить від вимог змагання Kaggle.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель (ML або статистичну) та необхідні компоненти (наприклад, список ознак, скейлер, якщо використовувався).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Фінальним результатом є **інтерактивний вебзастосунок**, створений за допомогою `streamlit`, `gradio` або `replit`, який дає змогу користувачеві *вибрати магазин та товар* і візуалізувати **історичні дані про продажі разом із прогнозом**, згенерованим найкращою моделлю.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `matplotlib`/`plotly`, `joblib`/`pickle`, можливо `statsmodels`.\n",
    "    3.  *Завантаження даних/моделі:* Завантажте історичні дані (або їх частину) та збережену модель/компоненти. Для ML моделей може знадобитися функція для генерації ознак на льоту. Для статистичних - завантаження навчених параметрів або сама модель.\n",
    "    4.  *Створення інтерфейсу:* Додайте випадаючі списки (`st.selectbox`/`gr.Dropdown`) для вибору магазину ('store') та товару ('item'). Додайте елемент для вибору горизонту прогнозування (кількість днів).\n",
    "    5.  *Оброблення вибору користувача:* Отримайте вибрані магазин та товар. Відфільтруйте відповідний часовий ряд з історичних даних.\n",
    "    6.  *Генерація прогнозу:*\n",
    "        *   (ML) Згенеруйте необхідні лагові/часові ознаки для майбутніх дат. Зробіть прогноз завантаженою моделлю.\n",
    "        *   (Статистичні) Використайте метод `.predict()` або `.forecast()` завантаженої моделі `statsmodels`.\n",
    "    7.  *Відображення результату:*\n",
    "        *   Побудуйте графік (`st.line_chart`, `st.plotly_chart` / `gr.Plot`) що показує *історичні продажі* та *згенерований прогноз*.\n",
    "        *   Опціонально, відобразіть прогноз у вигляді таблиці.\n",
    "    8.  *Запуск та Розгортання:* Як у попередніх темах.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/компоненти, історичні дані (можливо, зразок), `requirements.txt` (включно з `statsmodels`, `xgboost`, якщо використовуються).\n",
    "    3.  *Налаштування та Запуск:* Як у попередніх темах.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `statsmodels`, `xgboost`, опціонально `keras`/`tensorflow`/`pytorch`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Графіки часових рядів, графіки декомпозиції (тренд, сезонність, залишки), графіки ACF/PACF, графіки прогнозу проти фактичних значень.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, MAPE, SMAPE.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   *Ефективно керуйте* множинними часовими рядами (можливо, знадобляться цикли або групування).\n",
    "*   _Інженерія ознак_ (лаги, ковзні середні, часові ознаки) є вирішальною для моделей машинного навчання.\n",
    "*   Почніть з *підмножини* товарів/магазинів для швидшого прототипування.\n",
    "*   Зверніть увагу на **правильну обробку дат** та індексів часових рядів.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 4: Інтелектуальна інформаційна система для сегментування клієнтів торгового центру\n",
    "\n",
    "**Набір даних:** Mall Customer Segmentation Data ([https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python](https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python))\n",
    "\n",
    "**Мета:** **Сегментувати клієнтів** торгового центру на окремі групи на основі 'Annual Income (k$)' та 'Spending Score (1-100)' (_Кластеризація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Попереднє Оброблення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте CSV файл за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Огляньте основні статистичні показники (`df.describe()`), типи даних (`df.info()`) та наявність пропусків (`df.isnull().sum()`).\n",
    "    *   1.3. *Вибір ознак для кластеризації:* Зосередьтеся на основних ознаках: 'Annual Income (k$)' та 'Spending Score (1-100)'. Опціонально розгляньте включення 'Age'.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Візуалізуйте розподіли вибраних ознак за допомогою гістограм та KDE-графіків.\n",
    "        *   Побудуйте **діаграму розсіювання** 'Annual Income' проти 'Spending Score', щоб візуально оцінити потенційні кластери.\n",
    "        *   Розгляньте парні діаграми (`pairplot` з `seaborn`), якщо включено більше двох ознак.\n",
    "    *   1.5. *Масштабування ознак:* ***Застосуйте `StandardScaler`*** до вибраних ознак, оскільки алгоритми кластеризації, що базуються на відстані (як K-Means), чутливі до масштабу даних.\n",
    "\n",
    "2.  **Застосування Алгоритмів Кластеризації:**\n",
    "    *   2.1. *K-Means Кластеризація:*\n",
    "        *   **Визначення оптимальної кількості кластерів (k):**\n",
    "            *   Використайте **метод ліктя (Elbow method):** Побудуйте графік залежності суми квадратів відстаней до центрів кластерів (WCSS - Within-Cluster Sum of Squares, атрибут `inertia_` в `sklearn.cluster.KMeans`) від кількості кластерів (k). Шукайте точку \"ліктя\" на графіку.\n",
    "            *   Використайте **силуетний аналіз (Silhouette analysis):** Обчисліть середній силуетний коефіцієнт (`silhouette_score` з `sklearn.metrics`) для різних значень k. Оберіть k, що дає найвищий середній бал силуету. Побудуйте силуетні графіки для візуальної оцінки якості кластерів.\n",
    "        *   Застосуйте алгоритм K-Means з **оптимальним k**, знайденим на попередньому кроці.\n",
    "        *   Призначте мітки кластерів кожному клієнту.\n",
    "    *   2.2. *DBSCAN Кластеризація:*\n",
    "        *   Застосуйте алгоритм DBSCAN (`sklearn.cluster.DBSCAN`).\n",
    "        *   Експериментуйте з параметрами `eps` (максимальна відстань між зразками для одного сусідства) та `min_samples` (кількість зразків у сусідстві для точки, щоб вважатися базовою). Можна використати евристики для вибору `eps` (наприклад, аналіз k-відстаней).\n",
    "        *   Проаналізуйте отримані кластери та викиди (позначені як -1).\n",
    "    *   2.3. *Візуалізація Кластерів:*\n",
    "        *   Створіть діаграму розсіювання 'Annual Income' проти 'Spending Score'.\n",
    "        *   **Забарвте точки** відповідно до міток кластерів, отриманих від K-Means та DBSCAN (окремі графіки).\n",
    "        *   Для K-Means можна також позначити центри кластерів.\n",
    "\n",
    "3.  **Аналіз та Інтерпретація Кластерів:**\n",
    "    *   3.1. *Профілювання Кластерів (K-Means):*\n",
    "        *   Обчисліть середні значення ознак ('Annual Income', 'Spending Score', 'Age', якщо використовувалася) для кожного кластера.\n",
    "        *   Візуалізуйте ці середні значення (наприклад, за допомогою стовпчастих діаграм або радарних діаграм).\n",
    "        *   ***Сформулюйте описові назви та характеристики*** для кожного сегмента клієнтів (наприклад, \"Ощадливі з низьким доходом\", \"Марнотрати з високим доходом\", \"Стандартні клієнти\").\n",
    "    *   3.2. *Порівняння Алгоритмів:* Порівняйте результати, отримані від K-Means та DBSCAN. Зверніть увагу на форму кластерів, кількість виявлених кластерів та обробку викидів (DBSCAN може ідентифікувати шум).\n",
    "    *   3.3. *Збереження результатів:* Збережіть мітки кластерів або об'єкти кластеризаторів, якщо потрібно для застосунку.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Кінцевим продуктом є **вебзастосунок для візуалізації та аналізу сегментації клієнтів**, розроблений за допомогою `streamlit`, `gradio` або `replit`. Застосунок повинен *інтерактивно відображати* діаграму розсіювання клієнтів, забарвлену за кластерами, та надавати короткий опис характеристик кожного сегмента.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit`:** (Найкраще підходить для візуалізації кластерів)\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`, `pandas`, `numpy`, `matplotlib`/`seaborn`/`plotly`, `sklearn`.\n",
    "    3.  *Завантаження даних та виконання кластеризації:* Завантажте дані, виконайте попередню обробку (масштабування) та застосуйте вибраний алгоритм кластеризації (наприклад, K-Means з оптимальним k). Додайте мітки кластерів до DataFrame. Можна також завантажити попередньо оброблені дані з мітками.\n",
    "    4.  *Створення інтерфейсу та візуалізацій:*\n",
    "        *   Використовуйте `st.title`, `st.write` для опису.\n",
    "        *   Створіть **інтерактивну діаграму розсіювання** (`st.scatter_chart` або `st.plotly_chart` з `plotly.express.scatter`), де точки забарвлені за мітками кластерів. Додайте спливаючі підказки (hover info) з деталями клієнта.\n",
    "        *   Опціонально: Додайте можливість вибору алгоритму кластеризації (K-Means/DBSCAN) або параметрів (k для K-Means) через бічну панель (`st.sidebar`).\n",
    "    5.  *Відображення профілів кластерів:* Виведіть описи сегментів, можливо, з візуалізацією середніх значень ознак для кожного кластера (`st.dataframe` або `st.bar_chart`).\n",
    "    6.  *Запуск та Розгортання:* Локально (`streamlit run app.py`), потім на `Streamlit Community Cloud` або аналогічній платформі.\n",
    "\n",
    "*   **Використання `gradio` / `replit`:**\n",
    "    *   `Gradio` менш орієнтований на такі інтерактивні візуалізації, але може відображати статичні графіки (`gr.Plot`) та описи.\n",
    "    *   `Replit` може хостити `streamlit` або `gradio` застосунок, кроки аналогічні Теми 1.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `scipy` (опціонально, для ієрархічної кластеризації)\n",
    "*   **Розгортання:** `streamlit` (рекомендовано для візуалізації), `plotly` (для інтерактивних графіків), `joblib` або `pickle` (якщо зберігаються моделі/об'єкти)\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Діаграми розсіювання, забарвлені за кластерами, графік методу ліктя, силуетний графік, графіки розподілу ознак для кожного кластера (box plots, bar charts для середніх значень).\n",
    "*   **Метрики оцінки:** Silhouette Score, Davies-Bouldin Index. **Основний акцент на інтерпретованості та змістовності кластерів**.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   _Масштабування ознак_ є важливим для K-Means та DBSCAN.\n",
    "*   Порівняйте форми кластерів та інтерпретації, отримані за допомогою різних алгоритмів (K-Means припускає сферичні кластери, DBSCAN знаходить кластери довільної форми).\n",
    "*   **Головна мета - отримати зрозумілі та корисні для бізнесу сегменти клієнтів**.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 5: Інтелектуальна інформаційна система рекомендації фільмів\n",
    "\n",
    "**Набір даних:** MovieLens Small Dataset (ml-latest-small) ([https://grouplens.org/datasets/movielens/latest/](https://grouplens.org/datasets/movielens/latest/)) або Kaggle ([https://www.kaggle.com/datasets/shubhammehta21/movie-lens-small-latest-dataset](https://www.kaggle.com/datasets/shubhammehta21/movie-lens-small-latest-dataset))\n",
    "\n",
    "**Мета:** Побудувати систему для **рекомендації фільмів** користувачам на основі їхніх минулих оцінок (_Система рекомендацій_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження та Дослідження Даних:**\n",
    "    *   1.1. *Завантаження файлів:* Завантажте `ratings.csv` та `movies.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Огляд даних:* Перегляньте структуру файлів, типи даних, кількість записів.\n",
    "    *   1.3. *Об'єднання даних:* ***Об'єднайте `ratings` та `movies`*** за 'movieId', щоб мати назви фільмів разом з оцінками.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Проаналізуйте розподіл оцінок (ratings).\n",
    "        *   Дослідіть кількість оцінок на одного користувача та на один фільм. Визначте найактивніших користувачів та найпопулярніші фільми.\n",
    "        *   Візуалізуйте ці розподіли (гістограми, стовпчасті діаграми).\n",
    "    *   1.5. *Підготовка даних (якщо використовується `surprise`):* Завантажте дані у формат, сумісний з бібліотекою `surprise` (`Reader`, `Dataset.load_from_df`). Розділіть на тренувальну (trainset) та тестову (testset) вибірки.\n",
    "\n",
    "2.  **Колаборативна Фільтрація (Collaborative Filtering - CF):**\n",
    "    *   2.1. *Побудова матриці користувач-елемент:* (Якщо не використовується `surprise`) Створіть матрицю, де рядки - користувачі, стовпці - фільми, а значення - оцінки. Ця матриця, ймовірно, буде *дуже розрідженою*. Використовуйте `scipy.sparse` матриці для ефективності.\n",
    "    *   2.2. *Обчислення подібності:*\n",
    "        *   **User-based CF:** Обчисліть подібність між користувачами (наприклад, косинусна подібність (`cosine_similarity`) на векторах оцінок користувачів).\n",
    "        *   **Item-based CF:** Обчисліть подібність між фільмами (наприклад, косинусна подібність на векторах оцінок, отриманих фільмами).\n",
    "    *   2.3. *Генерація рекомендацій (User-based):*\n",
    "        *   Для цільового користувача знайдіть K найбільш подібних користувачів.\n",
    "        *   Спрогнозуйте оцінку для фільмів, які цільовий користувач ще не оцінив, на основі оцінок подібних користувачів (наприклад, зважене середнє).\n",
    "        *   **Згенеруйте топ-N рекомендацій** (фільми з найвищими прогнозованими оцінками).\n",
    "    *   2.4. *Генерація рекомендацій (Item-based):*\n",
    "        *   Для цільового користувача розгляньте фільми, які він високо оцінив.\n",
    "        *   Знайдіть фільми, найбільш подібні до тих, що сподобалися користувачеві.\n",
    "        *   **Згенеруйте топ-N рекомендацій** з цих подібних фільмів, виключаючи ті, що користувач вже бачив.\n",
    "    *   2.5. *Використання бібліотеки `surprise` (рекомендовано):*\n",
    "        *   Реалізуйте алгоритми KNNBasic, KNNWithMeans, KNNBaseline для user-based та item-based CF.\n",
    "        *   Навчіть моделі на trainset.\n",
    "        *   Використовуйте метод `get_neighbors` для дослідження подібності та метод `predict` для прогнозування оцінок.\n",
    "\n",
    "3.  **Модельна Фільтрація та Створення Простого UI:**\n",
    "    *   3.1. *Матрична Факторизація (SVD):*\n",
    "        *   Реалізуйте алгоритм Singular Value Decomposition (SVD) або його варіанти (наприклад, SVDpp, NMF) за допомогою бібліотеки `surprise` або `scikit-learn` (`TruncatedSVD`).\n",
    "        *   Навчіть модель на тренувальних даних.\n",
    "    *   3.2. *Оцінка та Порівняння:*\n",
    "        *   Оцініть точність прогнозування оцінок (RMSE, MAE) для всіх реалізованих моделей (CF та SVD) на тестовій вибірці (`testset` в `surprise`).\n",
    "        *   Опціонально (складніше): Оцініть якість ранжування за допомогою метрик Precision@k, Recall@k, MAP@k, NDCG@k. Це потребує специфічної стратегії оцінки (наприклад, відкидання частини історії користувача та прогнозування цих елементів).\n",
    "        *   ***Порівняйте результати*** різних підходів.\n",
    "    *   3.3. *Генерація Рекомендацій (SVD):* Використовуйте навчену SVD модель для прогнозування оцінок для всіх фільмів, які користувач ще не бачив, та виберіть топ-N.\n",
    "    *   3.4. *Збереження Моделі:* Збережіть найкращу модель (`surprise.dump.dump` або `joblib`/`pickle`). Збережіть також мапінги внутрішніх ID `surprise` на оригінальні `userId` та `movieId`.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створений **простий вебзастосунок для рекомендації фільмів** на основі `streamlit`, `gradio` або `replit`. Застосунок повинен дозволяти користувачеві (наприклад, ввести свій `userId`) отримати **персоналізований список топ-N рекомендованих фільмів**, згенерований найкращою моделлю.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `surprise` (або `sklearn`, `scipy`), `joblib`/`pickle`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, датафрейм `movies.csv` (для отримання назв фільмів), мапінги ID (якщо потрібно).\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   Створіть поле для введення `userId` (`st.number_input`/`gr.Number`).\n",
    "        *   Додайте кнопку для генерації рекомендацій.\n",
    "    5.  *Оброблення вводу:* Отримайте `userId`. Перевірте, чи існує такий користувач у даних.\n",
    "    6.  *Генерація Рекомендацій:*\n",
    "        *   Створіть список фільмів, які цей користувач *ще не оцінив*.\n",
    "        *   Для кожного з цих фільмів отримайте прогноз оцінки за допомогою завантаженої моделі (`model.predict(userId, movieId)`).\n",
    "        *   Відсортуйте фільми за прогнозованою оцінкою у спадному порядку.\n",
    "        *   Виберіть топ-N (наприклад, 10) фільмів.\n",
    "    7.  *Відображення результату:* Покажіть список назв топ-N рекомендованих фільмів (можливо, з прогнозованими оцінками або жанрами). Використовуйте `st.write`, `st.table`/`st.dataframe` або компоненти `gradio`.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, `movies.csv`, мапінги, `requirements.txt` (включно з `surprise`, якщо використовується).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `scipy` (розріджені матриці, подібність), `surprise` (**рекомендовано**)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл оцінок, кількість оцінок на користувача/фільм.\n",
    "*   **Метрики оцінки:** RMSE, MAE (для прогнозування оцінок). Precision@k, Recall@k, MAP@k, NDCG@k (для оцінки ранжування, потребує _ретельного поділу_ та специфічної реалізації оцінки).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Використовуйте *розріджені матриці* (`scipy.sparse`), якщо реалізуєте CF вручну, для ефективності.\n",
    "*   Бібліотека `surprise` значно **спрощує реалізацію** та оцінку багатьох стандартних алгоритмів рекомендацій.\n",
    "*   Зверніть увагу на **\"холодний старт\"** (cold start problem) - як рекомендувати фільми новим користувачам або як рекомендувати нові фільми. (Це виходить за рамки базового проєкту, але варто згадати).\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 6: Інтелектуальна інформаційна система для виявлення шахрайства з кредитними картками\n",
    "\n",
    "**Набір даних:** Credit Card Fraud Detection ([https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud))\n",
    "\n",
    "**Мета:** **Виявляти шахрайські транзакції** з кредитними картками на основі *дуже незбалансованих даних* (_Виявлення аномалій / Класифікація незбалансованих даних_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження Даних та Аналіз Незбалансованості:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте дані за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз ознак:* Дослідіть анонімізовані ознаки ('V1' - 'V28'), а також 'Time' та 'Amount'. Ознаки V1-V28 є результатом PCA перетворення, що приховує оригінальні дані.\n",
    "    *   1.3. ***Аналіз дисбалансу класів:***\n",
    "        *   Перевірте розподіл цільової змінної 'Class' (0 - нормальна транзакція, 1 - шахрайська).\n",
    "        *   Обчисліть відсоток шахрайських транзакцій. Зверніть увагу на **крайній дисбаланс**.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Візуалізуйте розподіли ознак 'Time' та 'Amount'.\n",
    "        *   Порівняйте розподіли ознак V1-V28 для нормальних та шахрайських транзакцій (наприклад, за допомогою `boxplot` або `violinplot`).\n",
    "    *   1.5. *Масштабування:* **Застосуйте `RobustScaler`** до ознак 'Amount' та 'Time' (або всіх числових ознак), оскільки він менш чутливий до викидів, ніж `StandardScaler`. Ознаки V1-V28 вже можуть бути масштабовані, але перевірте їх розподіли.\n",
    "    *   1.6. *Розділення даних:* Розділіть дані на тренувальну та тестову вибірки, використовуючи **стратифікацію** за змінною 'Class' (`stratify=y` в `train_test_split`), щоб зберегти пропорцію класів в обох вибірках.\n",
    "\n",
    "2.  **Базове Виявлення Аномалій / Класифікація Незбалансованих Даних:**\n",
    "    *   2.1. *Стратегії для дисбалансу:*\n",
    "        *   **Зважування класів:** Навчіть базовий класифікатор, наприклад, Logistic Regression, встановивши параметр `class_weight='balanced'`.\n",
    "        *   **Алгоритми виявлення аномалій:** Застосуйте алгоритми, призначені для виявлення викидів, такі як Isolation Forest (`sklearn.ensemble.IsolationForest`) або Local Outlier Factor (`sklearn.neighbors.LocalOutlierFactor`). Ці алгоритми не використовують мітки 'Class' під час навчання (некероване навчання).\n",
    "        *   **Семплінг:** Використайте бібліотеку `imblearn` для застосування технік семплінгу:\n",
    "            *   _Oversampling_ міноритарного класу (наприклад, SMOTE - Synthetic Minority Over-sampling Technique).\n",
    "            *   _Undersampling_ мажоритарного класу (наприклад, RandomUnderSampler).\n",
    "            *   *Увага:* Застосовуйте семплінг **тільки до тренувальної вибірки**, щоб уникнути витоку даних у тестову.\n",
    "    *   2.2. *Навчання та Оцінка Базових Моделей:* Навчіть моделі (Logistic Regression з вагами, моделі після семплінгу) на тренувальних даних.\n",
    "    *   2.3. *Вибір Метрик Оцінки:* Через сильний дисбаланс, **Accuracy не є інформативною метрикою**. Зосередьтеся на:\n",
    "        *   Precision, Recall, $F_1$-score (особливо для шахрайського класу, Class=1).\n",
    "        *   AUC-ROC (Area Under the Receiver Operating Characteristic Curve).\n",
    "        *   ***AUC-PR (Area Under the Precision-Recall Curve):*** Ця метрика є *особливо важливою* для сильно незбалансованих даних.\n",
    "    *   2.4. *Оцінка:* Оцініть базові моделі на тестовій вибірці за допомогою вибраних метрик. Візуалізуйте **Precision-Recall криву** та ROC-криву. Проаналізуйте матрицю плутанини.\n",
    "\n",
    "3.  **Просунуті Техніки та Оцінка:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть *просунуті моделі*, які добре працюють з незбалансованими даними та великими наборами, такі як XGBoost (`xgboost.XGBClassifier`) та LightGBM (`lightgbm.LGBMClassifier`). Ці моделі часто мають вбудовані параметри для роботи з дисбалансом (наприклад, `scale_pos_weight`).\n",
    "    *   3.2. *Порівняння Стратегій:* Порівняйте продуктивність різних підходів: використання `class_weight`/`scale_pos_weight`, різних технік семплінгу (`imblearn`) в поєднанні з різними класифікаторами.\n",
    "    *   3.3. *Оптимізація:* Налаштуйте гіперпараметри найкращих моделей, **зосереджуючись на максимізації AUC-PR** або $F_1$-score для шахрайського класу.\n",
    "    *   3.4. *Збереження Моделі:* Збережіть найкращу модель та необхідні об'єкти (скейлер).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок для демонстрації виявлення шахрайства з кредитними картками** на `streamlit`, `gradio` або `replit`. Застосунок може приймати на вхід (можливо, симульовані або вибрані) ознаки транзакції ('Amount', 'Time', та анонімізовані 'V' ознаки) і показувати ймовірність того, що транзакція є шахрайською.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm` (залежно від моделі).\n",
    "    3.  *Завантаження моделі:* Завантажте збережену модель та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для ключових ознак. Оскільки ознаки V1-V28 анонімізовані та їх багато, можливо, варто дозволити користувачеві вводити лише 'Time' та 'Amount', а інші ознаки встановити за замовчуванням (наприклад, медіанні значення з тренувальної вибірки) або створити спрощений інтерфейс, де користувач вибирає типовий сценарій. _Це потребує творчого підходу до дизайну інтерфейсу._\n",
    "    5.  *Оброблення вводу:* Зберіть дані користувача. Сформуйте вектор ознак у правильному порядку. **Застосуйте збережений скейлер `RobustScaler`** до 'Time' та 'Amount'.\n",
    "    6.  *Прогнозування:* Отримайте прогноз від моделі (`model.predict_proba()`). Нас цікавить ймовірність класу 1 (шахрайство).\n",
    "    7.  *Відображення результату:* Покажіть ймовірність шахрайства та/або висновок (наприклад, \"Низький ризик\", \"Високий ризик\" на основі порогу ймовірності).\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt` (включно з `imblearn`, `xgboost`, `lightgbm`, якщо використовуються).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, `imblearn`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл класів, **крива Precision-Recall (обов'язково!)**, ROC-крива, матриця плутанини.\n",
    "*   **Метрики оцінки:** Precision, Recall, $F_1$-score (особливо для класу шахрайства), AUC-ROC, _**AUC-PR (Precision-Recall Area)**_.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Використовуйте `RobustScaler` через потенційні викиди в 'Amount'.\n",
    "*   Наголошуйте на **метриках, окрім accuracy**, через сильний дисбаланс. **AUC-PR є ключовою метрикою**.\n",
    "*   Використовуйте `StratifiedKFold` для крос-валідації, щоб зберегти пропорції класів.\n",
    "*   Порівняння різних стратегій роботи з дисбалансом (зважування, семплінг) є важливою частиною проєкту.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 7: Інтелектуальна інформаційна система для розпізнавання рукописних цифр та зниження розмірності\n",
    "\n",
    "**Набір даних:** MNIST Handwritten Digit Database (CSV версія: [https://www.kaggle.com/datasets/oddrationale/mnist-in-csv](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)) або використати `fetch_openml('mnist_784', as_frame=True)` з `sklearn.datasets`.\n",
    "\n",
    "**Мета:** **Класифікувати рукописні цифри** (0-9) та дослідити *зниження розмірності* для візуалізації та ефективності класифікації (_Класифікація / Зниження розмірності_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних та Базова Класифікація:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте дані MNIST (CSV або через `fetch_openml`). Дані представляють собою 784 пікселі (28x28) для кожного зображення та відповідну мітку цифри (0-9).\n",
    "    *   1.2. *Підготовка даних:*\n",
    "        *   Розділіть дані на ознаки (пікселі) та цільову змінну (мітки цифр).\n",
    "        *   *Масштабуйте значення пікселів* (зазвичай діленням на 255.0 для нормалізації до діапазону [0, 1]).\n",
    "        *   Розділіть дані на тренувальну та тестову вибірки. Розгляньте використання **меншої підмножини даних** (наприклад, 10-20 тис. записів) для прискорення експериментів, особливо для t-SNE та налаштування гіперпараметрів.\n",
    "    *   1.3. *Візуалізація даних:* Відобразіть кілька прикладів зображень цифр з набору даних за допомогою `matplotlib`.\n",
    "    *   1.4. *Навчання базових класифікаторів:* Навчіть прості класифікатори (наприклад, Logistic Regression, SVM з лінійним або RBF ядром, K-NN) на **оригінальних 784 ознаках**.\n",
    "    *   1.5. *Оцінка:* Оцініть продуктивність базових моделей на тестовій вибірці. Використовуйте Accuracy, Classification Report (з Precision, Recall, F1 для кожної цифри) та матрицю плутанини. Виміряйте час навчання та прогнозування.\n",
    "\n",
    "2.  **Техніки Зниження Розмірності:**\n",
    "    *   2.1. *Застосування PCA (Principal Component Analysis):*\n",
    "        *   Навчіть `PCA` (`sklearn.decomposition.PCA`) на тренувальних даних (масштабованих).\n",
    "        *   Проаналізуйте **пояснену дисперсію**: Побудуйте графік кумулятивної поясненої дисперсії в залежності від кількості компонент. Визначте, скільки компонент потрібно для збереження певного відсотка дисперсії (наприклад, 95% або 99%).\n",
    "        *   Трансформуйте тренувальні та тестові дані, використовуючи обрану кількість компонент PCA.\n",
    "    *   2.2. *Застосування t-SNE (t-distributed Stochastic Neighbor Embedding):*\n",
    "        *   Застосуйте `t-SNE` (`sklearn.manifold.TSNE`), зазвичай з `n_components=2`, до **підмножини** тренувальних даних (t-SNE є обчислювально інтенсивним) для візуалізації. Експериментуйте з параметром `perplexity`.\n",
    "        *   *Примітка:* t-SNE переважно використовується для візуалізації, а не для зниження розмірності перед класифікацією, оскільки він не зберігає глобальну структуру так само добре, як PCA, і не має методу `transform` для нових даних у тому ж сенсі.\n",
    "    *   2.3. *Візуалізація Зменшених Даних:*\n",
    "        *   Створіть **2D діаграму розсіювання**, використовуючи перші дві компоненти PCA. Забарвте точки відповідно до міток цифр.\n",
    "        *   Створіть **2D діаграму розсіювання** результатів t-SNE. Забарвте точки за мітками цифр. Порівняйте якість візуального розділення класів між PCA та t-SNE.\n",
    "\n",
    "3.  **Класифікація на Зменшених Даних та Порівняння:**\n",
    "    *   3.1. *Навчання класифікаторів на PCA-даних:* Навчіть ті самі класифікатори (Logistic Regression, SVM, K-NN) на даних, **зменшених за допомогою PCA** (з вибраною кількістю компонент, наприклад, що зберігають 95% дисперсії).\n",
    "    *   3.2. *Оцінка та Порівняння:*\n",
    "        *   Оцініть продуктивність моделей на тестовій вибірці, трансформованій PCA.\n",
    "        *   ***Порівняйте результати*** (Accuracy, Classification Report, час навчання/прогнозування) моделей, навчених на оригінальних даних та на PCA-зменшених даних.\n",
    "        *   **Обговоріть компроміси:** Як зниження розмірності вплинуло на точність, швидкість навчання/прогнозування та складність моделі?\n",
    "    *   3.3. *Збереження Моделі/Компонентів:* Збережіть найкращу модель (на оригінальних чи PCA-даних) та об'єкти PCA/скейлера.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який виконує одну або декілька з наступних функцій:\n",
    "*   Візуалізує 2D-ембединги даних MNIST (PCA або t-SNE), забарвлені за класами.\n",
    "*   Дає змогу користувачеві завантажити (або *намалювати*, просунутий варіант) зображення цифри та класифікує його за допомогою найкращої навченої моделі.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `matplotlib`/`PIL` (для обробки/відображення зображень).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, об'єкт PCA (якщо модель використовує PCA-дані).\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   **Варіант 1 (Візуалізація ембедингів):** Додайте можливість відобразити 2D PCA/t-SNE графік (`st.scatter_chart`, `st.plotly_chart`).\n",
    "        *   **Варіант 2 (Класифікація):**\n",
    "            *   (`streamlit`) Використовуйте `st.file_uploader` для завантаження зображення або компонент `streamlit-drawable-canvas` (потребує встановлення) для малювання.\n",
    "            *   (`gradio`) Використовуйте `gr.Image` для завантаження/малювання.\n",
    "    5.  *Оброблення вводу (Класифікація):*\n",
    "        *   Якщо зображення завантажено/намальовано, перетворіть його на формат, сумісний з моделлю: зображення 28x28 у відтінках сірого, розгорнуте у вектор 784 пікселів.\n",
    "        *   **Застосуйте масштабування** (ділення на 255.0).\n",
    "        *   Якщо модель навчена на PCA-даних, **застосуйте збережений PCA об'єкт** (`pca.transform()`).\n",
    "    6.  *Прогнозування (Класифікація):* Зробіть прогноз за допомогою завантаженої моделі (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану цифру. Якщо є можливість, відобразіть вхідне зображення.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, PCA об'єкт, `requirements.txt` (включно з `streamlit-drawable-canvas`, якщо використовується).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `PIL` (або `opencv-python`), опціонально `streamlit-drawable-canvas`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Приклади зображень цифр, графік поясненої дисперсії PCA, **2D діаграми розсіювання ембедингів PCA/t-SNE (забарвлені за класом)**, матриця плутанини.\n",
    "*   **Метрики оцінки:** Accuracy, Classification Report (Precision, Recall, F1 для кожної цифри), час навчання/прогнозування.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Використовуйте _підмножину MNIST_ для швидшого дослідження, якщо необхідно, особливо для t-SNE.\n",
    "*   Порівняйте *якість візуалізації* PCA та t-SNE. t-SNE зазвичай дає краще візуальне розділення, але є повільнішим і не підходить для трансформації нових даних.\n",
    "*   Обговоріть **компроміси зниження розмірності** щодо точності та швидкості.\n",
    "*   Реалізація компонента для малювання цифри є *просунутим, але цікавим* доповненням.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 8: Інтелектуальна інформаційна система для прогнозування цін на Airbnb для міст\n",
    "\n",
    "**Набір даних:** Дані Inside Airbnb для конкретного міста (наприклад, New York City `listings.csv.gz`). Направте студентів на [http://insideairbnb.com/get-the-data/](http://insideairbnb.com/get-the-data/), щоб вибрати місто/дату. (Приклад URL для NYC, березень 2023: [http://data.insideairbnb.com/united-states/ny/new-york-city/2023-03-14/data/listings.csv.gz](http://data.insideairbnb.com/united-states/ny/new-york-city/2023-03-14/data/listings.csv.gz) - ***перевірте актуальне посилання перед використанням***)\n",
    "\n",
    "**Мета:** **Прогнозувати добову ціну** ('price') на житло Airbnb на основі ознак, таких як місцезнаходження, тип нерухомості, зручності, відгуки тощо (_Регресія_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та Інженерія Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте файл `listings.csv` (може бути великим, `.csv.gz`) за допомогою `pandas`.\n",
    "    *   1.2. *Попередній аналіз та Очищення:*\n",
    "        *   Дослідіть **велику кількість стовпців** та їх типи. Виберіть потенційно релевантні ознаки для прогнозування ціни (наприклад, 'host\\_response\\_rate', 'host\\_is\\_superhost', 'neighbourhood\\_cleansed', 'latitude', 'longitude', 'property\\_type', 'room\\_type', 'accommodates', 'bathrooms\\_text', 'bedrooms', 'beds', 'amenities', 'price', 'minimum\\_nights', 'number\\_of\\_reviews', 'review\\_scores\\_rating', 'instant\\_bookable' тощо).\n",
    "        *   **Очистіть ознаку 'price':** Видаліть символ '$' та коми, перетворіть на числовий тип. Обробіть викиди або нереалістичні ціни.\n",
    "        *   Обробіть ***численні пропущені значення*** в вибраних стовпцях (імпутація середнім/медіаною/модою, створення індикаторних ознак пропуску, видалення рядків/стовпців з великою кількістю пропусків).\n",
    "        *   Очистіть та стандартизуйте інші ознаки (наприклад, 'bathrooms\\_text').\n",
    "    *   1.3. *Інженерія Ознак:*\n",
    "        *   **Розпарсіть ознаку 'amenities':** Це складний, але *дуже важливий* крок. Перетворіть список зручностей (рядок) на множинні бінарні ознаки (наприклад, 'has\\_wifi', 'has\\_kitchen', 'has\\_pool') або на ознаку 'amenities\\_count'.\n",
    "        *   Створіть геопросторові ознаки (якщо можливо та релевантно): відстань до центру міста, щільність інших оголошень поруч (потребує додаткових бібліотек або розрахунків).\n",
    "        *   Розгляньте створення часових ознак, якщо дані містять відповідну інформацію (наприклад, 'host\\_since').\n",
    "    *   1.4. *Кодування категоріальних ознак:* Закодуйте 'neighbourhood\\_cleansed', 'property\\_type', 'room\\_type' та інші категоріальні ознаки (One-Hot Encoding, Target Encoding).\n",
    "    *   1.5. *EDA:*\n",
    "        *   Візуалізуйте розподіл ціни 'price'. **Застосуйте логарифмічне перетворення** (`np.log1p`), оскільки ціни часто мають сильний правий хвіст.\n",
    "        *   Проаналізуйте кореляції між числовими ознаками та (логарифмованою) ціною.\n",
    "        *   Візуалізуйте залежність ціни від категоріальних ознак (наприклад, 'room\\_type', 'neighbourhood\\_cleansed') за допомогою `boxplot`.\n",
    "\n",
    "2.  **Базова Регресія та Геопросторовий Аналіз:**\n",
    "    *   2.1. *Підготовка даних:* Розділіть дані на тренувальну/тестову вибірки. Застосуйте масштабування (`StandardScaler`) до числових ознак.\n",
    "    *   2.2. *Реалізація Базових Моделей:* Навчіть моделі Linear Regression, Ridge, Lasso на підготовлених даних (використовуючи логарифмовану ціну як цільову змінну).\n",
    "    *   2.3. *Оцінка:* Оцініть моделі за допомогою MAE, MSE, RMSE, R-squared (на логарифмованій шкалі). Для інтерпретації можна перетворити прогнози назад на оригінальну шкалу (`np.expm1()`) і обчислити метрики там, але оптимізувати краще на логарифмованій шкалі.\n",
    "    *   2.4. *Геопросторовий Аналіз:*\n",
    "        *   ***Візуалізуйте ціни на карті*** міста, використовуючи 'latitude' та 'longitude'. Колір або розмір точок може відображати ціну або залишки моделі. Використовуйте `matplotlib`/`seaborn` з `scatter` або бібліотеки `folium` чи `geopandas` для інтерактивних карт.\n",
    "        *   Проаналізуйте, як місцезнаходження (район) впливає на ціну та чи враховують це моделі.\n",
    "\n",
    "3.  **Розширене Моделювання та Важливість Ознак:**\n",
    "    *   3.1. *Реалізація Просунутих Моделей:* Навчіть _просунуті моделі_, такі як Random Forest Regressor, XGBoost Regressor, LightGBM Regressor.\n",
    "    *   3.2. *Налаштування Гіперпараметрів:* Оптимізуйте гіперпараметри для кращих моделей за допомогою крос-валідації.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх моделей.\n",
    "    *   3.4. *Аналіз Важливості Ознак:* **Визначте ключові фактори**, що впливають на ціну Airbnb у вибраному місті, аналізуючи важливість ознак з моделей Random Forest/XGBoost. Зверніть увагу на вплив зручностей ('amenities'), місцезнаходження та характеристик житла.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та всі необхідні компоненти для попереднього оброблення (скейлер, кодувальники, список ознак, можливо, список зручностей).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Розроблено **вебзастосунок на `streamlit`, `gradio` або `replit` для оцінки добової ціни житла Airbnb** у вибраному місті. Користувач повинен мати змогу ввести ключові характеристики житла (тип, район, кількість спалень/ванних кімнат, основні зручності) та отримати прогнозовану ціну.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, кодувальники, список ознак, можливо, список районів/типів нерухомості/зручностей, що використовувалися моделлю.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи вводу для користувача: випадаючі списки для району (`neighbourhood_cleansed`), типу кімнати (`room_type`), типу нерухомості (`property_type`); числові поля для кількості гостей (`accommodates`), спалень, ванних; прапорці (`st.checkbox`) або мультиселект (`st.multiselect`) для ключових зручностей (`amenities`).\n",
    "    5.  *Оброблення вводу:* Зберіть дані користувача. **Дуже важливо застосувати ту саму послідовність попереднього оброблення, що й під час навчання**:\n",
    "        *   Створіть DataFrame з введеними даними.\n",
    "        *   Закодуйте категоріальні ознаки (використовуючи завантажені кодувальники або списки категорій).\n",
    "        *   Створіть бінарні ознаки для вибраних зручностей.\n",
    "        *   Переконайтесь, що всі необхідні моделі ознаки присутні та в правильному порядку.\n",
    "        *   Застосуйте збережений скейлер до числових ознак.\n",
    "    6.  *Прогнозування:* Зробіть прогноз за допомогою моделі на підготовленому векторі ознак. **Застосуйте обернене логарифмічне перетворення** (`np.expm1()`) до результату.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану добову ціну.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, кодувальники, списки категорій/зручностей, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, опціонально `folium` або `geopandas` для карт.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл цін (*логарифмоване перетворення*), візуалізація цін/залишків на карті, діаграми розсіювання, діаграма важливості ознак.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, R-squared (оптимально на логарифмованій шкалі, але можна звітувати і на оригінальній після зворотного перетворення).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Потребує _**значного очищення даних**_ та обробки пропусків.\n",
    "*   **Розбір ознаки 'amenities'** є складним, але дуже цінним для моделі.\n",
    "*   **Логарифмуйте цільову змінну 'price'**.\n",
    "*   Аналіз **важливості ознак** допомагає зрозуміти ключові фактори, що визначають ціни на ринку Airbnb у конкретному місті.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 9: Інтелектуальна інформаційна система для прогнозування звільнення співробітників\n",
    "\n",
    "**Набір даних:** IBM HR Analytics Employee Attrition & Performance ([https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset))\n",
    "\n",
    "**Мета:** **Прогнозувати звільнення співробітників** ('Attrition') на основі таких факторів, як посада, задоволеність, зарплата, стаж (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Візуалізація Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте CSV файл за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Перевірте типи даних, наявність пропусків (в цьому наборі даних їх зазвичай немає), унікальні значення.\n",
    "    *   1.3. *Аналіз цільової змінної:*\n",
    "        *   Дослідіть розподіл цільової змінної 'Attrition' ('Yes'/'No').\n",
    "        *   Обчисліть рівень звільнень (attrition rate). Зверніть увагу на **дисбаланс класів**.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Візуалізуйте взаємозв'язки між ключовими ознаками та звільненням ('Attrition'). Використовуйте:\n",
    "            *   Груповані стовпчасті діаграми (countplot) для категоріальних ознак ('Department', 'JobRole', 'MaritalStatus').\n",
    "            *   Коробкові діаграми (boxplot) або скрипкові діаграми (violinplot) для числових ознак ('Age', 'MonthlyIncome', 'JobSatisfaction', 'YearsAtCompany') проти 'Attrition'.\n",
    "        *   Проаналізуйте кореляції між числовими ознаками за допомогою теплової карти.\n",
    "\n",
    "2.  **Інженерія Ознак та Побудова Базових Моделей:**\n",
    "    *   2.1. *Кодування категоріальних ознак:* Перетворіть бінарні ознаки ('Attrition', 'Gender', 'OverTime') на числові (0/1). Закодуйте інші категоріальні ознаки ('BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus') за допомогою One-Hot Encoding (`pd.get_dummies`).\n",
    "    *   2.2. *Масштабування ознак:* Застосуйте `StandardScaler` до числових ознак.\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки, використовуючи стратифікацію за 'Attrition' (`stratify=y`).\n",
    "    *   2.4. *Навчання базових класифікаторів:*\n",
    "        *   Навчіть Logistic Regression та Decision Tree.\n",
    "        *   **Врахуйте дисбаланс класів:** Використовуйте параметр `class_weight='balanced'` у моделях, які це підтримують.\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на тестовій вибірці. Через дисбаланс, зосередьтеся на: Precision, Recall, $F_1$-score (особливо для класу 'Yes'), AUC-ROC та **AUC-PR**. Проаналізуйте матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Аналіз Рушійних Факторів:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть _просунуті класифікатори_, такі як Random Forest, XGBoost, LightGBM. Вони часто добре справляються з дисбалансом або мають параметри для його врахування (`scale_pos_weight`).\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Використовуйте `GridSearchCV` або `RandomizedSearchCV` для оптимізації моделей, максимізуючи AUC-PR або $F_1$-score для класу 'Yes'.\n",
    "    *   3.3. *Порівняння моделей:* Порівняйте продуктивність усіх підходів (базові з `class_weight`, просунуті моделі).\n",
    "    *   3.4. *Аналіз Рушійних Факторів Звільнення:*\n",
    "        *   **Проаналізуйте важливість ознак** (feature importance) з моделей Random Forest/XGBoost. Визначте ключові фактори, що найбільше корелюють зі звільненням.\n",
    "        *   _Просунуто (опціонально):_ Використайте бібліотеку **SHAP (`shap`)** для глибшої інтерпретації моделі, щоб зрозуміти внесок кожної ознаки у прогноз для окремих співробітників або в середньому.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та об'єкти для попереднього оброблення (скейлер, кодувальники/список стовпців).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **інтерактивний вебзастосунок на `streamlit`, `gradio` або `replit`**, який дає змогу:\n",
    "*   Ввести дані (гіпотетичного) співробітника.\n",
    "*   Отримати **прогноз ризику звільнення** ('Attrition').\n",
    "*   (Опціонально, просунуто) Відобразити **ключові фактори**, що вплинули на цей конкретний прогноз (наприклад, за допомогою SHAP values або просто списку найважливіших ознак моделі).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`, опціонально `shap`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, список категорій/стовпців (для кодування). Якщо використовується SHAP, завантажте пояснювач (`explainer`).\n",
    "    4.  *Створення інтерфейсу:* Створіть відповідні елементи введення для основних ознак, що використовуються моделлю (наприклад, `st.number_input` для 'Age', 'MonthlyIncome'; `st.selectbox` для 'JobRole', 'MaritalStatus'; `st.radio` для 'OverTime').\n",
    "    5.  *Оброблення вводу:* Зберіть дані користувача. **Застосуйте ту саму попередню обробку**: кодування категоріальних ознак (переконайтесь, що всі стовпці присутні, як у тренувальних даних), масштабування числових.\n",
    "    6.  *Прогнозування:* Отримайте ймовірність звільнення (`model.predict_proba()[:, 1]`).\n",
    "    7.  *Відображення результату:* Покажіть ймовірність звільнення або текстовий висновок (\"Високий/Низький ризик\").\n",
    "    8.  *Відображення Факторів (опціонально):* Якщо використовується SHAP, обчисліть SHAP values для введених даних і відобразіть графік (`st.pyplot(shap.force_plot(...))` або `shap.waterfall_plot`). Або просто виведіть список найважливіших ознак моделі.\n",
    "    9.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, список стовпців, `requirements.txt` (включно з `shap`, якщо використовується).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, опціонально `shap`, `lightgbm`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, опціонально `shap`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіли ознак проти 'Attrition', кореляційна теплова карта, **діаграма важливості ознак**, матриця плутанини, ROC-крива, **PR-крива**, візуалізації SHAP (якщо використовується).\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall (для 'Yes'), $F_1$-score (для 'Yes'), AUC-ROC. Розгляньте _**AUC-PR**_, якщо дисбаланс значний.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   *Врахуйте дисбаланс класів*, якщо необхідно (наприклад, за допомогою `class_weight` або `scale_pos_weight`).\n",
    "*   **Зосередьтеся на інтерпретації важливості ознак** для отримання бізнес-інсайтів щодо факторів, які призводять до звільнень.\n",
    "*   Використання SHAP може значно покращити розуміння прогнозів моделі (але є просунутою технікою).\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 10: Інтелектуальна інформаційна система для прогнозування індексу якості повітря (AQI)\n",
    "\n",
    "**Набір даних:** Air Quality Dataset in India ([https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india](https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india)). Рекомендується зосередитися на конкретному місті/станції (наприклад, з `city_day.csv` або `station_day.csv`).\n",
    "\n",
    "**Мета:** **Прогнозувати AQI** (Air Quality Index) або рівні конкретних забруднювачів (наприклад, PM2.5) на майбутній момент часу (наприклад, наступний день), використовуючи історичні дані (_Прогнозування часових рядів / Регресія_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та Аналіз Часових Рядів:**\n",
    "    *   1.1. *Вибір локації та цілі:* Виберіть конкретне місто або станцію для аналізу (наприклад, з файлу `city_day.csv`). Визначте цільову змінну: прогнозувати AQI (потребує розрахунку або використання наявної колонки) або рівень конкретного забруднювача (наприклад, 'PM2.5', 'NO2').\n",
    "    *   1.2. *Завантаження та підготовка даних:* Завантажте відповідний файл (`city_day.csv` або `station_day.csv`). Відфільтруйте дані для вибраної локації.\n",
    "    *   1.3. *Оброблення дат та пропусків:*\n",
    "        *   Перетворіть стовпець дати ('Date') на тип datetime та встановіть як індекс. Переконайтесь, що дані відсортовані за датою.\n",
    "        *   Дослідіть та **обробіть пропущені значення** в ознаках забруднювачів та цільовій змінній. Стратегії імпутації для часових рядів включають: інтерполяцію (`.interpolate()`), заповнення попереднім/наступним значенням (`.fillna(method='ffill'/'bfill')`), або використання більш складних методів.\n",
    "    *   1.4. *Розрахунок AQI (Опціонально):* Якщо AQI не надано, його можна розрахувати на основі концентрацій окремих забруднювачів за офіційними формулами (наприклад, індійськими стандартами AQI). Це може бути складним кроком. Альтернативно, зосередьтеся на прогнозуванні одного забруднювача, наприклад, 'PM2.5'.\n",
    "    *   1.5. *EDA Часових Рядів:*\n",
    "        *   Візуалізуйте часовий ряд цільової змінної (AQI або PM2.5).\n",
    "        *   Дослідіть тренди, сезонність (річну, можливо тижневу) та стаціонарність (візуально, тест ADF, ACF/PACF).\n",
    "        *   Проаналізуйте кореляції між різними забруднювачами та цільовою змінною.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Прогнозування:**\n",
    "    *   2.1. *Інженерія Часових Ознак:*\n",
    "        *   Створіть ознаки на основі дати (день тижня, місяць, рік тощо).\n",
    "        *   Створіть **лагові ознаки** для цільової змінної та інших релевантних забруднювачів (значення за попередні дні).\n",
    "        *   Створіть **ознаки ковзного вікна** (середнє, медіана тощо за останні N днів).\n",
    "    *   2.2. *Побудова Базових Моделей:*\n",
    "        *   **Статистичні моделі:** Застосуйте ARIMA/SARIMA або ETS (Holt-Winters) до часового ряду цільової змінної. Використовуйте ACF/PACF для визначення порядків моделі.\n",
    "        *   **Прості регресійні моделі:** Використовуйте створені часові/лагові ознаки для навчання простих регресійних моделей (наприклад, Linear Regression, Ridge) для прогнозування цільової змінної.\n",
    "    *   2.3. *Оцінка:* Оцініть базові моделі на валідаційній частині даних (останній період часу). Використовуйте метрики MAE, MSE, RMSE, MAPE.\n",
    "\n",
    "3.  **Розширене Моделювання та Розгортання:**\n",
    "    *   3.1. *Застосування ML Моделей:* Навчіть моделі машинного навчання (Random Forest Regressor, XGBoost Regressor, LightGBM) на табличному наборі даних зі створеними ознаками.\n",
    "    *   3.2. *Застосування DL Моделей (Опціонально):* Розгляньте використання LSTM або GRU, подаючи на вхід послідовності минулих значень забруднювачів та/або цільової змінної.\n",
    "    *   3.3. *Порівняння Продуктивності:* ***Порівняйте*** продуктивність статистичних, ML та DL (якщо реалізовано) моделей на валідаційній вибірці.\n",
    "    *   3.4. *Аналіз Важливості Ознак (для ML):* Визначте, які лаги, часові ознаки або інші забруднювачі найбільше впливають на прогноз цільової змінної.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та необхідні компоненти (список ознак, скейлер, якщо використовувався).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який для вибраної локації (міста/станції):\n",
    "*   Відображає **історичні дані** про якість повітря (AQI або конкретний забруднювач).\n",
    "*   Показує **прогноз** на найближчий період (наприклад, наступні кілька днів), згенерований найкращою моделлю.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `matplotlib`/`plotly`, `joblib`/`pickle`, `statsmodels`/`xgboost` тощо.\n",
    "    3.  *Завантаження ресурсів:* Завантажте історичні дані для вибраної локації, збережену модель/компоненти.\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   Відобразіть назву вибраної локації.\n",
    "        *   Дозвольте користувачеві (опціонально) вибрати горизонт прогнозування.\n",
    "    5.  *Генерація прогнозу:*\n",
    "        *   Підготуйте дані для прогнозу (створіть необхідні лагові/часові ознаки для майбутніх дат).\n",
    "        *   Зробіть прогноз за допомогою завантаженої моделі (`.predict()` або `.forecast()`).\n",
    "    6.  *Відображення результату:*\n",
    "        *   Побудуйте **інтерактивний графік** (`st.line_chart`, `st.plotly_chart` / `gr.Plot`), що показує історичні дані та прогнозні значення.\n",
    "        *   Опціонально: Додайте таблицю з прогнозними значеннями.\n",
    "    7.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/компоненти, історичні дані, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `statsmodels`, `xgboost`, опціонально `keras`/`tensorflow`/`pytorch`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `plotly` (для графіків)\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Графіки часових рядів, графіки сезонної декомпозиції, графіки ACF/PACF, **графіки прогнозу проти фактичних значень**.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, MAPE для цільового забруднювача/AQI.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Розрахунок AQI може бути складним і залежить від місцевих стандартів; прогнозування одного ключового забруднювача (наприклад, PM2.5) може бути простішим.\n",
    "*   _**Оброблення пропущених даних є критично важливою**_ у часових рядах якості повітря.\n",
    "*   **Інженерія ознак** (лаги, ковзні середні, часові ознаки) є ключовою для ML підходів.\n",
    "*   *Порівняйте продуктивність* статистичних та ML моделей; часто гібридні підходи або ML на добре створених ознаках працюють краще.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 11: Інтелектуальна інформаційна система для прогнозування медичних витрат\n",
    "\n",
    "**Набір даних:** Medical Cost Personal Datasets ([https://www.kaggle.com/datasets/mirichoi0218/insurance](https://www.kaggle.com/datasets/mirichoi0218/insurance))\n",
    "\n",
    "**Мета:** **Прогнозувати індивідуальні медичні витрати** ('charges'), виставлені медичним страхуванням, на основі персональних атрибутів (_Регресія_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідницький Аналіз Даних (EDA):**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `insurance.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Перевірте типи даних, наявність пропусків (зазвичай немає в цьому наборі), основні статистики.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('charges'):*\n",
    "        *   **Візуалізуйте розподіл 'charges'** за допомогою гістограми та KDE. Зверніть увагу на *сильну асиметрію* (правий хвіст).\n",
    "        *   Розгляньте **логарифмічне перетворення** `charges` (`np.log1p`) та візуалізуйте розподіл трансформованої змінної.\n",
    "    *   1.4. *Аналіз Взаємозв'язків:*\n",
    "        *   Дослідіть зв'язок між предикторами та 'charges' (або log-трансформованими 'charges'):\n",
    "            *   Діаграми розсіювання для числових ознак ('age', 'bmi') проти 'charges', **забарвлені за категоріальною ознакою 'smoker'**.\n",
    "            *   Коробкові діаграми ('boxplot') для категоріальних ознак ('sex', 'smoker', 'region') проти 'charges'.\n",
    "        *   Обчисліть та візуалізуйте **кореляційну матрицю** для числових ознак.\n",
    "\n",
    "2.  **Попереднє Оброблення та Побудова Базових Моделей:**\n",
    "    *   2.1. *Кодування категоріальних ознак:* Закодуйте 'sex', 'smoker', 'region' (наприклад, One-Hot Encoding).\n",
    "    *   2.2. *Масштабування ознак:* Застосуйте `StandardScaler` до числових ознак ('age', 'bmi', 'children').\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки. Використовуйте **log-трансформовані 'charges'** як цільову змінну для навчання моделей.\n",
    "    *   2.4. *Реалізація базових моделей:*\n",
    "        *   Навчіть Linear Regression.\n",
    "        *   Навчіть Polynomial Regression (використовуючи `PolynomialFeatures` з `sklearn.preprocessing` для створення поліноміальних та взаємодіючих ознак, наприклад, ступінь 2).\n",
    "        *   Розгляньте Ridge або Lasso регресію.\n",
    "    *   2.5. *Оцінка та Аналіз Залишків:* Оцініть моделі на тестовій вибірці (на log-шкалі) за допомогою MAE, MSE, RMSE, R-squared. **Проаналізуйте залишки** (графік залишків проти прогнозів), щоб перевірити припущення моделей.\n",
    "\n",
    "3.  **Розширене Моделювання та Інтерпретація:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть *просунуті регресійні моделі*: Random Forest Regressor, XGBoost Regressor.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте гіперпараметри для Random Forest та XGBoost за допомогою `GridSearchCV` або `RandomizedSearchCV`.\n",
    "    *   3.3. *Порівняння моделей:* **Порівняйте продуктивність** усіх моделей (базових та просунутих) на log-шкалі. Виберіть найкращу модель.\n",
    "    *   3.4. *Аналіз важливості ознак:*\n",
    "        *   Для найкращої моделі (ймовірно, RF або XGBoost) **проаналізуйте важливість ознак**. Визначте ключові фактори, що впливають на медичні витрати (наприклад, 'smoker', 'age', 'bmi').\n",
    "        *   Дослідіть *взаємодію ознак* (наприклад, ефект 'bmi' може бути різним для курців та некурців).\n",
    "    *   3.5. *Збереження моделі:* Збережіть найкращу модель та об'єкти попереднього оброблення (скейлер, кодувальники).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для оцінки індивідуальних медичних витрат**. Користувач повинен мати змогу ввести свої дані (вік, стать, ІМТ, кількість дітей, статус куріння, регіон) та отримати прогнозовану суму медичних витрат.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost` (якщо використовується).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, кодувальник(и).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для користувача: `st.number_input` для 'age', 'bmi', 'children'; `st.radio` або `st.selectbox` для 'sex', 'smoker', 'region'.\n",
    "    5.  *Оброблення вводу:* Зберіть дані користувача. **Застосуйте ту саму попередню обробку**: кодування категоріальних ознак, масштабування числових. Переконайтесь, що порядок ознак відповідає тому, що очікує модель.\n",
    "    6.  *Прогнозування:* Зробіть прогноз за допомогою моделі. **Застосуйте обернене логарифмічне перетворення** (`np.expm1()`) до результату, оскільки модель прогнозує log(charges).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану суму витрат користувачеві.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, кодувальники, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Гістограма 'charges' (вихідна та log-трансформована), **діаграми розсіювання (наприклад, 'age'/'bmi' проти 'charges', забарвлені за 'smoker')**, коробкові діаграми ('region'/'smoker' проти 'charges'), **діаграма важливості ознак**.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, R-squared (оцінюйте на log-шкалі, але можна звітувати і на оригінальній шкалі для кращої інтерпретації).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Логарифмічне перетворення 'charges'*** (`np.log1p`) значно покращує продуктивність лінійних моделей та моделей на основі дерев через зменшення асиметрії.\n",
    "*   Дослідіть **взаємодію ознак**, особливо між 'smoker' та іншими ознаками, як 'bmi' або 'age' (наприклад, додавши їх як `PolynomialFeatures` або аналізуючи важливість ознак).\n",
    "*   *Зосередьтеся на інтерпретації* факторів, що визначають медичні витрати, особливо важливості статусу куріння.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 12: Інтелектуальна інформаційна система для класифікування галактик за морфологією\n",
    "\n",
    "**Набір даних:** Galaxy Classification (ознаки з SDSS) ([https://www.kaggle.com/datasets/solarmainframe/galaxy-classification](https://www.kaggle.com/datasets/solarmainframe/galaxy-classification) - `Galaxy_Classification.csv`)\n",
    "\n",
    "**Мета:** **Класифікувати галактики** за типами (Еліптична - Elliptical, Спіральна - Spiral, Злиття - Merger/Irregular, зазвичай позначається як 'star' в цьому датасеті, що може бути помилкою, або потребує уточнення; приймемо три класи: Elliptical, Spiral, Irregular/Merger) на основі фотометричних та спектроскопічних ознак (_Класифікація_). *Примітка: Ознака 'class' в цьому датасеті містить GALAXY, QSO, STAR. Ймовірно, потрібно фільтрувати тільки GALAXY і використовувати інші ознаки для класифікації морфологічного типу, якщо він доступний, або переформулювати завдання на класифікацію GALAXY/QSO/STAR.* **Будемо вважати, що завдання - класифікувати об'єкти як GALAXY, QSO, STAR.**\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження Даних та Розуміння Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `Galaxy_Classification.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Перевірте типи даних, пропуски, основні статистики. Видаліть непотрібні ідентифікатори ('obj_ID', 'spec_obj_ID').\n",
    "    *   1.3. *Розуміння ознак:* Ознайомтеся з описом ознак (якщо доступний на Kaggle або в джерелі SDSS):\n",
    "        *   `ra`, `dec`: Координати (Right Ascension, Declination).\n",
    "        *   `u, g, r, i, z`: Фотометричні вимірювання у п'яти фільтрах (величини).\n",
    "        *   `run_ID, rerun_ID, cam_col, field_ID`: Ідентифікатори спостереження SDSS.\n",
    "        *   `redshift`: Червоне зміщення (вказує на відстань/швидкість).\n",
    "        *   `plate, mjd, fiber_ID`: Ідентифікатори спектроскопічних даних.\n",
    "    *   1.4. *Аналіз Цільової Змінної ('class'):*\n",
    "        *   Дослідіть розподіл класів ('GALAXY', 'QSO', 'STAR'). Перевірте наявність дисбалансу.\n",
    "    *   1.5. *EDA:*\n",
    "        *   Візуалізуйте **розподіли ключових ознак** (наприклад, величин `u, g, r, i, z`, `redshift`) для кожного класу ('GALAXY', 'QSO', 'STAR') за допомогою гістограм, KDE або коробкових діаграм. Це допоможе побачити, які ознаки найкраще розрізняють класи.\n",
    "        *   Побудуйте парні діаграми (`pairplot`) для підмножини важливих ознак (наприклад, кольорів `u-g`, `g-r` та `redshift`), забарвлені за класом.\n",
    "    *   1.6. *Масштабування ознак:* ***Застосуйте `StandardScaler`*** до всіх числових ознак.\n",
    "\n",
    "2.  **Побудова Базових Моделей Класифікації:**\n",
    "    *   2.1. *Підготовка даних:* Розділіть дані на ознаки та цільову змінну 'class'. Розділіть на тренувальну та тестову вибірки (з стратифікацією).\n",
    "    *   2.2. *Реалізація базових класифікаторів:* Навчіть прості, але різноманітні моделі:\n",
    "        *   K-Nearest Neighbors (K-NN).\n",
    "        *   Gaussian Naive Bayes (добре працює, якщо ознаки умовно незалежні).\n",
    "        *   Decision Tree.\n",
    "        *   Logistic Regression.\n",
    "    *   2.3. *Оцінка:* Оцініть моделі на тестовій вибірці. Використовуйте:\n",
    "        *   Accuracy.\n",
    "        *   Classification Report (з Precision, Recall, $F_1$-score для кожного класу).\n",
    "        *   Матрицю плутанини (Confusion Matrix) для аналізу помилок між класами.\n",
    "\n",
    "3.  **Розширене Моделювання та Зниження Розмірності:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть *просунуті моделі*:\n",
    "        *   Random Forest.\n",
    "        *   Support Vector Machine (SVM) (може бути повільним на великих даних, розгляньте `LinearSVC` або SVM з RBF ядром на підмножині).\n",
    "        *   XGBoost.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте гіперпараметри для кращих моделей (Random Forest, XGBoost).\n",
    "    *   3.3. *Зниження Розмірності для Візуалізації (Опціонально):*\n",
    "        *   Застосуйте PCA або t-SNE (на підмножині) до масштабованих ознак.\n",
    "        *   ***Візуалізуйте дані у 2D***, забарвлюючи точки за класом ('GALAXY', 'QSO', 'STAR'), щоб побачити, наскільки добре класи розділяються у зменшеному просторі.\n",
    "    *   3.4. *Порівняння Моделей:* **Порівняйте продуктивність** усіх моделей (базових та просунутих) за вибраними метриками. Визначте найкращу модель.\n",
    "    *   3.5. *Аналіз важливості ознак (для RF/XGBoost):* Визначте, які фотометричні або спектроскопічні ознаки є найважливішими для розрізнення типів астрономічних об'єктів.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який дає змогу користувачеві ввести значення ключових фотометричних/спектроскопічних ознак (наприклад, `u, g, r, i, z`, `redshift`) та **отримати прогноз класу об'єкта** ('GALAXY', 'QSO', 'STAR') за допомогою найкращої навченої моделі.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost` (якщо використовується).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input` / `gr.Number`) для основних ознак, що використовуються моделлю (наприклад, `u, g, r, i, z`, `redshift`). Можливо, знадобляться поля для інших важливих ознак.\n",
    "    5.  *Оброблення вводу:* Зберіть дані користувача. Створіть DataFrame або масив NumPy. **Застосуйте збережений скейлер `StandardScaler`**. Переконайтеся, що порядок ознак правильний.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований клас об'єкта ('GALAXY', 'QSO' або 'STAR').\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Гістограми ознак для кожного класу, парні діаграми (підмножина), **матриця плутанини**, діаграма розсіювання PCA/t-SNE, забарвлена за класом, діаграма важливості ознак.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score (macro/weighted та для кожного класу), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Масштабування ознак є важливим*** для багатьох класифікаторів.\n",
    "*   Порівняйте продуктивність лінійних та нелінійних моделей.\n",
    "*   _Візуалізація ембедингів_ (PCA/t-SNE) може дати гарне уявлення про складність задачі класифікації.\n",
    "*   Зверніть увагу на **інтерпретацію важливості ознак** в астрономічному контексті.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 13: Інтелектуальна інформаційна система для прогнозування покупок клієнтів в електронній комерції\n",
    "\n",
    "**Набір даних:** E-Commerce Behavior Data from Multi Category Store ([https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store)) - Використати один з місячних файлів (наприклад, `2019-Oct.csv`, ~5GB). ***Потребує значної вибірки*** (наприклад, 0.5%-1% даних або даних за кілька днів) через величезний розмір.\n",
    "\n",
    "**Мета:** **Прогнозувати, чи призведе сесія користувача до покупки**, на основі його поведінки під час перегляду (послідовності подій) (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Вибірка Даних, Очищення та Сесіонізація:**\n",
    "    *   1.1. *Вибірка та Завантаження:* Через великий розмір файлу, **зробіть вибірку**:\n",
    "        *   Або зчитайте файл частинами (`chunksize` в `pd.read_csv`) і обробіть/збережіть невеликий відсоток.\n",
    "        *   Або використайте інструменти командного рядка (`shuf`, `head`) для створення меншого файлу перед завантаженням в `pandas`.\n",
    "        *   *Завантажте вибрані дані*.\n",
    "    *   1.2. *Попереднє Оброблення:*\n",
    "        *   Перетворіть 'event\\_time' на datetime об'єкт.\n",
    "        *   Обробіть пропущені значення (особливо в 'category\\_code', 'brand').\n",
    "        *   Видаліть дублікати подій, якщо є.\n",
    "    *   1.3. ***Сесіонізація:*** Це **ключовий і складний крок**.\n",
    "        *   Визначте критерії для розбиття потоку подій на окремі сесії для кожного 'user\\_id'. Поширений підхід - встановлення **максимального часу неактивності** між подіями (наприклад, 30 хвилин).\n",
    "        *   Згрупуйте події за 'user\\_id' та відсортуйте за 'event\\_time'.\n",
    "        *   Призначте унікальний 'session\\_id' кожній сесії.\n",
    "    *   1.4. *Визначення Цільової Змінної:*\n",
    "        *   Для кожної сесії визначте, чи відбулася подія 'purchase'. Створіть бінарну цільову змінну `purchased_in_session` (1, якщо була покупка, 0 - якщо ні).\n",
    "    *   1.5. *Перевірка Дисбалансу:* Перевірте співвідношення сесій з покупкою та без. Ймовірно, буде **значний дисбаланс**.\n",
    "\n",
    "2.  **Інженерія Ознак та Базова Модель Прогнозування Покупок:**\n",
    "    *   2.1. *Агрегація Ознак на Рівні Сесії:* Для кожної 'user\\_session' обчисліть агреговані ознаки:\n",
    "        *   *Кількісні:* Тривалість сесії, кількість подій ('view', 'cart', 'purchase'), кількість унікальних переглянутих товарів ('product\\_id'), кількість унікальних категорій ('category\\_code'), кількість унікальних брендів ('brand').\n",
    "        *   *Послідовні (прості):* Тип першої/останньої події в сесії.\n",
    "        *   *Часові:* Година початку/кінця сесії, день тижня.\n",
    "    *   2.2. *Оброблення Категоріальних Ознак:* Обробіть агреговані категоріальні ознаки (наприклад, найчастіше відвідувана категорія/бренд), якщо створили такі.\n",
    "    *   2.3. *Підготовка даних:* Створіть фінальний DataFrame, де кожен рядок - це одна сесія з її агрегованими ознаками та цільовою змінною `purchased_in_session`. Розділіть на тренувальну/тестову вибірки (з стратифікацією). Застосуйте масштабування до числових ознак.\n",
    "    *   2.4. *Навчання базових моделей:* Навчіть Logistic Regression, Naive Bayes (GaussianNB), Decision Tree. Врахуйте дисбаланс (наприклад, `class_weight='balanced'` або техніки з `imblearn` на тренувальній вибірці).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі, зосереджуючись на **AUC-ROC** та **AUC-PR** через дисбаланс. Також дивіться на Precision, Recall, F1 для класу покупок.\n",
    "\n",
    "3.  **Розширене Моделювання та Інсайти:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM на агрегованих ознаках сесії.\n",
    "    *   3.2. *Оптимізація:* Налаштуйте гіперпараметри, оптимізуючи за AUC-PR або F1.\n",
    "    *   3.3. *Аналіз Важливості Ознак:* **Проаналізуйте, які характеристики сесії** (тривалість, кількість переглядів/додавань у кошик, відвідані категорії) є найсильнішими предикторами покупки.\n",
    "    *   3.4. _Просунуто (Опціонально):_ Замість агрегації ознак, розгляньте моделювання послідовностей подій за допомогою RNN (LSTM/GRU) або Transformer-ів. Це значно складніше.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель (ймовірно, на основі агрегованих ознак) та компоненти попереднього оброблення.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Розроблено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який демонструє прогнозування покупки. Він може приймати на вхід **симульовані характеристики сесії** (наприклад, кількість переглядів, додавань у кошик, тривалість) та видавати **ймовірність покупки** в цій сесії.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, список ознак.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення (`st.number_input`, `st.slider`) для ключових **агрегованих ознак сесії**, які використовує ваша модель (наприклад, кількість переглядів, кількість додавань у кошик, тривалість сесії в секундах, кількість унікальних товарів).\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Створіть вектор ознак у правильному порядку. **Застосуйте збережений скейлер**.\n",
    "    6.  *Прогнозування:* Отримайте ймовірність покупки (`model.predict_proba()[:, 1]`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану ймовірність покупки. Можна додати інтерпретацію (наприклад, \"Висока ймовірність покупки\", якщо > певного порогу).\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, `imblearn` (ймовірно)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіли агрегованих ознак для сесій з покупкою та без, **діаграма важливості ознак**, ROC/PR криві.\n",
    "*   **Метрики оцінки:** Accuracy (з обережністю), Precision, Recall, F1 (для класу покупок), **AUC-ROC**, ***AUC-PR (дуже важливо через дисбаланс)***.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Вибірка даних є абсолютно необхідною*** через величезний розмір оригіналу.\n",
    "*   **Визначення логіки сесії (сесіонізація) та інженерія агрегованих ознак є критично важливими** і найбільш трудомісткими етапами.\n",
    "*   Ретельно врахуйте та обробіть **дисбаланс класів**.\n",
    "*   Моделювання послідовностей є більш потужним, але значно складнішим підходом порівняно з агрегацією ознак сесії. Для курсового проєкту агрегації зазвичай достатньо.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 14: Інтелектуальна інформаційна система для прогнозування придатності води до споживання\n",
    "\n",
    "**Набір даних:** Water Quality (Potability Prediction) ([https://www.kaggle.com/datasets/adityakadiwal/water-potability](https://www.kaggle.com/datasets/adityakadiwal/water-potability))\n",
    "\n",
    "**Мета:** **Прогнозувати придатність води до споживання** ('Potability', 0 - непридатна, 1 - придатна) на основі показників якості (pH, твердість, вміст твердих речовин тощо) (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та Дослідницький Аналіз:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `water_potability.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз пропущених значень:*\n",
    "        *   Перевірте наявність пропущених значень (`df.isnull().sum()`). У цьому наборі даних часто є **значна кількість пропусків** в 'ph', 'Sulfate', 'Trihalomethanes'.\n",
    "        *   **Обробіть пропущені значення:**\n",
    "            *   Стратегія 1: Імпутація середнім або медіаною (`df.fillna(df.mean())` / `df.fillna(df.median())`).\n",
    "            *   Стратегія 2: Більш просунута імпутація, наприклад, за допомогою `KNNImputer` (`sklearn.impute.KNNImputer`), який враховує значення інших ознак.\n",
    "            *   *Порівняння стратегій імпутації* може бути частиною проєкту.\n",
    "    *   1.3. *EDA:*\n",
    "        *   Дослідіть розподіли кожної ознаки якості води (гістограми, KDE).\n",
    "        *   **Візуалізуйте відмінності** в розподілах ознак між придатними (Potability=1) та непридатними (Potability=0) зразками води (наприклад, `boxplot` або `violinplot` для кожної ознаки, згруповані за 'Potability').\n",
    "        *   Проаналізуйте кореляції між ознаками за допомогою теплової карти.\n",
    "    *   1.4. *Аналіз Балансу Класів:* Перевірте співвідношення класів у змінній 'Potability'.\n",
    "\n",
    "2.  **Масштабування Ознак та Побудова Базових Моделей:**\n",
    "    *   2.1. *Масштабування ознак:* Застосуйте **`StandardScaler`** до всіх числових ознак якості води.\n",
    "    *   2.2. *Підготовка даних:* Розділіть дані (після імпутації та масштабування) на тренувальну та тестову вибірки. Використовуйте **стратифікацію** за 'Potability'.\n",
    "    *   2.3. *Навчання базових класифікаторів:* Навчіть Logistic Regression, K-Nearest Neighbors (K-NN), Decision Tree.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на тестовій вибірці. Використовуйте accuracy, precision, recall, $F_1$-score (особливо для класу 1 - придатна вода), AUC-ROC. Проаналізуйте матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Порівняння:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть *просунуті моделі*: Random Forest, Support Vector Machine (SVM), XGBoost.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте гіперпараметри для кращих моделей за допомогою `GridSearchCV` або `RandomizedSearchCV`, використовуючи відповідну метрику (наприклад, F1 для класу 1 або AUC-ROC).\n",
    "    *   3.3. ***Порівняння Продуктивності:***\n",
    "        *   Порівняйте результати всіх моделей (базових та просунутих).\n",
    "        *   Якщо ви пробували різні стратегії імпутації на кроці 1.2, порівняйте, як вони вплинули на продуктивність найкращої моделі.\n",
    "    *   3.4. *Аналіз важливості ознак (для RF/XGBoost):* Визначте, які показники якості води є найбільш важливими для прогнозування її придатності.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та об'єкти попереднього оброблення (імпутер, скейлер).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який дає змогу користувачеві ввести значення показників якості води (pH, Hardness, Solids тощо) та **отримати прогноз щодо її придатності до споживання**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost` (якщо використовується).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, імпутер (якщо використовувався складний, як KNNImputer) та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input` / `gr.Number`) для всіх ознак, що використовувалися моделлю (ph, Hardness, Solids, Chloramines і т.д.).\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Зберіть дані користувача.\n",
    "        *   **Важливо:** Якщо користувач може залишити поле порожнім, застосуйте ту саму **стратегію імпутації**, що й під час тренування, до даних користувача.\n",
    "        *   Сформуйте DataFrame або масив NumPy.\n",
    "        *   **Застосуйте збережений скейлер `StandardScaler`**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`) та, можливо, ймовірності (`model.predict_proba()`).\n",
    "    7.  *Відображення результату:* Покажіть результат: \"Вода придатна до споживання\" або \"Вода непридатна до споживання\". Можна додати ймовірність.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, імпутер (якщо потрібно), скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Гістограми/KDE ознак, **коробкові діаграми (ознаки проти Potability)**, кореляційна теплова карта, матриця плутанини, ROC-крива, діаграма важливості ознак.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score (**особливо для класу 1** - придатна вода), AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Стратегія оброблення пропущених значень*** (особливо для 'ph', 'Sulfate', 'Trihalomethanes') є **дуже важливою** і може суттєво вплинути на результат. Порівняння кількох стратегій є цінним доповненням.\n",
    "*   **Масштабування ознак** (`StandardScaler`) важливе для багатьох алгоритмів.\n",
    "*   Використовуйте **стратифіковану крос-валідацію** через потенційний дисбаланс класів.\n",
    "*   Аналіз важливості ознак допоможе зрозуміти, які параметри води є критичними.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 15: Інтелектуальна інформаційна система для прогнозування ціни криптовалюти (Bitcoin)\n",
    "\n",
    "**Набір даних:** Історичні ціни Bitcoin (наприклад, 2011-2023) ([https://www.kaggle.com/datasets/rishidamarla/bitcoin-prices-2011-2023](https://www.kaggle.com/datasets/rishidamarla/bitcoin-prices-2011-2023) - `BTC-USD.csv`) або завантажити свіжіші дані за допомогою бібліотеки `yfinance`.\n",
    "\n",
    "**Мета:** **Прогнозувати ціну закриття Bitcoin ('Close')** на найближче майбутнє (наприклад, на наступний день або кілька днів), використовуючи історичні дані (_Прогнозування часових рядів_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних та Дослідження Часових Рядів:**\n",
    "    *   1.1. *Завантаження даних:*\n",
    "        *   Або завантажте CSV файл з Kaggle.\n",
    "        *   Або використайте `yfinance`: `import yfinance as yf; btc_data = yf.download('BTC-USD', start='YYYY-MM-DD', end='YYYY-MM-DD')`.\n",
    "    *   1.2. *Підготовка даних:*\n",
    "        *   Перетворіть індекс на datetime об'єкт (якщо потрібно).\n",
    "        *   Перевірте наявність пропусків (зазвичай у фінансових даних OHLCV їх немає, але перевірте).\n",
    "        *   Зосередьтеся на ціні закриття ('Close') як на цільовій змінній.\n",
    "    *   1.3. *EDA Часових Рядів:*\n",
    "        *   **Візуалізуйте часовий ряд ціни 'Close'**.\n",
    "        *   Візуалізуйте обсяг торгів ('Volume').\n",
    "        *   Розгляньте візуалізацію **свічкових графіків (candlestick charts)** за допомогою `mplfinance` або `plotly` для відображення OHLC даних.\n",
    "        *   Дослідіть тренди та можливу сезонність (хоча у фінансових рядах вона часто слабка або відсутня).\n",
    "        *   Перевірте **стаціонарність** ряду ціни 'Close' та ряду її різниць (returns) за допомогою тесту ADF. Фінансові цінові ряди зазвичай нестаціонарні.\n",
    "        *   Проаналізуйте ACF/PACF для ряду цін та ряду різниць.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Прогнозування:**\n",
    "    *   2.1. *Інженерія Часових Ознак (на основі дати):*\n",
    "        *   Створіть ознаки, що відображають календарні ефекти: **день тижня, день місяця, тиждень року, місяць, рік**.\n",
    "        *   Розгляньте додавання ознак для святкових днів або важливих подій (якщо є зовнішні дані).\n",
    "    *   2.2. *Інженерія Лагових та Ковзних Ознак:*\n",
    "        *   Створіть **лагові ознаки** для ціни 'Close' та 'Volume' (значення за попередні N днів).\n",
    "        *   Створіть **ознаки ковзного вікна**: ковзні середні (`rolling().mean()`), стандартні відхилення (`rolling().std()`) для 'Close' та 'Volume' за різні періоди (наприклад, 7, 14, 30 днів).\n",
    "        *   Обчисліть технічні індикатори (опціонально, просунуто): RSI (Relative Strength Index), MACD (Moving Average Convergence Divergence), Bollinger Bands.\n",
    "    *   2.3. *Побудова Базових Моделей:*\n",
    "        *   **Статистичні моделі:** Застосуйте ARIMA до ряду різниць ціни (якщо ціна нестаціонарна).\n",
    "        *   **Прості моделі:** Naive forecast (прогноз = ціна попереднього дня), Moving Average forecast.\n",
    "    *   2.4. *Оцінка:* Розділіть дані на тренувальну та валідаційну вибірки (останній період часу для валідації). Оцініть базові моделі за допомогою RMSE, MAE, MAPE.\n",
    "\n",
    "3.  **Прогнозування за допомогою Машинного Навчання/Глибокого Навчання:**\n",
    "    *   3.1. *Підготовка Даних для ML/DL:*\n",
    "        *   Створіть табличний набір даних з інженерними ознаками (часові, лаги, ковзні середні, індикатори) та цільовою змінною ('Close' наступного дня).\n",
    "        *   **Масштабуйте ознаки** за допомогою `MinMaxScaler` (часто рекомендується для нейронних мереж) або `StandardScaler`.\n",
    "        *   Для LSTM/GRU підготуйте дані у вигляді послідовностей (наприклад, використовувати останні 60 днів для прогнозування наступного дня).\n",
    "    *   3.2. *Застосування ML Моделей:* Навчіть моделі, такі як Support Vector Regressor (SVR), Random Forest Regressor, XGBoost Regressor на табличних даних.\n",
    "    *   3.3. *Застосування DL Моделей (LSTM/GRU):*\n",
    "        *   Побудуйте та навчіть модель LSTM або GRU за допомогою Keras/TensorFlow/PyTorch на підготовлених послідовностях.\n",
    "    *   3.4. *Порівняння Продуктивності:* ***Порівняйте*** продуктивність статистичних, ML та DL моделей на валідаційній вибірці.\n",
    "    *   3.5. *Аналіз Важливості Ознак (для ML):* Якщо використовуються моделі на основі дерев, проаналізуйте, які ознаки (лаги, ковзні середні, обсяг, часові) найбільше впливають на прогноз.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та об'єкти попереднього оброблення (скейлер).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який:\n",
    "*   Відображає **історичний графік ціни Bitcoin** (можливо, зі свічками та обсягом).\n",
    "*   Показує **прогноз ціни 'Close'** на найближчий період (наприклад, наступний день), згенерований найкращою моделлю.\n",
    "*   (Опціонально) Відображає ключові інженерні ознаки, що використовуються моделлю.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `yfinance` (для отримання останніх даних), `matplotlib`/`plotly`/`mplfinance`, `statsmodels`/`xgboost`/`keras` (залежно від моделі).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер.\n",
    "    4.  *Отримання останніх даних:* Використовуйте `yfinance` для завантаження останніх історичних даних Bitcoin, необхідних для генерації ознак для прогнозу.\n",
    "    5.  *Створення інтерфейсу:*\n",
    "        *   Відобразіть заголовок.\n",
    "        *   (Опціонально) Дозвольте користувачеві вибрати період для відображення історичних даних або горизонт прогнозування.\n",
    "    6.  *Генерація ознак та прогнозування:*\n",
    "        *   На основі останніх завантажених даних згенеруйте необхідні часові, лагові та ковзні ознаки для наступного(их) дня(ів).\n",
    "        *   **Застосуйте збережений скейлер** до ознак.\n",
    "        *   Зробіть прогноз за допомогою завантаженої моделі.\n",
    "        *   **Застосуйте обернене масштабування**, якщо ознаки масштабувалися, щоб отримати прогноз у реальних цінових одиницях.\n",
    "    7.  *Відображення результату:*\n",
    "        *   Побудуйте **графік історичних цін** (лінійний або свічковий) за допомогою `plotly` або `mplfinance` (`st.plotly_chart` / `gr.Plot`).\n",
    "        *   Додайте на графік **прогнозоване значення(я)**.\n",
    "        *   Виведіть текстовий прогноз ціни.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt` (включно з `yfinance`, `mplfinance`/`plotly`, бібліотеками моделі).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `statsmodels`, `xgboost`, опціонально `keras`/`tensorflow`/`pytorch`, `mplfinance` (для свічкових графіків), `yfinance` (для даних)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `plotly` або `mplfinance`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Графіки ціни/обсягу, **Свічковий графік (Candlestick chart)**, ACF/PACF plots, **графік прогнозу проти фактичних значень**.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, MAPE.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Фінансові ринки ***дуже важко прогнозувати***; зосередьтеся на методології, інженерії ознак та порівнянні моделей, а не на досягненні надвисокої точності.\n",
    "*   _Масштабуйте ознаки_ (`MinMaxScaler` часто використовується для цін та для нейронних мереж).\n",
    "*   Обговоріть **обмеження моделі** та непередбачуваність ринків. Прогнозування фінансових рядів часто є складним завданням.\n",
    "*   Використання `yfinance` дає змогу створити застосунок, що показує актуальні прогнози.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 16: Інтелектуальна інформаційна система для виявлення мережевих вторгнень\n",
    "\n",
    "**Набір даних:** NSL-KDD Dataset (Версія Kaggle: [https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection](https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection)) - Використати `KDDTrain+.txt`, `KDDTest+.txt`. Зверніть увагу, що цей набір даних є покращеною, але все ще застарілою версією KDD Cup 99.\n",
    "\n",
    "**Мета:** **Виявляти мережеві вторгнення**, класифікуючи записи з'єднань як 'normal' або 'anomaly' (або за конкретним типом атаки) (_Класифікація / Виявлення аномалій_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних та Попереднє Оброблення:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `KDDTrain+.txt` та `KDDTest+.txt`. Оскільки файли не мають заголовків, вам потрібно буде **призначити назви стовпців** відповідно до документації NSL-KDD (41 ознака + мітка класу + складність).\n",
    "    *   1.2. *Аналіз даних:* Дослідіть типи даних. Визначте **символьні (категоріальні) ознаки**: 'protocol\\_type', 'service', 'flag'.\n",
    "    *   1.3. *Аналіз класів:*\n",
    "        *   Дослідіть розподіл основної мітки класу (останній стовпець у `KDDTrain+`, зазвичай 'normal' або тип атаки).\n",
    "        *   Проаналізуйте **дисбаланс класів**, особливо між 'normal' та різними типами атак.\n",
    "    *   1.4. *Оброблення категоріальних ознак:*\n",
    "        *   Застосуйте **One-Hot Encoding** (`pd.get_dummies`) до символьних ознак ('protocol\\_type', 'service', 'flag'). _Увага:_ ознака 'service' має **високу кардинальність**, що призведе до великої кількості нових ознак. Розгляньте можливість об'єднання рідкісних категорій або використання інших технік кодування (наприклад, Target Encoding, з обережністю щодо витоку даних).\n",
    "        *   Переконайтеся, що кодування застосовується **однаково** до тренувального та тестового наборів (збережіть список стовпців після кодування тренувального набору).\n",
    "    *   1.5. *Нормалізація числових ознак:* Застосуйте `StandardScaler` або `MinMaxScaler` до всіх числових ознак.\n",
    "    *   1.6. *Підготовка даних:* Розділіть тренувальний набір на власні тренувальну та валідаційну вибірки для налаштування гіперпараметрів. Використовуйте наданий `KDDTest+.txt` як **фінальний тестовий набір**.\n",
    "\n",
    "2.  **Базове Виявлення Аномалій/Класифікація:**\n",
    "    *   2.1. *Визначення Завдання:* Вирішіть, чи будете ви виконувати:\n",
    "        *   **Бінарну класифікацію:** 'normal' проти 'anomaly' (всі типи атак об'єднані).\n",
    "        *   **Багатокласову класифікацію:** 'normal' та різні основні категорії атак (DoS, Probe, R2L, U2R).\n",
    "        *   **Виявлення аномалій (некероване):** Використовуючи лише 'normal' дані для навчання моделі виявлення аномалій (наприклад, One-Class SVM, Isolation Forest) і тестуючи її здатність відрізняти 'normal' від атак у тестовому наборі.\n",
    "    *   2.2. *Реалізація базових моделей (для класифікації):* Навчіть прості моделі, такі як Logistic Regression, Decision Tree, Naive Bayes (GaussianNB або MultinomialNB, залежно від даних). Враховуйте дисбаланс класів (наприклад, `class_weight`).\n",
    "    *   2.3. *Реалізація моделей виявлення аномалій (якщо обрано):* Навчіть Isolation Forest або One-Class SVM на нормальних даних з тренувального набору.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на валідаційній вибірці (або тестовій, якщо не було валідаційної). Використовуйте:\n",
    "        *   Accuracy (з обережністю через дисбаланс).\n",
    "        *   Precision, Recall, $F_1$-score (**особливо для класів атак**).\n",
    "        *   Confusion Matrix.\n",
    "        *   Для багатокласової класифікації - **Classification Report** з метриками для кожного класу та усередненими (macro/weighted).\n",
    "        *   Detection Rate (Recall для атак), False Positive Rate.\n",
    "\n",
    "3.  **Розширене Моделювання та Оцінка на Тестовому Наборі:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, XGBoost, LightGBM. Ці моделі часто показують хороші результати на подібних завданнях.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей за допомогою крос-валідації на тренувальних/валідаційних даних.\n",
    "    *   3.3. ***Фінальна Оцінка на `KDDTest+`:***\n",
    "        *   Застосуйте **ту саму послідовність попереднього оброблення** (кодування, масштабування), що й до тренувального набору, до `KDDTest+.txt`. Будьте готові обробити категорії в `KDDTest+`, яких не було в `KDDTrain+`.\n",
    "        *   Зробіть прогнози за допомогою вашої найкращої моделі на обробленому `KDDTest+`.\n",
    "        *   **Ретельно оцініть продуктивність** на цьому незалежному наборі, використовуючи ті ж метрики, що й раніше. Зверніть увагу на можливі відмінності у продуктивності порівняно з валідаційною вибіркою (це може вказувати на відмінності у розподілах даних).\n",
    "    *   3.4. *Аналіз важливості ознак:* Проаналізуйте, які ознаки з'єднання є найважливішими для виявлення вторгнень.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та об'єкти попереднього оброблення (кодувальники, скейлер).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який дає змогу класифікувати **приклад запису мережевого з'єднання**. Користувач може ввести (або вибрати з прикладів) значення ключових ознак, і застосунок видасть прогноз ('Normal' або тип атаки).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, кодувальник(и) (або список стовпців після One-Hot Encoding).\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення для ключових ознак: випадаючі списки для 'protocol\\_type', 'service', 'flag'; числові поля для тривалості, байтів тощо. *Примітка:* Через велику кількість ознак (особливо після OHE), можливо, доведеться вибрати підмножину найважливіших ознак для введення користувачем, а інші встановити за замовчуванням.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте ту саму попередню обробку**: One-Hot Encoding категоріальних (переконайтесь, що всі стовпці, очікувані моделлю, присутні), масштабування числових.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований клас ('Normal' або тип атаки).\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, кодувальники/список стовпців, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (рекомендовано)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл типів атак, розподіли ознак (normal проти атак), **матриця плутанини** (для багатокласового випадку), діаграма важливості ознак.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score (**для кожного класу та macro/weighted**), Detection Rate, False Positive Rate. Використовуйте `classification_report`.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Обережно обробляйте категоріальні ознаки*** з високою кардинальністю ('service') під час кодування. Розгляньте стратегії зменшення розмірності після OHE або альтернативні методи кодування.\n",
    "*   _Масштабування ознак_ важливе.\n",
    "*   Зверніть увагу на **потенційні відмінності між розподілами даних** у тренувальному (`KDDTrain+`) та тестовому (`KDDTest+`) наборах даних NSL-KDD, що може вплинути на результати фінальної оцінки.\n",
    "*   У реальному світі використовуються більш сучасні набори даних (наприклад, CICIDS2017), але NSL-KDD є класичним для навчальних цілей.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 17: Інтелектуальна інформаційна система для розпізнавання вимовлених цифр\n",
    "\n",
    "**Набір даних:** Free Spoken Digit Dataset (FSDD) ([https://github.com/Jakobovski/free-spoken-digit-dataset](https://github.com/Jakobovski/free-spoken-digit-dataset)). Потребує завантаження архіву з `.wav` файлами та їх організації.\n",
    "\n",
    "**Мета:** **Класифікувати вимовлені цифри** (0-9) з аудіозаписів, використовуючи витягнуті аудіо ознаки (_Класифікація аудіо_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Аудіоданих та Вилучення Ознак:**\n",
    "    *   1.1. *Завантаження та Організація Даних:* Завантажте набір даних з GitHub. Структура зазвичай: папки для кожного диктора, що містять файли типу `{digit}_{speaker}_{index}.wav` (наприклад, `0_jackson_0.wav`).\n",
    "    *   1.2. *Завантаження .wav файлів:* Використовуйте бібліотеку `librosa` або `scipy.io.wavfile` для завантаження аудіофайлів. `librosa.load()` зручна, оскільки вона автоматично перетворює аудіо в моно та нормалізує до плаваючої точки.\n",
    "    *   1.3. *Візуалізація Аудіо:*\n",
    "        *   Для кількох прикладів візуалізуйте **форму хвилі** (waveform) за допомогою `librosa.display.waveshow` або `matplotlib.pyplot.plot`.\n",
    "        *   Візуалізуйте **спектрограму** (залежність частоти від часу) за допомогою `librosa.display.specshow`.\n",
    "    *   1.4. ***Вилучення Аудіо Ознак:*** Це **ключовий крок**. Використовуйте `librosa.feature` для вилучення різноманітних ознак з кожного аудіофайлу. Популярні ознаки включають:\n",
    "        *   **MFCCs (Mel-frequency cepstral coefficients):** Найбільш поширені ознаки для розпізнавання мови/аудіо. Витягніть, наприклад, 13-40 коефіцієнтів (`librosa.feature.mfcc`). Результатом буде матриця (кількість коефіцієнтів x кількість часових фреймів).\n",
    "        *   **Chroma Features:** Відображають енергію в 12 класах висоти тону (`librosa.feature.chroma_stft`).\n",
    "        *   **Spectral Contrast:** Вимірює різницю між піками та долинами в спектрі (`librosa.feature.spectral_contrast`).\n",
    "        *   **Zero Crossing Rate:** Частота зміни знаку сигналу (`librosa.feature.zero_crossing_rate`).\n",
    "        *   **Root Mean Square (RMS) Energy:** Енергія сигналу (`librosa.feature.rms`).\n",
    "    *   1.5. *Збір даних:* Створіть структуру даних (наприклад, список словників або DataFrame `pandas`), де кожен запис містить витягнуті ознаки (або їх агрегації) та відповідну мітку цифри (яку можна отримати з назви файлу).\n",
    "\n",
    "2.  **Агрегація Ознак та Базова Класифікація:**\n",
    "    *   2.1. *Оброблення Ознак Змінної Довжини:* Оскільки аудіофайли мають різну тривалість, витягнуті матриці ознак (як MFCCs) матимуть різну кількість часових фреймів. Існує кілька підходів:\n",
    "        *   **Агрегація:** Обчисліть **статистики по часових фреймах** для кожної ознаки (наприклад, середнє, стандартне відхилення, медіана, min, max). Це перетворить матрицю ознак для кожного файлу на фіксований вектор ознак. _Це найпростіший підхід для початку._\n",
    "        *   *Паддінг/Обрізка:* Доповніть короткі послідовності нулями або обріжте довгі до фіксованої довжини (використовується для CNN/LSTM).\n",
    "    *   2.2. *Підготовка даних (для агрегованих ознак):* Створіть DataFrame з агрегованими ознаками та мітками. Розділіть на тренувальну/тестову вибірки. Застосуйте `StandardScaler`.\n",
    "    *   2.3. *Навчання базових класифікаторів:* Навчіть K-NN, SVM (з різними ядрами), Logistic Regression на **агрегованих ознаках**.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі за допомогою Accuracy, Classification Report (Precision, Recall, F1 для кожної цифри) та матриці плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання (CNN/LSTM) та Розгортання:**\n",
    "    *   3.1. *Моделі на Агрегованих Ознаках:* Навчіть Random Forest/XGBoost на агрегованих ознаках. Порівняйте з базовими моделями.\n",
    "    *   3.2. ***Моделі на Послідовностях Ознак (Просунуто):***\n",
    "        *   Підготуйте дані: Використовуйте необроблені послідовності ознак (наприклад, MFCCs), застосовуючи паддінг/обрізку до фіксованої довжини.\n",
    "        *   Побудуйте та навчіть **1D Convolutional Neural Networks (CNN)** або **Recurrent Neural Networks (LSTM/GRU)** за допомогою Keras/TensorFlow/PyTorch. Ці моделі можуть краще вловлювати часові залежності в аудіо.\n",
    "    *   3.3. *Порівняння Підходів:* Порівняйте продуктивність моделей на агрегованих ознаках з моделями на послідовностях (якщо реалізовано).\n",
    "    *   3.4. *Аналіз Помилок:* Проаналізуйте матрицю плутанини найкращої моделі, щоб побачити, які цифри найчастіше плутаються.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель (на агрегованих ознаках або послідовностях) та об'єкти попереднього оброблення (скейлер, можливо, параметри паддінга).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який дає змогу користувачеві **завантажити `.wav` файл** з вимовленою цифрою та **отримати прогноз** цієї цифри від найкращої навченої моделі.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `numpy`, `joblib`/`pickle`, `librosa`, `sklearn`, можливо `tensorflow`/`pytorch`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер (якщо модель на агрегованих ознаках), параметри обробки (довжина для паддінга, якщо модель на послідовностях).\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   (`streamlit`) Використовуйте `st.file_uploader(type=['wav'])` для завантаження аудіофайлу.\n",
    "        *   (`gradio`) Використовуйте `gr.Audio(source=\"upload\", type=\"filepath\")` для завантаження аудіо.\n",
    "    5.  ***Оброблення Аудіовводу:*** Це **найскладніша частина** розгортання аудіомоделі.\n",
    "        *   Коли файл завантажено, використайте `librosa.load()` для його зчитування.\n",
    "        *   **Вилучіть ті самі ознаки**, що й під час тренування (наприклад, MFCCs).\n",
    "        *   **Застосуйте ту саму обробку**:\n",
    "            *   (Агреговані ознаки) Обчисліть ті самі статистики (середнє, std тощо). Застосуйте збережений скейлер.\n",
    "            *   (Послідовні ознаки) Застосуйте паддінг/обрізку до тієї ж довжини, що й під час тренування. Масштабуйте, якщо потрібно.\n",
    "    6.  *Прогнозування:* Зробіть прогноз за допомогою завантаженої моделі (`model.predict()`). Для класифікації результат буде масивом з одним елементом (прогнозована цифра).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану цифру користувачеві. Можна також відтворити завантажене аудіо для перевірки.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки. Можуть знадобитися додаткові системні бібліотеки для обробки аудіо (`ffmpeg`) на платформах розгортання.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер/параметри, `requirements.txt` (включно з `librosa`). Може знадобитися налаштування системних залежностей у `replit.nix`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `librosa` (обов'язково), `scipy` (для аудіо I/O), опціонально `keras`/`tensorflow`/`pytorch` (для CNN/LSTM)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `librosa`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Форми хвиль, спектрограми, **матриця плутанини**, t-SNE візуалізація агрегованих ознак (забарвлених за цифрою).\n",
    "*   **Метрики оцінки:** Accuracy, **Classification Report** (Precision, Recall, F1 для кожної цифри).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   **Вилучення ознак (особливо MFCCs) є ключовим**. Експериментуйте з кількістю коефіцієнтів.\n",
    "*   Порівняйте підхід з **агрегацією статичних ознак** (простіший) та **моделювання послідовностей** (складніший, але потенційно точніший).\n",
    "*   Обробляйте різну тривалість аудіофайлів (агрегація або паддінг/обрізка).\n",
    "*   ***Оброблення аудіофайлу, завантаженого користувачем у вебзастосунку, потребує ретельної імплементації***, щоб відповідати формату даних, на яких навчалася модель.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 18: Інтелектуальна інформаційна система для визначення відтоку клієнтів у телеком-компанії\n",
    "\n",
    "**Набір даних:** Telco Customer Churn ([https://www.kaggle.com/datasets/blastchar/telco-customer-churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn))\n",
    "\n",
    "**Мета:** **Прогнозувати відтік клієнтів** ('Churn') телеком-компанії на основі даних акаунту, послуг та демографічних даних (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Попереднє Оброблення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `WA_Fn-UseC_-Telco-Customer-Churn.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз та Очищення:*\n",
    "        *   Перевірте типи даних (`df.info()`). Зверніть увагу, що 'TotalCharges' може бути типу 'object'.\n",
    "        *   **Обробіть 'TotalCharges':** Перетворіть на числовий тип (`pd.to_numeric`), обробляючи помилки (значення типу ' ' можуть стати NaN). Імпутуйте отримані NaN (наприклад, медіаною або значенням, що базується на 'tenure' та 'MonthlyCharges').\n",
    "        *   Видаліть стовпець 'customerID', оскільки він не є корисною ознакою для моделювання.\n",
    "        *   Перетворіть цільову змінну 'Churn' на бінарний формат (0/1).\n",
    "    *   1.3. *Аналіз Цільової Змінної:* Дослідіть розподіл 'Churn'. Перевірте на **дисбаланс класів**.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Візуалізуйте взаємозв'язки між ознаками та відтоком ('Churn'):\n",
    "            *   Груповані стовпчасті діаграми для категоріальних ознак ('gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod').\n",
    "            *   Коробкові/скрипкові діаграми для числових ознак ('tenure', 'MonthlyCharges', 'TotalCharges') проти 'Churn'.\n",
    "        *   Проаналізуйте кореляції між числовими ознаками.\n",
    "\n",
    "2.  **Побудова Базових Моделей Прогнозування Відтоку:**\n",
    "    *   2.1. *Кодування та Масштабування:*\n",
    "        *   Закодуйте всі категоріальні ознаки (включаючи бінарні типу 'Yes'/'No') за допомогою One-Hot Encoding.\n",
    "        *   Застосуйте `StandardScaler` до числових ознак ('tenure', 'MonthlyCharges', 'TotalCharges').\n",
    "    *   2.2. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (з стратифікацією за 'Churn').\n",
    "    *   2.3. *Навчання базових класифікаторів:* Навчіть Logistic Regression та Decision Tree. **Врахуйте дисбаланс класів** (параметр `class_weight='balanced'`).\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на тестовій вибірці, **зосереджуючись на метриках для класу 'Yes' (відтік)**: Precision, Recall, $F_1$-score. Також обчисліть Accuracy, AUC-ROC, AUC-PR. Проаналізуйте матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Важливість Ознак:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть *просунуті класифікатори*: Random Forest, XGBoost, LightGBM. Використовуйте їхні можливості для роботи з дисбалансом, якщо потрібно (`scale_pos_weight`).\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте гіперпараметри для кращих моделей (RF, XGBoost, LightGBM), максимізуючи $F_1$-score для класу 'Yes' або AUC-PR.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх моделей за ключовими метриками.\n",
    "    *   3.4. ***Аналіз Важливості Ознак:***\n",
    "        *   Визначте **ключові фактори (рушійні сили) відтоку**, аналізуючи важливість ознак для моделей Random Forest/XGBoost/LightGBM. Які послуги, умови контракту або характеристики клієнтів найбільше пов'язані зі звільненням?\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та об'єкти попереднього оброблення (скейлер, кодувальники/список стовпців).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який:\n",
    "*   Дає змогу ввести характеристики клієнта телеком-компанії.\n",
    "*   **Прогнозує ймовірність відтоку** цього клієнта.\n",
    "*   (Опціонально) Виділяє **основні фактори**, що сприяють або запобігають відтоку для цього конкретного клієнта (на основі важливості ознак моделі).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, список стовпців/категорій (для кодування).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для ключових ознак: `st.number_input` для 'tenure', 'MonthlyCharges', 'TotalCharges'; `st.selectbox` або `st.radio` для 'gender', 'Partner', 'Contract', 'PaymentMethod' та інших послуг ('PhoneService', 'InternetService' тощо).\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте ту саму попередню обробку**: кодування категоріальних (One-Hot Encoding), масштабування числових. Сформуйте вектор ознак у правильному порядку.\n",
    "    6.  *Прогнозування:* Отримайте ймовірність відтоку (клас 'Yes') за допомогою `model.predict_proba()[:, 1]`.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану ймовірність відтоку. Можна додати інтерпретацію (\"Високий/Середній/Низький ризик відтоку\").\n",
    "    8.  *Відображення факторів (опціонально):* Виведіть список найважливіших ознак моделі або використайте SHAP (якщо реалізовано) для показу факторів для конкретного вводу.\n",
    "    9.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, список стовпців, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально), `imblearn` (опціонально)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Рівень відтоку, **розподіли ознак проти 'Churn'**, **діаграма важливості ознак**, матриця плутанини, ROC-крива, **PR-крива**.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall (**для 'Yes'**), $F_1$-score (**для 'Yes'**), AUC-ROC. Розгляньте **AUC-PR** через дисбаланс.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Потрібна **ретельна Попереднє Оброблення** для 'TotalCharges'.\n",
    "*   **Адресуйте дисбаланс класів**, якщо необхідно (використовуючи `class_weight`, `scale_pos_weight` або `imblearn`).\n",
    "*   Аналіз **важливості ознак надає значну бізнес-цінність**, допомагаючи зрозуміти причини відтоку клієнтів.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 19: Інтелектуальна інформаційна система для прогнозування якості вина\n",
    "\n",
    "**Набір даних:** Wine Quality Dataset (Червоне вино: [https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009)). Є також набір для білого вина. Рекомендується сфокусуватися на одному типі, наприклад, червоному.\n",
    "\n",
    "**Мета:** **Прогнозувати оцінку якості вина** ('quality', шкала від 0 до 10, але в даних зазвичай від 3 до 8) на основі фізико-хімічних тестів (_Регресія або Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Попереднє Оброблення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `winequality-red.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Перевірте типи даних, пропуски (зазвичай немає), основні статистики.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('quality'):*\n",
    "        *   **Візуалізуйте розподіл оцінок 'quality'** (гістограма, `countplot`). Зверніть увагу, що це *порядкова змінна*, і розподіл може бути незбалансованим, з малою кількістю зразків для крайніх оцінок (3, 8).\n",
    "    *   1.4. *EDA:*\n",
    "        *   Дослідіть взаємозв'язок між фізико-хімічними ознаками ('fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol') та 'quality'. Використовуйте:\n",
    "            *   Кореляційну теплову карту для всіх ознак.\n",
    "            *   Коробкові діаграми (boxplot) для кожної ознаки проти 'quality'.\n",
    "    *   1.5. *Масштабування ознак:* ***Застосуйте `StandardScaler`*** до всіх вхідних фізико-хімічних ознак.\n",
    "\n",
    "2.  **Регресійне Моделювання:**\n",
    "    *   2.1. *Підхід:* Розглядайте 'quality' як неперервну змінну.\n",
    "    *   2.2. *Підготовка даних:* Розділіть масштабовані дані на тренувальну та тестову вибірки.\n",
    "    *   2.3. *Навчання регресійних моделей:* Навчіть Linear Regression, Ridge, Lasso, Support Vector Regressor (SVR), Random Forest Regressor, XGBoost Regressor.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі за допомогою метрик MAE, MSE, RMSE, R-squared. Проаналізуйте, наскільки добре моделі можуть передбачити точну оцінку якості.\n",
    "\n",
    "3.  **Класифікаційний Підхід та Порівняння:**\n",
    "    *   3.1. *Перетворення Цільової Змінної:* **Створіть категорії якості**, оскільки точне прогнозування оцінки може бути складним, а розрізнення \"поганого\", \"середнього\" та \"доброго\" вина може бути більш практичним. Наприклад:\n",
    "        *   _Бінарна класифікація:_ 'Погане' (3-5) vs 'Добре' (6-8).\n",
    "        *   _Багатокласова класифікація:_ 'Погане' (3-4), 'Середнє' (5-6), 'Добре' (7-8).\n",
    "        *   *Обґрунтуйте вибір категорій.* Перевірте баланс нових класів.\n",
    "    *   3.2. *Підготовка даних для класифікації:* Використовуйте ті ж масштабовані ознаки, але нову категоріальну цільову змінну. Розділіть на тренувальну/тестову вибірки (з стратифікацією).\n",
    "    *   3.3. *Навчання класифікаторів:* Навчіть Logistic Regression, Random Forest Classifier, XGBoost Classifier. Врахуйте дисбаланс, якщо він суттєвий (`class_weight`).\n",
    "    *   3.4. *Оцінка класифікації:* Оцініть моделі за допомогою Accuracy, Precision, Recall, F1 (macro/weighted та для кожного класу), матриці плутанини.\n",
    "    *   3.5. ***Порівняння Підходів:***\n",
    "        *   Порівняйте результати найкращої регресійної моделі та найкращої класифікаційної моделі.\n",
    "        *   **Обговоріть переваги та недоліки** кожного підходу (регресія vs класифікація) для цього конкретного завдання з порядковою цільовою змінною. Який підхід дає більш корисні результати?\n",
    "    *   3.6. *Аналіз важливості ознак:* Для найкращої моделі (регресійної або класифікаційної) визначте, які хімічні властивості найбільше впливають на якість вина.\n",
    "    *   3.7. *Збереження Моделі:* Збережіть найкращу модель (регресійну або класифікаційну) та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування якості вина**. Застосунок приймає на вхід фізико-хімічні показники вина та видає або прогнозовану оцінку (регресія), або прогнозовану категорію якості (класифікація).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost` (якщо використовується).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель (регресійну чи класифікаційну) та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input` / `gr.Number`) для всіх фізико-хімічних ознак ('fixed acidity', 'volatile acidity', ..., 'alcohol').\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Сформуйте вектор ознак. **Застосуйте збережений скейлер `StandardScaler`**.\n",
    "    6.  *Прогнозування:*\n",
    "        *   (Регресія) Зробіть прогноз (`model.predict()`). Результат - числова оцінка.\n",
    "        *   (Класифікація) Зробіть прогноз класу (`model.predict()`). Результат - мітка категорії ('Погане', 'Середнє', 'Добре'). Можна також отримати ймовірності (`model.predict_proba()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану оцінку або категорію якості вина.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Гістограма 'quality', кореляційна теплова карта, **діаграми важливості ознак**, матриця плутанини (для класифікації), графік прогнозів проти фактичних значень (для регресії).\n",
    "*   **Метрики оцінки:**\n",
    "    *   *Регресія:* MAE, MSE, RMSE, R-squared.\n",
    "    *   *Класифікація:* Accuracy, Precision, Recall, F1 (macro/weighted та для кожного класу).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   **Масштабування ознак** (`StandardScaler`) необхідне.\n",
    "*   **Критично обміркуйте та обґрунтуйте вибір між регресійним та класифікаційним підходом**. Класифікація часто є більш стабільною та інтерпретованою для цієї задачі через дискретний та незбалансований характер оцінок 'quality'.\n",
    "*   Проаналізуйте **важливість ознак**, щоб зрозуміти, які хімічні показники є ключовими для якості червоного вина (наприклад, 'alcohol', 'volatile acidity', 'sulphates' часто є важливими).\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 20: Аналіз тональності відгуків на фільми\n",
    "\n",
    "**Набір даних:** Large Movie Review Dataset (IMDb) ([https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews))\n",
    "\n",
    "**Мета:** **Класифікувати відгуки на фільми** як позитивні чи негативні на основі текстового вмісту (_Класифікація тексту / Аналіз тональності_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження та Попереднє Оброблення Текстових Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `IMDB Dataset.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Перегляньте стовпці ('review', 'sentiment'). Перевірте баланс класів ('positive'/'negative').\n",
    "    *   1.3. ***Попереднє Оброблення тексту:*** Це **ключовий етап** для НЛП завдань. Створіть функцію для очищення тексту, яка виконує наступні кроки:\n",
    "        *   Видалення HTML-тегів (часто присутні в веб-відгуках).\n",
    "        *   Приведення до **нижнього регістру**.\n",
    "        *   Видалення **пунктуації** та чисел (або заміна їх спеціальним токеном).\n",
    "        *   **Токенізація:** Розбиття тексту на окремі слова (токени).\n",
    "        *   Видалення **стоп-слів** (поширені слова, як-от 'the', 'is', 'in', які зазвичай не несуть багато інформації про тональність) за допомогою списку стоп-слів з `nltk.corpus.stopwords`.\n",
    "        *   **Стемінг або Лематизація:** Приведення слів до їх базової форми.\n",
    "            *   *Стемінг* (наприклад, `PorterStemmer` з `nltk.stem`): Грубий процес відкидання закінчень. Швидший, але може призводити до некоректних основ.\n",
    "            *   *Лематизація* (наприклад, `WordNetLemmatizer` з `nltk.stem`): Використовує словник для приведення слова до його канонічної форми (леми). Зазвичай краще за якістю, але повільніше.\n",
    "        *   Застосуйте цю функцію до стовпця 'review'.\n",
    "    *   1.4. *EDA Тексту:*\n",
    "        *   Проаналізуйте довжину відгуків (кількість слів/символів) для позитивних та негативних класів.\n",
    "        *   Створіть **хмари слів (Word Clouds)** для позитивних та негативних відгуків, щоб візуально побачити найчастіші слова в кожному класі.\n",
    "\n",
    "2.  **Вилучення Ознак та Базова Класифікація:**\n",
    "    *   2.1. *Розділення даних:* Розділіть дані на тренувальну та тестову вибірки.\n",
    "    *   2.2. ***Векторизація Тексту:*** Перетворіть очищені текстові дані на числові вектори ознак:\n",
    "        *   **CountVectorizer (`sklearn.feature_extraction.text.CountVectorizer`):** Створює матрицю, де значення - це кількість входжень кожного слова (токену) в документі (Bag-of-Words).\n",
    "        *   **TfidfVectorizer (`sklearn.feature_extraction.text.TfidfVectorizer`):** Створює матрицю, де значення - це TF-IDF (Term Frequency-Inverse Document Frequency) вага кожного слова. TF-IDF зменшує вагу дуже поширених слів і збільшує вагу слів, характерних для конкретного документа. **Зазвичай дає кращі результати.**\n",
    "        *   Експериментуйте з параметрами векторизаторів (наприклад, `max_features`, `ngram_range`).\n",
    "    *   2.3. *Навчання базових класифікаторів:* Навчіть класичні ML моделі на TF-IDF ознаках:\n",
    "        *   Multinomial Naive Bayes (`sklearn.naive_bayes.MultinomialNB`) - добре підходить для текстових даних.\n",
    "        *   Logistic Regression (`sklearn.linear_model.LogisticRegression`).\n",
    "        *   Support Vector Machine (SVM) (`sklearn.svm.LinearSVC` або `SVC`) - часто показує високу точність у задачах класифікації тексту.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на тестовій вибірці за допомогою accuracy, precision, recall, $F_1$-score, AUC-ROC. Проаналізуйте матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання (Ембединги/DL) та Розгортання:**\n",
    "    *   3.1. *Використання Ембедингів (Опціонально):*\n",
    "        *   Дослідіть використання **натренованих векторних представлень слів (word embeddings)**, таких як Word2Vec (`gensim`) або GloVe. Це може вимагати агрегації векторів слів для отримання вектора документа (наприклад, усереднення).\n",
    "        *   Навчіть класифікатори на цих ембедингах.\n",
    "    *   3.2. *Прості Моделі Глибокого Навчання (Опціонально):*\n",
    "        *   Підготуйте дані для DL (токенізація, паддінг послідовностей).\n",
    "        *   Побудуйте та навчіть прості **CNN** або **LSTM/GRU** моделі для класифікації тексту за допомогою Keras/TensorFlow/PyTorch. Можна використовувати шар `Embedding` для навчання власних ембедингів або ініціалізації натренованими.\n",
    "    *   3.3. *Порівняння Продуктивності:* **Порівняйте** результати базових моделей на TF-IDF з результатами моделей на ембедингах або DL-моделей (якщо реалізовано).\n",
    "    *   3.4. *Збереження Моделі:* Збережіть найкращу модель (ймовірно, SVM/Logistic Regression на TF-IDF або DL модель) та відповідний векторизатор/компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для аналізу тональності тексту**. Користувач може ввести текст відгуку на фільм (або будь-який інший текст), і застосунок **прогнозує його тональність** (позитивна/негативна).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `numpy`, `joblib`/`pickle`, `sklearn`, `nltk`, `re`. Якщо DL модель, то `tensorflow`/`pytorch`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та **векторизатор** (CountVectorizer/TfidfVectorizer) або компоненти обробки для DL. Завантажте функцію попереднього оброблення тексту.\n",
    "    4.  *Створення інтерфейсу:* Створіть текстове поле для введення відгуку (`st.text_area` / `gr.Textbox`).\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Отримайте текст від користувача.\n",
    "        *   **Застосуйте ту саму функцію попереднього оброблення тексту**, що й під час тренування.\n",
    "        *   **Застосуйте збережений векторизатор** (`vectorizer.transform()`) до обробленого тексту.\n",
    "        *   (Для DL) Виконайте токенізацію та паддінг, як під час тренування.\n",
    "    6.  *Прогнозування:* Зробіть прогноз тональності (`model.predict()`). Можна також отримати ймовірності (`predict_proba`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану тональність (\"Позитивний відгук\" / \"Негативний відгук\") та, можливо, впевненість моделі (ймовірність).\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки. Переконайтеся, що дані `nltk` (stopwords, wordnet) доступні на платформі розгортання (можна завантажити їх у скрипті або додати до файлів).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, векторизатор, скрипт з функцією очищення, `requirements.txt` (включно з `nltk`). Можливо, знадобиться завантажити дані `nltk` через Shell (`python -m nltk.downloader all`).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `nltk` (обов'язково), `re`, опціонально `gensim` (для Word2Vec), `keras`/`tensorflow`/`pytorch` (для DL), `wordcloud`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `nltk`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Хмари слів (Word clouds)**, розподіл довжини відгуків, матриця плутанини, ROC-крива.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score, AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Вибір методів попереднього оброблення тексту*** (видалення стоп-слів, стемінг/лематизація) суттєво впливає на результати. Експериментуйте з різними варіантами.\n",
    "*   **TF-IDF** є дуже сильним та ефективним базовим методом для класифікації тексту.\n",
    "*   Порівняйте результати традиційних ML моделей (SVM, Logistic Regression) на TF-IDF з DL підходами, якщо їх реалізуєте. SVM часто є дуже конкурентоспроможним.\n",
    "*   Переконайтеся, що **Оброблення тексту в застосунку точно відповідає** обробці під час тренування моделі.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 21: Інтелектуальна інформаційна система для прогнозування попиту на прокат велосипедів\n",
    "\n",
    "**Набір даних:** Bike Sharing Dataset (Capital Bikeshare `hour.csv`) ([https://www.kaggle.com/datasets/yasserh/bike-sharing-dataset](https://www.kaggle.com/datasets/yasserh/bike-sharing-dataset))\n",
    "\n",
    "**Мета:** **Прогнозувати погодинну кількість прокатів велосипедів** ('cnt') на основі погодних та часових ознак (_Регресія / Часові ряди_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження Даних та Інженерія Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `hour.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Оброблення Дат та Часу:*\n",
    "        *   Перетворіть стовпець 'dteday' на дату, а потім об'єднайте з годиною ('hr') для створення повноцінного datetime індексу або стовпця.\n",
    "        *   Видаліть стовпець 'instant', оскільки це просто індекс рядка.\n",
    "    *   1.3. *Інженерія Часових Ознак:* Витягніть з дати/часу корисні ознаки:\n",
    "        *   **Година дня ('hr')** - вже є, але важлива.\n",
    "        *   **День тижня ('weekday')**.\n",
    "        *   **Місяць ('mnth')**.\n",
    "        *   **Рік ('yr')**.\n",
    "        *   Ознака вихідного дня ('workingday') - вже є.\n",
    "        *   Сезон ('season') - вже є.\n",
    "    *   1.4. *Аналіз Цільової Змінної ('cnt'):*\n",
    "        *   Візуалізуйте розподіл 'cnt'. Він, ймовірно, буде **асиметричним**. Розгляньте **логарифмічне перетворення** (`np.log1p`) для 'cnt', а також для 'casual' та 'registered', якщо будете їх моделювати.\n",
    "    *   1.5. *EDA:*\n",
    "        *   **Візуалізуйте патерни:** Побудуйте графіки залежності середньої кількості прокатів ('cnt') від години дня, дня тижня, місяця, сезону, погоди ('weathersit'). Використовуйте `boxplot` або `pointplot`.\n",
    "        *   Дослідіть взаємозв'язок між погодними умовами ('temp', 'atemp', 'hum', 'windspeed') та кількістю прокатів ('cnt') за допомогою діаграм розсіювання.\n",
    "        *   Візуалізуйте часовий ряд 'cnt' за весь період.\n",
    "        *   Проаналізуйте кореляції між числовими ознаками.\n",
    "\n",
    "2.  **Попереднє Оброблення та Базова Регресія:**\n",
    "    *   2.1. *Оброблення Категоріальних Ознак:* Ознаки 'season', 'yr', 'mnth', 'hr', 'weekday', 'weathersit' можна розглядати як категоріальні. Застосуйте One-Hot Encoding. (Альтернативно, деякі моделі на основі дерев можуть обробляти їх як числові, але OHE є безпечнішим варіантом).\n",
    "    *   2.2. *Масштабування Ознак:* Застосуйте `StandardScaler` або `MinMaxScaler` до числових погодних ознак ('temp', 'atemp', 'hum', 'windspeed').\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (враховуючи часову природу даних, краще використовувати **часовий поділ** - наприклад, останній місяць/рік для тестування). Використовуйте log-трансформовану 'cnt' як цільову змінну.\n",
    "    *   2.4. *Навчання базових регресійних моделей:* Навчіть Linear Regression, Ridge, Lasso.\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на тестовій вибірці (на log-шкалі) за допомогою MAE, MSE, RMSE, R-squared. Розгляньте **RMSLE (Root Mean Squared Logarithmic Error)**, яка є поширеною метрикою для подібних завдань і природно працює з log-трансформованою цільовою змінною.\n",
    "\n",
    "3.  **Розширене Моделювання та Часовий Аналіз:**\n",
    "    *   3.1. *Реалізація моделей на основі дерев:* Навчіть *просунуті моделі*, які добре працюють з табличними даними та взаємодіями ознак: Random Forest Regressor, XGBoost Regressor, LightGBM Regressor.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте гіперпараметри для кращих моделей за допомогою крос-валідації (використовуйте `TimeSeriesSplit` з `sklearn.model_selection` для збереження часової структури під час валідації).\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх моделей за вибраними метриками (RMSE, RMSLE).\n",
    "    *   3.4. ***Аналіз Важливості Ознак:*** Визначте, які **часові** (година, день тижня, сезон) та **погодні** (температура, вологість, погода) фактори найбільше впливають на попит на велосипеди.\n",
    "    *   3.5. *Аналіз Продуктивності у Часі:* Візуалізуйте прогнози найкращої моделі проти фактичних значень на тестовій вибірці у вигляді часового ряду. Проаналізуйте, чи добре модель вловлює добові та тижневі патерни.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та об'єкти попереднього оброблення.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування погодинного попиту на прокат велосипедів**. Застосунок дає змогу користувачеві ввести (або вибрати) часові та погодні умови і отримати **прогноз очікуваної кількості прокатів**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, кодувальник(и) або список стовпців OHE.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення: `st.selectbox` для сезону, місяця, години, дня тижня, погоди; `st.number_input` або `st.slider` для температури, вологості, швидкості вітру.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте ту саму попередню обробку**: кодування категоріальних (OHE), масштабування числових. Переконайтесь, що всі ознаки моделі присутні у правильному порядку.\n",
    "    6.  *Прогнозування:* Зробіть прогноз за допомогою моделі. **Застосуйте обернене логарифмічне перетворення** (`np.expm1()`) до результату, щоб отримати кількість велосипедів. Округліть до цілого числа.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану кількість прокатів велосипедів.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, кодувальники/список стовпців, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Часовий ряд 'cnt', **box plots ('cnt' проти години/дня тижня/сезону/погоди)**, scatter plots ('temp'/'hum' проти 'cnt'), **діаграма важливості ознак**, **графік прогнозованих проти фактичних значень у часі**.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, R-squared. Розгляньте ***RMSLE***, якщо використовуєте log(cnt), оскільки вона штрафує недооцінку більше, ніж переоцінку, і нечутлива до масштабу.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Інженерія часових ознак є вирішальною*** для вловлювання добових та сезонних патернів.\n",
    "*   **Логарифмічне перетворення 'cnt'** (`np.log1p`) зазвичай необхідне для стабілізації дисперсії та наближення розподілу до нормального.\n",
    "*   Використовуйте `TimeSeriesSplit` для крос-валідації, щоб уникнути витоку майбутніх даних у тренувальну вибірку.\n",
    "*   Порівняйте, наскільки добре різні моделі (лінійні vs дерева) вловлюють складні **часові динаміки** та взаємодії ознак.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 22: Інтелектуальна інформаційна система для визначення твітів про катастрофи\n",
    "\n",
    "**Набір даних:** Real or Not? NLP with Disaster Tweets ([https://www.kaggle.com/competitions/nlp-getting-started/data](https://www.kaggle.com/competitions/nlp-getting-started/data)) - _Потрібен акаунт Kaggle для доступу до даних змагання (`train.csv`, `test.csv`, `sample_submission.csv`)_.\n",
    "\n",
    "**Мета:** **Класифікувати твіти** як такі, що описують реальні катастрофи (target=1), чи ні (target=0), на основі тексту (_Класифікація тексту_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Очищення Текстових Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `train.csv` та `test.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Перегляньте стовпці ('id', 'keyword', 'location', 'text', 'target'). Дослідіть розподіл цільової змінної 'target'. Перевірте наявність пропусків у 'keyword' та 'location'.\n",
    "    *   1.3. ***Очищення твітів:*** Створіть функцію для ретельного очищення стовпця 'text':\n",
    "        *   Видалення URL-адрес.\n",
    "        *   Видалення згадок користувачів (@username).\n",
    "        *   Видалення хештегів (#hashtag) або збереження лише тексту хештегу.\n",
    "        *   Видалення спеціальних символів, пунктуації, чисел (або їх стандартизація).\n",
    "        *   Обробка скорочень та інтернет-сленгу (складно, можливо, пропустити для базової версії).\n",
    "        *   Приведення до нижнього регістру.\n",
    "        *   Токенізація.\n",
    "        *   Видалення стоп-слів (`nltk`).\n",
    "        *   Стемінг або Лематизація (`nltk`).\n",
    "        *   Застосуйте функцію очищення до 'text' в `train.csv` та `test.csv`.\n",
    "    *   1.4. *Аналіз тексту та метаданих:*\n",
    "        *   Проаналізуйте довжину очищених твітів для кожного класу 'target'.\n",
    "        *   Дослідіть найчастіші слова/біграми для кожного класу.\n",
    "        *   Проаналізуйте стовпці 'keyword' та 'location': чи є вони корисними? Як обробити пропуски або унікальні значення в 'location'?\n",
    "\n",
    "2.  **Вилучення Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Підготовка даних:* Розділіть `train.csv` на тренувальну та валідаційну вибірки (з стратифікацією за 'target').\n",
    "    *   2.2. *Векторизація Тексту:* Перетворіть очищений 'text' на числові вектори за допомогою **Bag-of-Words (CountVectorizer)** або **TF-IDF (TfidfVectorizer)**.\n",
    "    *   2.3. *Інженерія Додаткових Ознак (Опціонально):* Розгляньте додавання ознак на основі 'keyword' (після очищення та кодування) або простих текстових статистик (довжина твіту, кількість великих літер тощо).\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Naive Bayes (MultinomialNB або BernoulliNB), Logistic Regression, SVM (`LinearSVC`) на створених векторних ознаках.\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на валідаційній вибірці. **Основною метрикою змагання є $F_1$-score**. Також обчисліть Accuracy, Precision, Recall, AUC-ROC. Проаналізуйте матрицю плутанини.\n",
    "\n",
    "3.  **Просунуті Техніки НЛП та Підготовка до Подання:**\n",
    "    *   3.1. *Експерименти:*\n",
    "        *   Спробуйте різні параметри векторизації (n-грами `ngram_range=(1, 2)` часто покращують результат).\n",
    "        *   Експериментуйте з різними методами очищення тексту.\n",
    "        *   Навчіть більш просунуті моделі, як-от Random Forest або XGBoost (можуть бути повільними на високорозмірних текстових даних).\n",
    "    *   3.2. *Використання Натренованих Ембедингів (Опціонально):* Дослідіть використання натренованих ембедингів (GloVe, FastText) для представлення слів/твітів.\n",
    "    *   3.3. *Прості DL Моделі (Опціонально):* Реалізуйте просту CNN або LSTM модель для класифікації твітів.\n",
    "    *   3.4. *Вибір Найкращої Моделі:* Виберіть модель та параметри, що дають найкращий $F_1$-score на валідаційній вибірці.\n",
    "    *   3.5. ***Підготовка Файлу для Подання на Kaggle:***\n",
    "        *   Навчіть найкращу модель на **всьому** тренувальному наборі (`train.csv`).\n",
    "        *   Зробіть прогнози (`0` або `1`) для **очищених та векторизованих** даних з `test.csv`.\n",
    "        *   Створіть файл `submission.csv` у форматі, вказаному в `sample_submission.csv` (стовпці 'id' та 'target').\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та векторизатор/компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для класифікації твітів**. Користувач може ввести текст твіту, і застосунок визначить, чи описує він **реальну катастрофу чи ні**. Також підготовлено файл `submission.csv` для можливого подання на платформу Kaggle.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `numpy`, `joblib`/`pickle`, `sklearn`, `nltk`, `re`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, **векторизатор** та функцію очищення тексту.\n",
    "    4.  *Створення інтерфейсу:* Створіть текстове поле (`st.text_area` / `gr.Textbox`) для введення тексту твіту.\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Отримайте текст.\n",
    "        *   **Застосуйте функцію очищення тексту**.\n",
    "        *   **Застосуйте збережений векторизатор** (`vectorizer.transform()`).\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть результат: \"Твіт про реальну катастрофу\" або \"Твіт не про катастрофу\".\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки (зверніть увагу на доступність даних `nltk`).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, векторизатор, скрипт очищення, `requirements.txt`. Налаштуйте завантаження даних `nltk`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `nltk` (обов'язково), `re`, опціонально `keras`/`tensorflow`/`pytorch`, `gensim`, `wordcloud`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `nltk`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл класів, хмари слів, матриця плутанини.\n",
    "*   **Метрики оцінки:** ***$F_1$-score (основна)***, Accuracy, Precision, Recall, AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Очищення твітів потребує обережного використання регулярних виразів (`re`)***. Це дуже важливий етап.\n",
    "*   Інженерія ознак на основі метаданих ('keyword', 'location') може покращити результати, але потребує ретельної обробки пропусків та категорій.\n",
    "*   **Підготовка файлу для подання на Kaggle** є корисною практикою для розуміння формату змагань.\n",
    "*   TF-IDF з n-грамами (1, 2) та Logistic Regression або LinearSVC часто є дуже сильним базовим рішенням для цього завдання.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 23: Інтелектуальна інформаційна система для прогнозування цін на будинки (Розширена регресія)\n",
    "\n",
    "**Набір даних:** House Prices - Advanced Regression Techniques ([https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)) - _Потрібен акаунт Kaggle для доступу до даних змагання (`train.csv`, `test.csv`, `sample_submission.csv`, `data_description.txt`)_.\n",
    "\n",
    "**Мета:** **Прогнозувати ціни продажу будинків** ('SalePrice') в Еймсі, штат Айова, використовуючи ~80 різноманітних ознак (_Розширена регресія_). **Основна метрика оцінки - RMSLE**.\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Розширене Очищення Даних та EDA:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `train.csv` та `test.csv`. Об'єднайте їх (зберігаючи мітку джерела) для спільної обробки ознак (окрім цільової змінної). **Уважно вивчіть `data_description.txt`!**\n",
    "    *   1.2. *Аналіз Цільової Змінної ('SalePrice'):*\n",
    "        *   Візуалізуйте розподіл 'SalePrice'. Він **сильно асиметричний**.\n",
    "        *   ***Застосуйте логарифмічне перетворення*** (`np.log1p`) до 'SalePrice' у тренувальному наборі. Працюйте з log(SalePrice) як з цільовою змінною.\n",
    "    *   1.3. ***Оброблення Пропущених Значень:*** Це **критично важливий етап**.\n",
    "        *   Ідентифікуйте ознаки з пропусками та їх відсоток.\n",
    "        *   **Стратегічно імпутуйте пропуски**, керуючись `data_description.txt`:\n",
    "            *   Деякі NaN мають значення (наприклад, NaN в 'PoolQC' означає відсутність басейну) - замініть на 'None' або 0.\n",
    "            *   Для числових ознак використовуйте медіану/середнє (можливо, згруповане за районом 'Neighborhood').\n",
    "            *   Для категоріальних - моду.\n",
    "            *   Розгляньте `KNNImputer` або моделі для імпутації.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Проаналізуйте кореляції між числовими ознаками та log(SalePrice).\n",
    "        *   Візуалізуйте взаємозв'язки ключових ознак (наприклад, 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'YearBuilt') з log(SalePrice) (діаграми розсіювання, коробкові діаграми).\n",
    "        *   Дослідіть викиди, особливо в ознаках площі.\n",
    "    *   1.5. *Оброблення Асиметрії Ознак:* Деякі числові ознаки можуть бути асиметричними. Розгляньте логарифмічне перетворення (`np.log1p`) для них (наприклад, ознак площі).\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Інженерія Ознак:*\n",
    "        *   Створіть нові ознаки, комбінуючи існуючі: загальна площа ('TotalSF' = 'TotalBsmtSF' + '1stFlrSF' + '2ndFlrSF'), загальна кількість ванних кімнат, вік будинку ('YrSold' - 'YearBuilt'), вік реконструкції ('YrSold' - 'YearRemodAdd').\n",
    "        *   Перетворіть деякі числові ознаки, що насправді є категоріальними (наприклад, 'MSSubClass'), на рядковий тип.\n",
    "    *   2.2. *Оброблення Категоріальних Ознак:*\n",
    "        *   Визначте порядкові та номінальні категоріальні ознаки.\n",
    "        *   Застосуйте **Label Encoding** для порядкових ознак (якщо порядок має значення, наприклад, якість 'Ex', 'Gd', 'TA').\n",
    "        *   Застосуйте **One-Hot Encoding** (`pd.get_dummies`) для номінальних ознак.\n",
    "    *   2.3. *Масштабування:* Застосуйте `StandardScaler` або `RobustScaler` до всіх числових ознак (після обробки асиметрії та інженерії).\n",
    "    *   2.4. *Підготовка даних:* Розділіть об'єднані дані назад на тренувальний та тестовий набори.\n",
    "    *   2.5. *Навчання базових моделей:* Навчіть Ridge та Lasso регресію. Використовуйте крос-валідацію (наприклад, з 5-10 фолдами) на тренувальних даних для оцінки RMSLE. `Lasso` може виконувати неявний вибір ознак.\n",
    "\n",
    "3.  **Розширене Моделювання та Ансамблювання:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть потужні моделі градієнтного бустингу: **XGBoost**, **LightGBM**, **CatBoost** (останній може добре обробляти категоріальні ознаки без явного OHE).\n",
    "    *   3.2. *Налаштування Гіперпараметрів:* **Ретельно налаштуйте гіперпараметри** для моделей бустингу за допомогою крос-валідації (наприклад, `GridSearchCV`, `RandomizedSearchCV` або `Optuna`/`Hyperopt`), оптимізуючи за **RMSLE** (або MSE на log-шкалі).\n",
    "    *   3.3. *Ансамблювання/Стекінг (Опціонально):*\n",
    "        *   Спробуйте усереднити прогнози кількох кращих моделей (простий ансамбль).\n",
    "        *   Реалізуйте **стекінг**: навчіть мета-модель (наприклад, Ridge або Lasso) на прогнозах базових моделей (XGBoost, LightGBM тощо), отриманих за допомогою крос-валідації.\n",
    "    *   3.4. *Підготовка Файлу для Подання на Kaggle:*\n",
    "        *   Зробіть прогнози на **тестовому наборі** (`test.csv`) за допомогою вашої найкращої моделі (або ансамблю).\n",
    "        *   **Не забудьте застосувати обернене логарифмічне перетворення** (`np.expm1()`) до прогнозів, щоб отримати ціни в оригінальній шкалі.\n",
    "        *   Створіть файл `submission.csv` у потрібному форматі.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель/ансамбль та всі компоненти пайплайну попереднього оброблення.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Розроблено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування цін на будинки** в Еймсі. Застосунок приймає на вхід ключові характеристики будинку та видає прогнозовану ціну. Також підготовлено файл `submission.csv` для Kaggle.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`/`catboost`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель/ансамбль та **повний пайплайн попереднього оброблення** (імпутери, скейлери, кодувальники, список ознак). Це зручно зробити за допомогою `sklearn.pipeline.Pipeline`.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для **ключових ознак**, які найбільше впливають на ціну (наприклад, 'OverallQual', 'GrLivArea', 'Neighborhood', 'YearBuilt', 'TotalBsmtSF', 'GarageCars'). Можливо, знадобиться спростити введення для деяких категоріальних ознак.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Створіть DataFrame. **Застосуйте збережений пайплайн попереднього оброблення** до введених даних.\n",
    "    6.  *Прогнозування:* Зробіть прогноз за допомогою моделі/ансамблю. **Застосуйте обернене логарифмічне перетворення** (`np.expm1()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану ціну будинку.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, `catboost` (опціонально), `scipy` (для статистики)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл 'SalePrice' (вихідний/log), **діаграми розсіювання ключових ознак проти log(SalePrice)**, кореляційна теплова карта, діаграма важливості ознак, діаграми залишків.\n",
    "*   **Метрики оцінки:** ***RMSLE (Root Mean Squared Logarithmic Error) - основна***. Також MAE, RMSE, R-squared (на оригінальній шкалі після зворотного перетворення для інтерпретації).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Цей проєкт робить **великий наголос на інженерії ознак та ретельній обробці пропущених значень**. `data_description.txt` є вашим найкращим другом.\n",
    "*   ***Логарифмічне перетворення 'SalePrice'*** (`np.log1p`) є майже обов'язковим для покращення продуктивності моделей.\n",
    "*   Використовуйте потужні моделі **градієнтного бустингу** (XGBoost, LightGBM).\n",
    "*   **Крос-валідація** є ключовою для надійної оцінки та налаштування гіперпараметрів.\n",
    "*   Ансамблювання/стекінг часто допомагають отримати кращі результати на Kaggle.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 24: Система рекомендації сільськогосподарських культур\n",
    "\n",
    "**Набір даних:** Crop Recommendation Dataset ([https://www.kaggle.com/datasets/atharvaingle/crop-recommendation-dataset](https://www.kaggle.com/datasets/atharvaingle/crop-recommendation-dataset))\n",
    "\n",
    "**Мета:** **Рекомендувати найкращу сільськогосподарську культуру** ('label') для вирощування на основі ґрунтових (вміст Азоту - 'N', Фосфору - 'P', Калію - 'K') та кліматичних/екологічних (температура - 'temperature', вологість - 'humidity', кислотність ґрунту - 'ph', кількість опадів - 'rainfall') факторів (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Візуалізація Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `Crop_recommendation.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Перевірте типи даних, наявність пропусків (зазвичай немає), основні статистики.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('label'):*\n",
    "        *   Дослідіть унікальні значення культур у стовпці 'label'.\n",
    "        *   Перевірте **розподіл культур** (кількість записів для кожної культури). Чи є дані збалансованими?\n",
    "    *   1.4. *EDA:*\n",
    "        *   **Візуалізуйте розподіли ґрунтових та кліматичних ознак для кожної культури.** Використовуйте:\n",
    "            *   `boxplot` або `violinplot` для кожної числової ознаки ('N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'), згруповані за 'label'. Це допоможе побачити, які умови є оптимальними для кожної культури.\n",
    "        *   Побудуйте **парні діаграми (`pairplot`)** для підмножини ознак, забарвлені за типом культури, щоб побачити взаємодію між факторами.\n",
    "        *   Проаналізуйте кореляції між вхідними ознаками за допомогою теплової карти.\n",
    "\n",
    "2.  **Попереднє Оброблення та Базова Класифікація:**\n",
    "    *   2.1. *Масштабування ознак:* Оскільки ознаки мають різні діапазони значень (наприклад, N/P/K можуть відрізнятися від температури чи pH), ***застосуйте `StandardScaler`*** до всіх числових вхідних ознак.\n",
    "    *   2.2. *Підготовка даних:* Розділіть дані на ознаки (N, P, K, ...) та цільову змінну ('label'). Розділіть на тренувальну та тестову вибірки (з стратифікацією за 'label', якщо є дисбаланс).\n",
    "    *   2.3. *Навчання базових багатокласових класифікаторів:* Навчіть:\n",
    "        *   K-Nearest Neighbors (K-NN).\n",
    "        *   Gaussian Naive Bayes (може добре працювати, якщо припустити умовну незалежність ознак).\n",
    "        *   Logistic Regression (з параметром `multi_class='ovr'` або `'multinomial'`).\n",
    "        *   Decision Tree.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на тестовій вибірці. Використовуйте:\n",
    "        *   Accuracy.\n",
    "        *   **Classification Report** (з Precision, Recall, $F_1$-score **для кожної культури**).\n",
    "        *   Macro/Weighted Average $F_1$-score.\n",
    "        *   Матрицю плутанини для аналізу помилок між культурами.\n",
    "\n",
    "3.  **Розширене Моделювання та Розгортання:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть *просунуті моделі*, які добре справляються з багатокласовою класифікацією: Random Forest, Support Vector Machine (SVM), XGBoost.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте гіперпараметри для кращих моделей (RF, XGBoost), максимізуючи Accuracy або Weighted $F_1$-score.\n",
    "    *   3.3. *Порівняння Моделей:* **Порівняйте продуктивність** усіх моделей. Зверніть увагу на те, наскільки добре кожна модель класифікує окремі культури (аналіз метрик у Classification Report).\n",
    "    *   3.4. *Аналіз Важливості Ознак:* Для найкращої моделі (RF/XGBoost) **визначте, які ґрунтові та кліматичні фактори** є найбільш важливими для рекомендації культур.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для рекомендації сільськогосподарських культур**. Користувач може ввести значення ґрунтових (N, P, K) та кліматичних (температура, вологість, pH, опади) показників, і застосунок **рекомендує найкращу культуру** для вирощування за цих умов.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost` (якщо використовується).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input` або `st.slider` / `gr.Number` або `gr.Slider`) для всіх 7 вхідних ознак (N, P, K, temperature, humidity, ph, rainfall). Встановіть розумні діапазони значень.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Сформуйте вектор ознак. **Застосуйте збережений скейлер `StandardScaler`**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`). Результат - назва рекомендованої культури. Можна також отримати ймовірності для всіх культур (`model.predict_proba()`), якщо це цікаво.\n",
    "    7.  *Відображення результату:* Покажіть назву рекомендованої культури. Можна додати зображення культури або коротку інформацію про неї.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Розподіли ознак для кожної культури (boxplot)**, парна діаграма (підмножина), матриця плутанини, **діаграма важливості ознак**.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score (**macro/weighted та для кожної культури**), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Масштабування ознак (`StandardScaler`) необхідне***, оскільки ознаки мають дуже різні діапазони.\n",
    "*   **Ретельно аналізуйте продуктивність для кожного класу (культури)** за допомогою Classification Report та матриці плутанини, щоб зрозуміти, які культури модель плутає.\n",
    "*   Інтерпретуйте **важливість ознак**, щоб отримати уявлення про ключові фактори для вибору культур в агрономії.\n",
    "*   Набір даних відносно чистий і збалансований, що робить його добрим для навчальних цілей.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 25: Інтелектуальна інформаційна система для виявлення аномалій у даних сенсорів (NAB)\n",
    "\n",
    "**Набір даних:** Numenta Anomaly Benchmark (NAB) ([https://github.com/numenta/NAB](https://github.com/numenta/NAB)). Набір містить кілька часових рядів з різних доменів (AWS CPU, Twitter volume, traffic тощо) з міченими аномальними вікнами. Потрібно вибрати 1-2 конкретні датасети з репозиторію (наприклад, з папки `data/`). Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/bollevitta/numenta-anomaly-benchmark-nab](https://www.kaggle.com/datasets/bollevitta/numenta-anomaly-benchmark-nab))\n",
    "\n",
    "**Мета:** **Виявляти аномалії** (несподівані відхилення, зміни патернів) в реальних часових рядах даних сенсорів, використовуючи переважно **некеровані методи** (_Виявлення аномалій у часових рядах_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних та Візуалізація Часових Рядів:**\n",
    "    *   1.1. *Вибір та Завантаження даних:*\n",
    "        *   Виберіть 1-2 часових ряди з NAB (наприклад, `realAWSCloudwatch/ec2_cpu_utilization_...csv`, `realTraffic/occupancy_t4013.csv`).\n",
    "        *   Завантажте відповідний CSV файл за допомогою `pandas`.\n",
    "        *   **Завантажте мітки аномалій:** Аномальні вікна зазвичай визначені в окремому JSON файлі (`labels/combined_labels.json` в репозиторії NAB). Розпарсіть цей файл, щоб отримати часові мітки початку та кінця відомих аномалій для вибраного ряду.\n",
    "    *   1.2. *Підготовка даних:*\n",
    "        *   Перетворіть стовпець часових міток на datetime індекс.\n",
    "        *   Переконайтесь, що ряд відсортований за часом.\n",
    "        *   Обробка пропусків (якщо є).\n",
    "    *   1.3. *Візуалізація:*\n",
    "        *   **Побудуйте графік часового ряду** (значення сенсора від часу).\n",
    "        *   ***Нанесіть на графік відомі аномальні вікна*** (з файлу міток), щоб візуально побачити, де знаходяться аномалії, які потрібно виявити.\n",
    "\n",
    "2.  **Застосування Алгоритмів Виявлення Аномалій:**\n",
    "    *   *Примітка:* Оскільки це завдання часто розглядається як некероване (ми не використовуємо мітки для навчання), моделі навчаються на всьому ряді або його частині.\n",
    "    *   2.1. *Статистичні методи:*\n",
    "        *   **Метод Z-score/Standard Deviation:** Обчисліть ковзне середнє та стандартне відхилення. Позначте точки, що виходять за межі певного порогу (наприклад, > 3 стандартних відхилень від середнього), як аномалії.\n",
    "    *   2.2. *Алгоритми на основі відстані/щільності (застосовуються до вікон або вбудувань):*\n",
    "        *   **Local Outlier Factor (LOF) (`sklearn.neighbors.LocalOutlierFactor`):** Можна застосувати до ознак, витягнутих з ковзних вікон часового ряду.\n",
    "    *   2.3. *Алгоритми на основі дерев/ізоляції:*\n",
    "        *   **Isolation Forest (`sklearn.ensemble.IsolationForest`):** Ефективний для виявлення викидів. Може працювати безпосередньо на значеннях ряду або на ознаках з вікон.\n",
    "    *   2.4. *Моделі на основі автокодувальників (Просунуто):*\n",
    "        *   Навчіть автокодувальник (наприклад, LSTM-Autoencoder) на нормальних ділянках ряду (якщо можливо їх ідентифікувати) або на всьому ряді. Аномалії відповідатимуть високій помилці реконструкції.\n",
    "    *   2.5. *Налаштування Параметрів:* Для багатьох алгоритмів (Isolation Forest, LOF) потрібно налаштувати параметр `contamination` (очікувана частка аномалій) або інші параметри (розмір вікна, `eps` для LOF). Експериментуйте з різними значеннями.\n",
    "\n",
    "3.  **Оцінка та Візуалізація Виявлених Аномалій:**\n",
    "    *   3.1. *Генерація Міток Аномалій Моделлю:* Кожен застосований алгоритм повинен видати мітку для кожної точки даних (аномалія/норма).\n",
    "    *   3.2. ***Оцінка з Використанням Міток NAB:***\n",
    "        *   Порівняйте прогнозовані аномалії з відомими аномальними вікнами з файлу міток.\n",
    "        *   Обчисліть стандартні метрики класифікації: **Precision, Recall, $F_1$-score** для класу аномалій. _Увага:_ Оцінка часових рядів з вікнами аномалій може бути складною (чи зараховувати часткове перекриття?). Можна використовувати стандартний підхід NAB або спрощену точкову оцінку.\n",
    "        *   Розгляньте можливість використання **спеціалізованих метрик NAB** (якщо вдасться знайти реалізацію їхнього скорингового механізму), які враховують своєчасність виявлення та штрафують за хибні спрацювання.\n",
    "    *   3.3. *Візуалізація Результатів:*\n",
    "        *   **Нанесіть на графік часового ряду** як відомі аномальні вікна, так і **точки/вікна, виявлені вашою моделлю**. Це дозволить візуально оцінити якість виявлення.\n",
    "    *   3.4. *Порівняння Алгоритмів:* Порівняйте результати (метрики та візуалізації) різних застосованих алгоритмів.\n",
    "    *   3.5. *Збереження Моделі (якщо застосовно):* Збережіть найкращу модель виявлення аномалій.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який для вибраного часового ряду NAB:\n",
    "*   Відображає **графік часового ряду**.\n",
    "*   Дає змогу вибрати та застосувати один або кілька **алгоритмів виявлення аномалій**.\n",
    "*   **Візуалізує виявлені аномалії** на графіку поруч з відомими аномальними вікнами.\n",
    "*   (Опціонально) Показує метрики оцінки (Precision, Recall, F1), якщо використовуються мітки NAB.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `matplotlib`/`plotly`, `sklearn`, `joblib`/`pickle`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте вибраний часовий ряд NAB та відповідні мітки аномалій. Можна завантажити попередньо навчену модель або навчати її на льоту (якщо швидко, наприклад, Isolation Forest).\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   Додайте можливість вибору часового ряду NAB (якщо їх декілька).\n",
    "        *   Додайте можливість вибору алгоритму виявлення аномалій (наприклад, `st.selectbox` / `gr.Dropdown`).\n",
    "        *   Додайте слайдери/поля для налаштування параметрів алгоритму (наприклад, `contamination`).\n",
    "    5.  *Оброблення та Запуск Алгоритму:*\n",
    "        *   Застосуйте вибраний алгоритм до часового ряду з вибраними параметрами.\n",
    "        *   Отримайте прогнозовані мітки аномалій.\n",
    "    6.  *Оцінка (опціонально):* Обчисліть Precision, Recall, F1, порівнюючи прогнози з мітками NAB.\n",
    "    7.  *Відображення результату:*\n",
    "        *   Побудуйте **інтерактивний графік часового ряду** (`plotly`).\n",
    "        *   **Виділіть на графіку** відомі аномальні вікна (наприклад, затіненням фону).\n",
    "        *   **Позначте точки/інтервали, виявлені моделлю** як аномальні (наприклад, іншим кольором або маркерами).\n",
    "        *   Виведіть обчислені метрики оцінки (якщо є).\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, файли даних NAB (.csv), файл міток (.json), модель (якщо є), `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `scipy` (для статистики), опціонально `pyod` (бібліотека для виявлення аномалій), `plotly` (для інтерактивних графіків).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `plotly`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Графіки часових рядів з виділеними справжніми/виявленими аномаліями** - це ключова візуалізація.\n",
    "*   **Метрики оцінки:** Precision, Recall, $F_1$-score для класу аномалій. Розгляньте **NAB scoring**, якщо знайдете реалізацію (просунуто).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   **Зрозумійте типи аномалій**, присутні у вибраному ряді (зміни рівня, піки, зміни волатильності).\n",
    "*   Попереднє Оброблення (згладжування, диференціювання) **може допомогти** деяким алгоритмам.\n",
    "*   **Налаштування параметрів** (особливо `contamination`) є ключовим для некерованих методів.\n",
    "*   **Оцінка виявлення аномалій у часових рядах потребує ретельного обмірковування**, особливо щодо обробки вікон та часткових збігів. Спрощена точкова оцінка може бути достатньою для початку.\n",
    "*   Бібліотека `pyod` містить багато алгоритмів виявлення аномалій.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 26: Інтелектуальна інформаційна система для прогнозування тяжкості ДТП\n",
    "\n",
    "**Набір даних:** US Accidents (2016-2021) ([https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents)). ***Дуже великий набір даних (~3GB)! Потребує значної вибірки*** (наприклад, за одним штатом, одним роком, або випадкової вибірки невеликого відсотка).\n",
    "\n",
    "**Мета:** **Прогнозувати тяжкість зіткнення** ('Severity', шкала 1-4, де 4 - найтяжча) на основі даних про ДТП, погоду, час, дорожні умови (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Вибірка Даних, Очищення та EDA:**\n",
    "    *   1.1. ***Вибірка та Завантаження:***\n",
    "        *   **Обов'язково зробіть вибірку** перед завантаженням або під час нього (`chunksize` + фільтрація/семплінг). Наприклад, відфільтруйте дані за одним штатом ('State') або одним роком.\n",
    "        *   Завантажте вибрані дані.\n",
    "    *   1.2. *Аналіз та Очищення:*\n",
    "        *   Огляньте велику кількість стовпців. Виберіть потенційно релевантні (наприклад, 'Severity', 'Start_Time', 'Start_Lat', 'Start_Lng', 'Distance(mi)', 'Street', 'City', 'County', 'State', 'Timezone', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight').\n",
    "        *   **Обробіть пропущені значення** у вибраних стовпцях (видалення, імпутація модою/середнім/медіаною, створення індикаторів).\n",
    "        *   **Оброблення Дати/Часу ('Start_Time'):** Розпарсіть datetime. Витягніть годину, день тижня, місяць.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('Severity'):*\n",
    "        *   Дослідіть **розподіл класів тяжкості**. Ймовірно, буде **дисбаланс** (більшість аварій матимуть низьку тяжкість, 1 або 2).\n",
    "    *   1.4. *EDA:*\n",
    "        *   Дослідіть взаємозв'язок між тяжкістю ('Severity') та різними факторами:\n",
    "            *   Час доби ('Sunrise_Sunset', година).\n",
    "            *   Погодні умови ('Weather_Condition', 'Temperature(F)', 'Visibility(mi)').\n",
    "            *   Дорожні умови (наявність 'Junction', 'Crossing', 'Traffic_Signal' тощо).\n",
    "            *   Місцезнаходження (візуалізація на карті, якщо можливо).\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Інженерія Ознак:*\n",
    "        *   Створіть бінарну ознаку 'Day/Night' на основі 'Sunrise_Sunset'.\n",
    "        *   Можливо, агрегуйте/спростіть категорії в 'Weather_Condition' або 'Wind_Direction'.\n",
    "    *   2.2. *Оброблення Категоріальних та Бінарних Ознак:*\n",
    "        *   Закодуйте категоріальні ознаки ('Weather_Condition', 'Wind_Direction', 'State', 'Timezone', 'Sunrise_Sunset') за допомогою One-Hot Encoding (обережно з кардинальністю).\n",
    "        *   Переконайтесь, що бінарні ознаки (True/False) перетворені на 0/1.\n",
    "    *   2.3. *Масштабування Числових Ознак:* Застосуйте `StandardScaler` до числових ознак (температура, вологість, видимість тощо).\n",
    "    *   2.4. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (з стратифікацією за 'Severity').\n",
    "    *   2.5. *Навчання базових багатокласових класифікаторів:* Навчіть Logistic Regression, Decision Tree. **Врахуйте дисбаланс класів** (`class_weight='balanced'`).\n",
    "    *   2.6. *Оцінка:* Оцініть моделі. Використовуйте Accuracy, **Weighted/Macro $F_1$-score**, **Classification Report** (з метриками для кожного класу тяжкості), матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Геопросторовий Аналіз:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, XGBoost, LightGBM. Ці моделі краще справляються зі складними взаємодіями та потенційним дисбалансом.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей, використовуючи крос-валідацію та відповідну метрику (наприклад, Weighted F1).\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх підходів.\n",
    "    *   3.4. ***Аналіз Важливості Ознак:*** Визначте, які **фактори (погодні, часові, дорожні, інфраструктурні)** найбільше впливають на тяжкість ДТП. Це може дати важливі інсайти для підвищення безпеки дорожнього руху.\n",
    "    *   3.5. *Геопросторовий Аналіз (Опціонально):*\n",
    "        *   Якщо використовували дані одного штату/міста, **візуалізуйте ДТП на карті**, забарвлюючи точки за фактичною або прогнозованою тяжкістю. Використовуйте `matplotlib`, `geopandas` або `folium`.\n",
    "        *   Проаналізуйте, чи існують \"гарячі точки\" з високою тяжкістю аварій.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та компоненти попереднього оброблення.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування тяжкості ДТП**. Застосунок приймає на вхід ключові характеристики аварії (час, погода, тип дороги тощо) та видає **прогноз рівня тяжкості** (1-4).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та пайплайн попереднього оброблення (скейлер, кодувальники, список ознак).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для **ключових ознак**: година, день тижня, температура, видимість, стан погоди (`st.selectbox`), наявність перехрестя, світлофора (`st.checkbox`) тощо. Спростіть інтерфейс, вибравши найважливіші предиктори.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте збережений пайплайн попереднього оброблення**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу тяжкості (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований рівень тяжкості (1-4) з коротким поясненням.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, опціонально `geopandas`/`folium` (для карт).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл класів 'Severity', **розподіли ознак для кожного класу тяжкості**, візуалізація на карті (опціонально), **діаграма важливості ознак**, матриця плутанини.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**weighted/macro та для кожного класу**), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Обов'язково виконайте вибірку даних*** через їх величезний розмір.\n",
    "*   Проєкт вимагає **ретельного очищення даних, обробки пропусків та інженерії ознак**.\n",
    "*   **Адресуйте дисбаланс класів 'Severity'** під час навчання та оцінки моделей.\n",
    "*   Аналіз **важливості ознак може надати цінні інсайти** для організацій, що займаються безпекою дорожнього руху.\n",
    "*   Геопросторовий аналіз може додати цікавий вимір до проєкту, але є опціональним.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 27: Інтелектуальна інформаційна система для класифікування музичних жанрів\n",
    "\n",
    "**Набір даних:** GTZAN Genre Collection (Версія з витягнутими ознаками: [https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)). Рекомендується використовувати CSV файли з ознаками (наприклад, `features_30_sec.csv` або `features_3_sec.csv`), оскільки обробка аудіофайлів може бути складною.\n",
    "\n",
    "**Мета:** **Класифікувати музичні кліпи** (30-секундні або 3-секундні уривки) за 10 жанрами (blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock), використовуючи **попередньо витягнуті аудіо ознаки** (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Попереднє Оброблення Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте вибраний CSV файл (наприклад, `features_30_sec.csv`) за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:*\n",
    "        *   Огляньте стовпці. Вони містять різноманітні аудіо ознаки (середні та дисперсії): 'chroma_stft', 'rmse', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate', MFCCs ('mfcc1'...'mfcc20').\n",
    "        *   Видаліть непотрібні стовпці ('filename', 'length').\n",
    "        *   Перевірте наявність пропусків.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('label'):*\n",
    "        *   Дослідіть розподіл 10 музичних жанрів. Чи збалансований набір даних?\n",
    "    *   1.4. *EDA:*\n",
    "        *   **Візуалізуйте розподіли ключових ознак** (наприклад, 'spectral_centroid', 'rolloff', кілька MFCCs) **для кожного жанру** за допомогою `boxplot` або `violinplot`. Це допоможе побачити, які ознаки розрізняють жанри.\n",
    "        *   Проаналізуйте кореляції між ознаками.\n",
    "    *   1.5. *Кодування та Масштабування:*\n",
    "        *   Закодуйте цільову змінну 'label' (назви жанрів) у числові мітки (`LabelEncoder`).\n",
    "        *   ***Застосуйте `StandardScaler`*** до всіх числових аудіо ознак.\n",
    "\n",
    "2.  **Базова Класифікація Жанрів:**\n",
    "    *   2.1. *Підготовка даних:* Розділіть масштабовані дані на тренувальну та тестову вибірки (з стратифікацією за 'label').\n",
    "    *   2.2. *Навчання базових багатокласових класифікаторів:* Навчіть K-NN, SVM (з лінійним та RBF ядром), Logistic Regression, Decision Tree.\n",
    "    *   2.3. *Оцінка:* Оцініть моделі на тестовій вибірці. Використовуйте:\n",
    "        *   Accuracy.\n",
    "        *   **Classification Report** (Precision, Recall, F1 для кожного жанру).\n",
    "        *   Macro/Weighted $F_1$-score.\n",
    "        *   **Матрицю плутанини** для аналізу помилок (які жанри модель плутає між собою?).\n",
    "\n",
    "3.  **Розширене Моделювання та Аналіз Ознак:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, XGBoost, LightGBM.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей (RF, XGBoost) за допомогою крос-валідації.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх моделей. Визначте найкращу.\n",
    "    *   3.4. ***Аналіз Важливості Ознак:*** Для найкращої моделі визначте, **які аудіо ознаки** (спектральні, ритмічні, MFCCs) є найбільш важливими для розрізнення музичних жанрів.\n",
    "    *   3.5. *Візуалізація Розділення Класів (Опціонально):* Застосуйте PCA або t-SNE до масштабованих ознак і візуалізуйте дані у 2D, забарвлюючи точки за жанрами. Оцініть, наскільки добре жанри розділяються в просторі ознак.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для класифікації музичних жанрів**. Застосунок може приймати на вхід:\n",
    "*   Або значення **попередньо витягнутих аудіо ознак**.\n",
    "*   Або (***просунутий варіант***) **аудіофайл (`.wav`)**, з якого застосунок самостійно витягує ознаки та робить прогноз.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`. Якщо обробляється аудіо, то `librosa`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, `LabelEncoder` (для перетворення прогнозованої мітки назад у назву жанру).\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   **Варіант 1 (Введення ознак):** Створіть поля введення (`st.number_input`) для ключових аудіо ознак (можливо, не всіх, а найважливіших).\n",
    "        *   **Варіант 2 (Завантаження аудіо - просунуто):** Додайте елемент для завантаження `.wav` файлу (`st.file_uploader` / `gr.Audio`).\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   (Варіант 1) Зберіть значення ознак. Сформуйте вектор. **Застосуйте збережений скейлер**.\n",
    "        *   (Варіант 2) Якщо аудіо завантажено:\n",
    "            *   Використайте `librosa` для завантаження файлу.\n",
    "            *   **Вилучіть ті самі аудіо ознаки**, що й у наборі даних (MFCCs, spectral centroid тощо).\n",
    "            *   **Агрегуйте ознаки** (середнє, std), щоб отримати вектор фіксованої довжини.\n",
    "            *   **Застосуйте збережений скейлер**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз числової мітки класу (`model.predict()`). **Перетворіть мітку назад у назву жанру** за допомогою збереженого `LabelEncoder` (`encoder.inverse_transform()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований музичний жанр.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки (для варіанту з аудіо можуть знадобитися дод. залежності типу `ffmpeg`).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, енкодер, `requirements.txt` (включно з `librosa`, якщо потрібно).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, опціонально `librosa` (якщо обробляється аудіо).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, опціонально `librosa`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Розподіли ознак для кожного жанру (boxplot)**, **матриця плутанини**, **діаграма важливості ознак**, візуалізація PCA/t-SNE.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**macro/weighted та для кожного жанру**), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   **Використання CSV файлу з готовими ознаками значно спрощує** проєкт порівняно з необхідністю самостійної обробки сирих `.wav` файлів.\n",
    "*   **Масштабування ознак** (`StandardScaler`) необхідне.\n",
    "*   **Аналізуйте матрицю плутанини**, щоб зрозуміти, які жанри є акустично подібними та складними для розрізнення моделлю (наприклад, rock/metal, classical/jazz).\n",
    "*   Якщо реалізуєте варіант із завантаженням аудіо, переконайтеся, що процес **вилучення та агрегації ознак у застосунку точно відповідає** тому, як були створені дані в CSV файлі.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 28: Інтелектуальна інформаційна система для виявлення хвороби Паркінсона за допомогою голосових ознак\n",
    "\n",
    "**Набір даних:** Parkinson's Disease Classification Dataset ([https://www.kaggle.com/datasets/nidaguler/parkinsons-data-set](https://www.kaggle.com/datasets/nidaguler/parkinsons-data-set) або з UCI Machine Learning Repository: [https://archive.ics.uci.edu/ml/datasets/Parkinsons](https://archive.ics.uci.edu/ml/datasets/Parkinsons))\n",
    "\n",
    "**Мета:** **Виявити наявність хвороби Паркінсона** ('status', 1 - хворий, 0 - здоровий) на основі набору різноманітних вимірювань голосу (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження Даних та Аналіз Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте дані (зазвичай CSV файл) за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:*\n",
    "        *   Огляньте стовпці. Вони включають різні вимірювання частоти голосу (MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Flo(Hz)), варіабельності амплітуди та частоти (Jitter, Shimmer), співвідношення шум/гармоніки (NHR, HNR), нелінійні міри (RPDE, DFA, spread1, spread2, PPE) та інші.\n",
    "        *   Видаліть стовпець 'name', якщо він є.\n",
    "        *   Перевірте типи даних та наявність пропусків (зазвичай немає).\n",
    "    *   1.3. *Аналіз Цільової Змінної ('status'):*\n",
    "        *   Дослідіть **розподіл класів** (хворі vs здорові). Часто в таких медичних наборах даних є **дисбаланс**.\n",
    "    *   1.4. *EDA:*\n",
    "        *   **Візуалізуйте відмінності** в розподілах голосових ознак між здоровими (status=0) та хворими (status=1) групами за допомогою `boxplot` або `violinplot`.\n",
    "        *   Проаналізуйте **кореляції** між ознаками за допомогою теплової карти. Зверніть увагу на можливу мультиколінеарність (сильні кореляції між предикторами).\n",
    "\n",
    "2.  **Масштабування Ознак та Базове Моделювання:**\n",
    "    *   2.1. ***Масштабування ознак:*** Оскільки ознаки мають дуже різні шкали та одиниці вимірювання, **застосування `StandardScaler` є критично важливим**.\n",
    "    *   2.2. *Підготовка даних:* Розділіть дані на ознаки та цільову змінну 'status'. Розділіть на тренувальну та тестову вибірки, використовуючи **стратифікацію** за 'status'.\n",
    "    *   2.3. *Навчання базових класифікаторів:* Навчіть Logistic Regression, SVM (з різними ядрами), K-NN. Враховуйте дисбаланс (`class_weight='balanced'`).\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на тестовій вибірці. Через медичний контекст та потенційний дисбаланс, **особливу увагу приділіть Recall для позитивного класу (status=1)**, щоб мінімізувати пропуск хворих пацієнтів. Також використовуйте Accuracy, Precision (для status=1), $F_1$-score (для status=1), AUC-ROC. Використовуйте **стратифіковану крос-валідацію** на тренувальних даних для більш надійної оцінки під час вибору моделі.\n",
    "\n",
    "3.  **Розширене Моделювання та Вибір Ознак:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, XGBoost.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей, можливо, максимізуючи Recall для класу 1 або AUC-ROC.\n",
    "    *   3.3. *Вибір Ознак (Опціонально):* Через можливу мультиколінеарність та велику кількість ознак, розгляньте застосування методів вибору ознак:\n",
    "        *   На основі важливості з Random Forest/XGBoost.\n",
    "        *   Рекурсивне усунення ознак (`RFECV` з `sklearn.feature_selection`).\n",
    "        *   На основі Lasso регресії.\n",
    "        *   Порівняйте продуктивність моделі з повним набором ознак та зі зменшеним.\n",
    "    *   3.4. *Порівняння Моделей:* Порівняйте продуктивність усіх моделей (з/без вибору ознак).\n",
    "    *   3.5. *Аналіз важливості ознак:* Визначте, які саме вимірювання голосу є найбільш інформативними для діагностики хвороби Паркінсона.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування статусу хвороби Паркінсона**. Застосунок приймає на вхід значення голосових вимірювань та видає **прогноз** (Хворий/Здоровий) на основі навченої моделі.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost` (якщо використовується).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input` / `gr.Number`) для всіх (або ключових, якщо використовувався вибір ознак) голосових ознак, що використовуються моделлю. Надайте користувачеві інформацію про діапазони значень або типові значення, якщо можливо.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Сформуйте вектор ознак. **Застосуйте збережений скейлер `StandardScaler`**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`) та, можливо, ймовірності (`model.predict_proba()`).\n",
    "    7.  *Відображення результату:* Покажіть прогноз: \"Прогнозований статус: Хворий\" або \"Прогнозований статус: Здоровий\". Можна додати ймовірність.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Розподіли ознак для кожного статусу (boxplot/violinplot)**, кореляційна теплова карта, **діаграма важливості ознак**, матриця плутанини, ROC-крива.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, ***Recall (для status=1)***, **$F_1$-score (для status=1)**, AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Масштабування ознак (`StandardScaler`) є абсолютно вирішальним*** через різнорідність вимірювань.\n",
    "*   Набір даних відносно **невеликий**, тому **крос-валідація** (стратифікована) є важливою для надійної оцінки та налаштування гіперпараметрів.\n",
    "*   **Зосередьтеся на показнику Recall для позитивного класу (status=1)**, оскільки в медичних застосуваннях пропуск хворого пацієнта (False Negative) зазвичай є більш критичною помилкою, ніж хибна тривога (False Positive).\n",
    "*   Розгляньте вибір ознак для потенційного покращення моделі та зменшення ризику перенавчання.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 29: Аналіз та сегментація особистості клієнтів\n",
    "\n",
    "**Набір даних:** Customer Personality Analysis ([https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis))\n",
    "\n",
    "**Мета:** **Сегментувати клієнтів** на основі їхніх демографічних даних, купівельної поведінки та реакції на маркетингові кампанії, використовуючи методи **кластеризації**.\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та Інженерія Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `marketing_campaign.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз та Очищення:*\n",
    "        *   Перевірте типи даних, основні статистики.\n",
    "        *   **Обробіть пропущені значення** у стовпці 'Income' (наприклад, імпутацією медіаною).\n",
    "        *   Перетворіть 'Dt_Customer' на datetime об'єкт.\n",
    "    *   1.3. ***Інженерія Ознак:*** Це важливий крок для отримання змістовних сегментів.\n",
    "        *   *Демографічні:* Створіть ознаку **'Age'** з 'Year\\_Birth'. Розрахуйте **'Customer\\_For'** (тривалість перебування клієнтом) з 'Dt\\_Customer'. Створіть ознаку **'Living\\_With'** (один/з партнером) на основі 'Marital\\_Status'. Створіть ознаку **'Has\\_Child'** або 'Num\\_Children' на основі 'Kidhome', 'Teenhome'.\n",
    "        *   *Витрати:* Створіть **'TotalSpent'** (сума всіх витрат `MntWines`, `MntFruits`, ...).\n",
    "        *   *Покупки:* Створіть **'NumTotalPurchases'** (сума покупок через різні канали `NumWebPurchases`, `NumCatalogPurchases`, ...).\n",
    "        *   *Кампанії:* Створіть **'NumCampaignAcc'** (кількість прийнятих кампаній `AcceptedCmp1`...`AcceptedCmp5` + `Response`). Можливо, **'CampaignAcceptRate'**.\n",
    "        *   Видаліть оригінальні стовпці, що використовувалися для створення нових, якщо вони більше не потрібні.\n",
    "    *   1.4. *Оброблення Категоріальних Ознак:* Закодуйте 'Education', 'Marital\\_Status' (або створені з них 'Living\\_With').\n",
    "    *   1.5. *EDA:* Дослідіть розподіли створених ознак ('Age', 'TotalSpent', 'NumTotalPurchases'). Проаналізуйте кореляції між ними.\n",
    "\n",
    "2.  **Зниження Розмірності та Кластеризація:**\n",
    "    *   2.1. *Вибір та Масштабування Ознак:*\n",
    "        *   Виберіть **набір ознак**, які найкраще відображають різні аспекти поведінки та демографії для кластеризації (наприклад, 'Age', 'Income', 'TotalSpent', 'NumTotalPurchases', 'NumCampaignAcc', 'Has_Child').\n",
    "        *   ***Застосуйте `StandardScaler`*** до вибраних числових ознак.\n",
    "    *   2.2. *Зниження Розмірності (для візуалізації):*\n",
    "        *   Застосуйте **PCA** до масштабованих ознак.\n",
    "        *   Проаналізуйте пояснену дисперсію. Виберіть перші 2-3 компоненти для візуалізації.\n",
    "    *   2.3. *Застосування Алгоритмів Кластеризації:*\n",
    "        *   **K-Means:**\n",
    "            *   Визначте **оптимальну кількість кластерів (k)** за допомогою методу ліктя та силуетного аналізу на масштабованих даних.\n",
    "            *   Застосуйте K-Means з оптимальним k.\n",
    "        *   **DBSCAN (Опціонально):** Застосуйте DBSCAN для пошуку кластерів довільної форми та виявлення викидів.\n",
    "        *   **Agglomerative Clustering (Ієрархічна кластеризація) (Опціонально):** Побудуйте дендрограму для візуалізації ієрархії кластерів та вибору кількості кластерів.\n",
    "    *   2.4. *Призначення Міток:* Додайте стовпець з мітками кластерів до вихідного (або обробленого) DataFrame.\n",
    "\n",
    "3.  **Профілювання та Інтерпретація Кластерів:**\n",
    "    *   3.1. ***Аналіз Характеристик Кластерів:*** Це **найважливіший етап**.\n",
    "        *   Для кожного кластера, отриманого за допомогою K-Means (або іншого вибраного алгоритму), обчисліть **середні значення (або медіани/моди)** для ключових вихідних та створених ознак ('Age', 'Income', 'TotalSpent', 'NumTotalPurchases', 'NumCampaignAcc', частки за освітою/сімейним станом тощо).\n",
    "        *   **Порівняйте ці середні значення між кластерами**.\n",
    "    *   3.2. *Візуалізація Профілів:*\n",
    "        *   Використовуйте стовпчасті діаграми, радарні діаграми або таблиці для візуального порівняння характеристик кластерів.\n",
    "        *   Візуалізуйте **розподіл кластерів у просторі перших двох PCA компонент**.\n",
    "    *   3.3. ***Інтерпретація та Опис Сегментів:***\n",
    "        *   На основі аналізу характеристик, **дайте кожному кластеру змістовну назву та детальний опис**. Наприклад: \"Заможні лояльні клієнти середнього віку\", \"Молоді економні клієнти\", \"Клієнти, що реагують на кампанії\" тощо.\n",
    "        *   **Сфокусуйтеся на відмінностях** між кластерами, які можуть бути корисними для маркетингових стратегій.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Розроблено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який:\n",
    "*   Візуалізує **розподіл клієнтів за кластерами** (наприклад, на 2D PCA графіку).\n",
    "*   Представляє **детальні профілі та описи** для кожного виявленого сегмента клієнтів.\n",
    "*   (Опціонально) Дає змогу інтерактивно досліджувати характеристики кластерів.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit`:** (Добре підходить для візуалізації та інтерактивності)\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`, `pandas`, `numpy`, `matplotlib`/`seaborn`/`plotly`, `sklearn`, `joblib`/`pickle`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте оброблені дані з мітками кластерів, PCA модель (якщо використовується для візуалізації), збережені профілі кластерів (наприклад, DataFrame з середніми значеннями).\n",
    "    4.  *Створення інтерфейсу та візуалізацій:*\n",
    "        *   Відобразіть **PCA графік** (`st.scatter_chart` або `st.plotly_chart`), забарвлений за кластерами.\n",
    "        *   Використовуйте вкладки (`st.tabs`) або випадаючий список (`st.selectbox`) для вибору кластера.\n",
    "        *   Для вибраного кластера відобразіть:\n",
    "            *   Його **назву та опис**.\n",
    "            *   **Ключові характеристики** (середні значення ознак) у вигляді таблиці (`st.dataframe`) або графіків (`st.bar_chart`).\n",
    "        *   (Опціонально) Додайте фільтри або інтерактивні елементи для дослідження даних.\n",
    "    5.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `gradio` / `replit`:**\n",
    "    *   `Gradio` менше підходить для таких візуально-орієнтованих дашбордів, але може відображати графіки та таблиці.\n",
    "    *   `Replit` може хостити `streamlit` застосунок.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `scipy` (для ієрархічної кластеризації, якщо використовується).\n",
    "*   **Розгортання:** `streamlit` (рекомендовано), `plotly` (для інтерактивних графіків), `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Графіки Elbow/Silhouette, **PCA scatter plot (забарвлений за кластером)**, **графіки/таблиці профілів кластерів (середні значення ознак)**, радарні діаграми для порівняння кластерів.\n",
    "*   **Метрики оцінки:** Silhouette Score, Davies-Bouldin Index. ***Основний акцент - на інтерпретованості та бізнес-цінності профілів кластерів***.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Інженерія ознак є ключовою*** для створення інформативних вхідних даних для кластеризації.\n",
    "*   **Масштабування ознак є необхідним** для K-Means та інших алгоритмів на основі відстані.\n",
    "*   Порівняйте результати різних алгоритмів кластеризації (K-Means, Agglomerative, DBSCAN), якщо є час.\n",
    "*   **Інтерпретація та опис сегментів є головною метою цього проєкту**. Сегменти повинні бути зрозумілими, відмінними та потенційно корисними для маркетингу.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 30: Інтелектуальна інформаційна система для прогнозування типу лісового покриву\n",
    "\n",
    "**Набір даних:** Forest Cover Type Dataset ([https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset](https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset)) - `covtype.csv`. **Великий набір даних** (>580 тис. записів, 54 ознаки).\n",
    "\n",
    "**Мета:** **Прогнозувати тип лісового покриву** ('Cover\\_Type', 7 типів дерев) на основі картографічних даних (висота, нахил, відстань до води тощо) та бінарних індикаторів типу дикої місцевості та типу ґрунту (_Багатокласова класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження Даних та Вибірка (якщо необхідно):**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `covtype.csv` за допомогою `pandas`. Через розмір, це може зайняти деякий час.\n",
    "    *   1.2. *Аналіз даних:*\n",
    "        *   Огляньте стовпці. Перші 10 - числові картографічні ознаки. Наступні 4 - бінарні індикатори типу дикої місцевості (Wilderness\\_Area). Останні 40 - бінарні індикатори типу ґрунту (Soil\\_Type).\n",
    "        *   Перевірте типи даних, наявність пропусків (зазвичай немає).\n",
    "    *   1.3. *Аналіз Цільової Змінної ('Cover\\_Type'):*\n",
    "        *   Дослідіть **розподіл 7 типів лісового покриву**. Перевірте на **дисбаланс класів**. Деякі типи можуть бути представлені значно менше, ніж інші.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Візуалізуйте розподіли ключових **числових картографічних ознак** ('Elevation', 'Aspect', 'Slope', 'Horizontal\\_Distance\\_To\\_Hydrology', 'Vertical\\_Distance\\_To\\_Hydrology', 'Horizontal\\_Distance\\_To\\_Roadways', 'Hillshade\\_9am', 'Hillshade\\_Noon', 'Hillshade\\_3pm', 'Horizontal\\_Distance\\_To\\_Fire\\_Points') **для кожного типу покриву** (`boxplot`, `violinplot`).\n",
    "        *   Проаналізуйте кореляції між числовими ознаками.\n",
    "        *   Дослідіть зв'язок між бінарними ознаками (тип дикої місцевості, тип ґрунту) та типом покриву (наприклад, за допомогою групованих стовпчастих діаграм).\n",
    "    *   1.5. *Вибірка (Опціонально, для прискорення):* Якщо обчислення займають занадто багато часу, розгляньте можливість роботи з **вибіркою даних** (наприклад, 10-20%) для початкового моделювання та налаштування гіперпараметрів, але фінальне навчання/оцінку краще проводити на повних даних, якщо ресурси дозволяють.\n",
    "\n",
    "2.  **Попереднє Оброблення та Базове Моделювання:**\n",
    "    *   2.1. *Розділення ознак:* Явно розділіть дані на числові картографічні ознаки та бінарні індикатори. Цільова змінна - 'Cover\\_Type'.\n",
    "    *   2.2. *Масштабування ознак:* ***Застосуйте `StandardScaler`*** лише до **числових картографічних ознак**. Бінарні ознаки (0/1) зазвичай не потребують масштабування.\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (з стратифікацією за 'Cover\\_Type').\n",
    "    *   2.4. *Навчання базових багатокласових класифікаторів:* Навчіть (на повних даних або вибірці):\n",
    "        *   Logistic Regression (може бути повільним через велику кількість даних/ознак).\n",
    "        *   K-Nearest Neighbors (також може бути повільним на великих даних).\n",
    "        *   Decision Tree.\n",
    "        *   Gaussian Naive Bayes (швидкий варіант).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на тестовій вибірці. Використовуйте Accuracy, **Classification Report** (з Precision, Recall, F1 для кожного типу покриву), Macro/Weighted $F_1$-score, матрицю плутанини. Зверніть увагу на продуктивність для менш представлених класів.\n",
    "\n",
    "3.  **Розширене Моделювання та Оптимізація Продуктивності:**\n",
    "    *   3.1. *Реалізація Ефективних Класифікаторів:* Через великий розмір даних, зосередьтеся на **ефективних моделях**:\n",
    "        *   **Random Forest** (може бути обчислювально інтенсивним, але часто дає хороші результати).\n",
    "        *   **LightGBM** (`lightgbm.LGBMClassifier`) - **рекомендовано**, зазвичай найшвидший та ефективний на великих табличних даних.\n",
    "        *   **XGBoost** (`xgboost.XGBClassifier`) - також потужний варіант.\n",
    "        *   Розгляньте використання нейронних мереж (MLPClassifier або Keras/TF/PyTorch), якщо є досвід.\n",
    "    *   3.2. *Оптимізація Гіперпараметрів:* **Налаштуйте гіперпараметри** для найкращих моделей (LightGBM/XGBoost) за допомогою крос-валідації (використовуйте `StratifiedKFold`). Через розмір даних, `RandomizedSearchCV` може бути швидшим за `GridSearchCV`.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх моделей, звертаючи увагу як на точність, так і на **час навчання/прогнозування**.\n",
    "    *   3.4. *Аналіз Важливості Ознак:* Для моделей на основі дерев (RF, LGBM, XGBoost) визначте, які **картографічні, ґрунтові або пов'язані з дикою природою ознаки** є найбільш важливими для визначення типу лісового покриву.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу та найефективнішу модель (ймовірно, LightGBM або XGBoost) та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування типу лісового покриву**. Застосунок приймає на вхід картографічні дані та інформацію про тип місцевості/ґрунту та видає **прогноз одного з 7 типів покриву**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `lightgbm`/`xgboost`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та скейлер. Можливо, знадобиться список назв ґрунтів/місцевостей для інтерфейсу.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input`, `st.slider` / `gr.Number`, `gr.Slider`) для **числових картографічних ознак**. Створіть випадаючі списки (`st.selectbox` / `gr.Dropdown`) або радіокнопки (`st.radio` / `gr.Radio`) для вибору **одного** типу дикої місцевості (Wilderness Area) та **одного** типу ґрунту (Soil Type) - це відповідає тому, як закодовані бінарні ознаки.\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Зберіть дані користувача.\n",
    "        *   **Застосуйте збережений скейлер** до числових картографічних ознак.\n",
    "        *   **Створіть бінарні ознаки** для вибраного типу дикої місцевості та типу ґрунту (встановивши відповідні стовпці в 1, а інші - в 0).\n",
    "        *   Сформуйте повний вектор ознак у правильному порядку.\n",
    "    6.  *Прогнозування:* Зробіть прогноз числової мітки класу (`model.predict()`). Перетворіть мітку на назву типу покриву (потрібен мапінг 1-7 на назви).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований тип лісового покриву.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, мапінг типів покриву, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (рекомендовано), опціонально `keras`/`tensorflow`/`pytorch`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл типів покриву, **розподіли ознак для кожного типу**, матриця плутанини, **діаграма важливості ознак**.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score (**macro/weighted та для кожного класу**), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Ефективно обробляйте великий розмір даних***: використовуйте вибірку для розробки, застосовуйте ефективні моделі (LightGBM), оптимізуйте використання пам'яті.\n",
    "*   **Масштабування потрібне лише для числових картографічних ознак**, бінарні індикатори залишайте як є.\n",
    "*   **Зосередьтеся на оптимізації продуктивності** як з точки зору точності (особливо для менших класів), так і швидкості навчання/прогнозування.\n",
    "*   Аналіз важливості ознак покаже, чи є домінуючими картографічні дані (як 'Elevation'), чи тип ґрунту/місцевості також відіграє значну роль.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 31: Інтелектуальна інформаційна система для прогнозування затримок авіарейсів\n",
    "\n",
    "**Набір даних:** Airline Delay and Cancellation Data (2009-2018) ([https://www.kaggle.com/datasets/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018](https://www.kaggle.com/datasets/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018)). ***Дуже великий набір даних! Потребує значної вибірки*** (наприклад, один рік, або дані для кількох великих аеропортів/авіаліній).\n",
    "\n",
    "**Мета:** **Прогнозувати, чи буде рейс значно затриманий** (наприклад, затримка прибуття `ARR_DELAY` > 15 хвилин) на основі інформації про рейс (авіалінія, аеропорти, час відправлення), погодні дані (якщо доступні, в цьому наборі їх немає) тощо (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Вибірка Даних, Очищення та EDA:**\n",
    "    *   1.1. ***Вибірка та Завантаження:***\n",
    "        *   **Обов'язково зробіть вибірку.** Наприклад, завантажте дані за один рік (наприклад, 2018) або відфільтруйте дані для топ-10 аеропортів або авіаліній.\n",
    "        *   Завантажте вибрані дані (`.csv` файли для кожного року) і об'єднайте їх.\n",
    "    *   1.2. *Вибір Ознак та Очищення:*\n",
    "        *   Виберіть релевантні стовпці: 'FL_DATE', 'OP_CARRIER' (авіалінія), 'ORIGIN', 'DEST' (аеропорти), 'CRS_DEP_TIME' (плановий час відправлення), 'DEP_TIME' (факт. час відправлення), 'DEP_DELAY' (затримка відправлення), 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY' (затримка прибуття - **основна для прогнозування**), 'CANCELLED', 'DIVERTED', 'AIR_TIME', 'DISTANCE'.\n",
    "        *   **Обробіть пропущені значення:** Особливо важливо для `ARR_DELAY`, `DEP_DELAY`, `AIR_TIME`. Часто пропуски пов'язані зі скасованими (`CANCELLED=1`) або перенаправленими (`DIVERTED=1`) рейсами. Вирішіть, як їх обробляти (наприклад, видалити скасовані/перенаправлені рейси або імпутувати затримки).\n",
    "    *   1.3. ***Створення Цільової Змінної:*** Створіть бінарну змінну `is_delayed` (наприклад, 1, якщо `ARR_DELAY > 15`, інакше 0).\n",
    "    *   1.4. *Інженерія Часових Ознак:*\n",
    "        *   Розпарсіть 'FL_DATE' та 'CRS_DEP_TIME'/'CRS_ARR_TIME'.\n",
    "        *   Витягніть: **місяць, день тижня, година відправлення**.\n",
    "    *   1.5. *EDA:*\n",
    "        *   Дослідіть **розподіл затримок** (`ARR_DELAY`).\n",
    "        *   Проаналізуйте **рівень затримок** (відсоток `is_delayed=1`) та перевірте на **дисбаланс класів**.\n",
    "        *   Візуалізуйте залежність затримок від: авіалінії ('OP_CARRIER'), аеропорту відправлення/призначення ('ORIGIN', 'DEST'), години дня, дня тижня, місяця.\n",
    "        *   Дослідіть кореляцію між `DEP_DELAY` та `ARR_DELAY`.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Оброблення Категоріальних Ознак:* Закодуйте 'OP_CARRIER', 'ORIGIN', 'DEST' за допомогою One-Hot Encoding або Target Encoding (враховуйте високу кардинальність аеропортів). Можливо, варто обмежитися найпопулярнішими аеропортами/авіалініями.\n",
    "    *   2.2. *Масштабування Числових Ознак:* Застосуйте `StandardScaler` до числових ознак ('DISTANCE', 'CRS_DEP_TIME' (якщо розглядається як число), 'DEP_DELAY' (якщо використовується як ```ukrainian\n",
    "ознака), 'AIR_TIME').\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (з стратифікацією за `is_delayed`).\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, Decision Tree. **Врахуйте дисбаланс класів** (`class_weight='balanced'`).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на тестовій вибірці. Через дисбаланс, зосередьтеся на **Precision, Recall, $F_1$-score для класу затриманих рейсів (is_delayed=1)**, а також на **AUC-ROC** та **AUC-PR**. Проаналізуйте матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Аналіз Факторів Затримки:**\n",
    "    *   3.1. *Реалізація Ефективних Класифікаторів:* Навчіть **ефективні моделі**, які добре працюють з великими та потенційно незбалансованими даними: **LightGBM**, **XGBoost**, Random Forest.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей за допомогою крос-валідації (стратифікованої), максимізуючи AUC-PR або F1 для класу затриманих рейсів.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх підходів.\n",
    "    *   3.4. ***Аналіз Важливості Ознак:*** Визначте **ключові фактори, що впливають на затримки рейсів**. Чи це час доби, день тижня, конкретна авіалінія, аеропорт відправлення/призначення, чи затримка відправлення (`DEP_DELAY`, якщо використовується)? *Примітка:* Використання `DEP_DELAY` як ознаки може значно покращити прогноз `ARR_DELAY`, але може бути недоступним на момент прогнозування до відльоту. Вирішіть, чи включати цю ознаку залежно від постановки задачі.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та компоненти попереднього оброблення (скейлер, кодувальники).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування ймовірності значної затримки авіарейсу**. Застосунок дає змогу користувачеві ввести інформацію про рейс (авіалінія, аеропорти, час) та отримати **прогноз ризику затримки**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `lightgbm`/`xgboost`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, кодувальник(и) (або списки авіаліній/аеропортів, якщо використовувалось обмеження категорій).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення: `st.selectbox` для авіалінії, аеропортів відправлення/призначення; `st.time_input` або `st.slider` для планового часу відправлення; `st.date_input` для дати (щоб визначити місяць/день тижня). Враховуйте, що введення аеропортів може бути складним через їх кількість - можливо, варто обмежитись списком найпопулярніших.\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Зберіть дані.\n",
    "        *   **Витягніть часові ознаки** (місяць, день тижня, година).\n",
    "        *   **Застосуйте ту саму попередню обробку**: кодування категоріальних, масштабування числових (наприклад, відстань, якщо вона потрібна моделі і користувач її вводить або вона визначається за аеропортами). Переконайтесь, що всі ознаки моделі присутні у правильному порядку.\n",
    "    6.  *Прогнозування:* Отримайте ймовірність затримки (`model.predict_proba()[:, 1]`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану ймовірність затримки та/або текстовий висновок (\"Високий/Низький ризик затримки > 15 хв\").\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, кодувальники/списки, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (рекомендовано)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл затримок, **рівень затримок за авіалініями/аеропортами/часом доби/днем тижня**, **діаграма важливості ознак**, ROC-крива, **PR-крива**, матриця плутанини.\n",
    "*   **Метрики оцінки:** Accuracy (з обережністю), Precision, Recall, $F_1$-score (**для класу затриманих**), AUC-ROC, ***AUC-PR (важливо через дисбаланс)***.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Стратегія вибірки даних є дуже важливою*** через величезний розмір оригінального набору.\n",
    "*   **Інженерія часових ознак** (година дня, день тижня, місяць) є ключовою.\n",
    "*   **Ретельно обробіть категоріальні ознаки** з високою кардинальністю (аеропорти, авіалінії) - можливо, знадобиться обмеження кількості категорій або використання Target Encoding.\n",
    "*   **Адресуйте дисбаланс класів** під час навчання та оцінки.\n",
    "*   Аналіз важливості ознак допоможе зрозуміти основні причини затримок в авіації (в межах доступних даних).\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 32: Інтелектуальна інформаційна система для прогнозування споживання електроенергії\n",
    "\n",
    "**Набір даних:** Individual household electric power consumption ([https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption)). Дані містять вимірювання з інтервалом в одну хвилину за майже 4 роки. Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set](https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set))\n",
    "\n",
    "**Мета:** **Прогнозувати споживання електроенергії домогосподарством** (наприклад, 'Global\\_active\\_power') на майбутній період (наприклад, наступний день або тиждень) на основі історичних даних (_Прогнозування часових рядів_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних, Очищення та Ресемплінг:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `household_power_consumption.txt` (роздільник - ';'). **Увага:** дані великі.\n",
    "    *   1.2. *Парсинг Дат та Часу:* Об'єднайте стовпці 'Date' та 'Time' і перетворіть їх на **datetime індекс**.\n",
    "    *   1.3. *Оброблення Пропусків та Типів:*\n",
    "        *   Пропущені значення позначені як '?'. Замініть їх на `NaN`.\n",
    "        *   Перетворіть всі числові стовпці (Global\\_active\\_power, Global\\_reactive\\_power, Voltage, Global\\_intensity, Sub\\_metering\\_1/2/3) на числовий тип (`float`).\n",
    "        *   Обробіть пропущені значення (`NaN`). Через високу частоту даних, **імпутація може бути складною**. Прості варіанти: заповнення попереднім значенням (`ffill`), інтерполяція. Складніші: використання даних за попередні дні/тижні. *Вибір методу імпутації вплине на результати.*\n",
    "    *   1.4. ***Ресемплінг:*** Хвилинні дані є занадто детальними для багатьох моделей та можуть містити багато шуму. **Зробіть ресемплінг** даних до нижчої частоти, наприклад:\n",
    "        *   **Годинної:** `df.resample('H').mean()` (або `.sum()` для потужності, залежно від мети).\n",
    "        *   **Денної:** `df.resample('D').mean()` (або `.sum()`).\n",
    "        *   *Вибір частоти ресемплінгу впливає на завдання та моделі.* Робота з денними даними простіша.\n",
    "    *   1.5. *EDA Часових Рядів (на ресемпльованих даних):*\n",
    "        *   Візуалізуйте ресемпльований часовий ряд 'Global\\_active\\_power'.\n",
    "        *   Дослідіть **сезонні патерни** (річні, тижневі, добові - залежно від частоти ресемплінгу) за допомогою графіків декомпозиції (`seasonal_decompose` з `statsmodels.tsa.seasonal`) та автокореляційних функцій (ACF/PACF).\n",
    "        *   Перевірте стаціонарність ресемпльованого ряду.\n",
    "\n",
    "2.  **Інженерія Ознак та Статистичне Прогнозування:**\n",
    "    *   2.1. *Інженерія Часових Ознак (на ресемпльованих даних):*\n",
    "        *   Створіть ознаки на основі дати/часу (година дня, день тижня, місяць, рік - залежно від частоти).\n",
    "        *   Створіть **лагові ознаки** (значення 'Global\\_active\\_power' за попередні періоди - години/дні).\n",
    "        *   Створіть **ознаки ковзного вікна** (середнє, std тощо за останні N періодів).\n",
    "    *   2.2. *Застосування Статистичних Моделей:*\n",
    "        *   Навчіть моделі ARIMA/SARIMA або ETS (Holt-Winters) на ресемпльованому часовому ряді 'Global\\_active\\_power'. Використовуйте ACF/PACF для вибору порядків моделі. SARIMA може бути доречною, якщо є чітка сезонність (наприклад, тижнева або річна на денних даних).\n",
    "    *   2.3. *Оцінка:* Розділіть ресемпльовані дані на тренувальну та валідаційну вибірки (часовий поділ). Оцініть статистичні моделі за допомогою MAE, MSE, RMSE, MAPE.\n",
    "\n",
    "3.  **Прогнозування за допомогою Машинного Навчання та Розгортання:**\n",
    "    *   3.1. *Підготовка Даних для ML:* Створіть табличний набір даних на основі ресемпльованих даних, де кожен рядок відповідає певному часовому періоду (година/день), а стовпці містять створені часові/лагові/ковзні ознаки та цільову змінну 'Global\\_active\\_power'.\n",
    "    *   3.2. *Застосування ML Моделей:* Навчіть регресійні моделі (Random Forest, XGBoost, LightGBM) на цих табличних даних.\n",
    "    *   3.3. *Застосування LSTM (Опціонально):* Побудуйте та навчіть LSTM модель, використовуючи послідовності минулих значень споживання для прогнозування майбутніх.\n",
    "    *   3.4. *Порівняння Продуктивності:* ***Порівняйте*** результати статистичних моделей та ML/DL моделей на валідаційній вибірці.\n",
    "    *   3.5. *Аналіз важливості ознак (для ML):* Визначте, які лаги або часові характеристики найбільше впливають на прогноз споживання.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки (наприклад, скейлер, якщо використовувався).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який:\n",
    "*   Відображає **історичний графік** ресемпльованого споживання електроенергії ('Global\\_active\\_power').\n",
    "*   Показує **прогноз споживання** на наступний період (наприклад, день або кілька годин), згенерований найкращою моделлю.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `matplotlib`/`plotly`, `joblib`/`pickle`, `statsmodels`/`xgboost`/`keras` тощо.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер (якщо є), та, можливо, останні історичні дані, необхідні для генерації ознак.\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   Відобразіть заголовок.\n",
    "        *   Дозвольте користувачеві вибрати горизонт прогнозування (кількість годин/днів).\n",
    "    5.  *Генерація ознак та прогнозування:*\n",
    "        *   Отримайте останні доступні історичні дані.\n",
    "        *   **Згенеруйте необхідні лагові/часові/ковзні ознаки** для майбутніх періодів часу.\n",
    "        *   **Застосуйте скейлер** до ознак, якщо модель навчалася на масштабованих даних.\n",
    "        *   Зробіть прогноз за допомогою завантаженої моделі.\n",
    "        *   **Застосуйте обернене масштабування** до прогнозу, якщо потрібно.\n",
    "    6.  *Відображення результату:*\n",
    "        *   Побудуйте **графік**, що показує останні історичні дані та прогнозні значення (`st.line_chart`, `st.plotly_chart` / `gr.Plot`).\n",
    "        *   Виведіть прогнозні значення у вигляді таблиці або тексту.\n",
    "    7.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, можливо, зразок історичних даних, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `statsmodels`, `xgboost`, `lightgbm`, опціонально `keras`/`tensorflow`/`pytorch`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `plotly`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Графіки часових рядів (оригінальні, ресемпльовані), графіки декомпозиції, ACF/PACF, **графік прогнозу проти фактичних значень**.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, MAPE.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Ретельне очищення даних, обробка пропусків ('?') та ресемплінг є критично важливими*** першими кроками. Вибір частоти ресемплінгу (година/день) суттєво вплине на подальші кроки.\n",
    "*   Інженерія ознак (лаги, ковзні вікна, часові характеристики) є ключовою для успіху ML моделей.\n",
    "*   Порівняйте продуктивність класичних статистичних моделей (SARIMA/ETS) з ML/DL підходами.\n",
    "*   Зверніть увагу на правильну обробку часових індексів та часовий поділ даних для валідації.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 33: Інтелектуальна інформаційна система для класифікування токсичних коментарів\n",
    "\n",
    "**Набір даних:** Toxic Comment Classification Challenge ([https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)) - _Потрібен акаунт Kaggle для доступу до `train.csv`, `test.csv`, `test_labels.csv`, `sample_submission.csv`_.\n",
    "\n",
    "**Мета:** **Класифікувати онлайн-коментарі** за кількома категоріями токсичності (toxic, severe\\_toxic, obscene, threat, insult, identity\\_hate). Оскільки один коментар може належати до кількох категорій одночасно, це завдання є ***багатозначною класифікацією тексту (Multi-label Text Classification)***.\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження та Попереднє Оброблення Текстових Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `train.csv`, `test.csv`, `test_labels.csv`. Файл `test_labels.csv` містить мітки для тестового набору (деякі мають значення -1, що означає, що вони не використовувалися для оцінки у змаганні, але можуть бути корисні для аналізу).\n",
    "    *   1.2. *Аналіз даних:*\n",
    "        *   Огляньте структуру `train.csv` (id, comment\\_text, та 6 бінарних стовпців-міток).\n",
    "        *   Проаналізуйте **розподіл кожної мітки токсичності** (скільки коментарів належать до кожного класу). Зверніть увагу на **дисбаланс**.\n",
    "        *   Дослідіть **кореляції між мітками токсичності** (наприклад, 'toxic' та 'severe\\_toxic' часто зустрічаються разом).\n",
    "    *   1.3. ***Попереднє Оброблення тексту ('comment_text'):*** Створіть функцію очищення тексту:\n",
    "        *   Приведення до нижнього регістру.\n",
    "        *   Видалення IP-адрес, URL-адрес, імен користувачів (якщо є).\n",
    "        *   Видалення спеціальних символів, пунктуації, можливо, чисел.\n",
    "        *   Токенізація.\n",
    "        *   Видалення стоп-слів.\n",
    "        *   Стемінг/Лематизація (опціонально, може допомогти).\n",
    "        *   Застосуйте функцію до `train.csv` та `test.csv`.\n",
    "\n",
    "2.  **Вилучення Ознак та Базова Багатозначна Модель:**\n",
    "    *   2.1. *Розділення даних:* Розділіть `train.csv` на тренувальну та валідаційну вибірки.\n",
    "    *   2.2. *Векторизація Тексту:* Використовуйте **TfidfVectorizer** для перетворення очищених коментарів на числові вектори. Розгляньте використання символьних n-грам (`analyzer='char_wb'`, `ngram_range=(2, 5)`) на додаток до слівних, оскільки вони можуть бути стійкими до одруків та варіацій написання образливих слів.\n",
    "    *   2.3. ***Багатозначна Класифікація (Multi-label):*** Існує кілька підходів:\n",
    "        *   **One-vs-Rest (OvR) / Binary Relevance:** Навчіть **окремий бінарний класифікатор** для кожної з 6 міток токсичності. Можна використовувати Logistic Regression, LinearSVC або MultinomialNB. Це найпростіший підхід. (`sklearn.multiclass.OneVsRestClassifier`).\n",
    "        *   **Classifier Chains:** Послідовно навчає класифікатори, де прогноз попереднього класифікатора використовується як ознака для наступного (може враховувати кореляції міток).\n",
    "        *   **Адаптовані Алгоритми:** Деякі алгоритми можуть напряму працювати з багатозначними виходами (рідше в `scikit-learn`).\n",
    "        *   **Використання `scikit-multilearn`:** Ця бібліотека надає зручні реалізації різних багатозначних стратегій (Binary Relevance, Classifier Chains, Label Powerset тощо).\n",
    "    *   2.4. *Оцінка Багатозначної Класифікації:* Використовуйте метрики, призначені для багатозначних завдань:\n",
    "        *   **Hamming Loss:** Частка неправильно спрогнозованих міток.\n",
    "        *   **Accuracy Score (Subset Accuracy):** Частка зразків, де *всі* мітки спрогнозовані правильно (дуже сувора метрика).\n",
    "        *   **Precision, Recall, $F_1$-score (Micro, Macro, Weighted, Samples):** Усереднені метрики по всіх мітках або для кожного зразка. Micro-F1 часто є хорошою загальною метрикою.\n",
    "        *   **AUC-ROC (per label / micro / macro):** Площа під ROC-кривою, обчислена для кожної мітки окремо або усереднена.\n",
    "        *   *Основною метрикою змагання часто була середня AUC-ROC по всіх стовпцях.*\n",
    "\n",
    "3.  **Розширене Моделювання (Ембединги/DL) та Розгортання:**\n",
    "    *   3.1. *Просунуті ML Моделі:* Спробуйте використовувати більш потужні класифікатори (LightGBM, XGBoost) у рамках стратегії One-vs-Rest.\n",
    "    *   3.2. *Використання Ембедингів (Опціонально):* Дослідіть використання натренованих ембедингів слів/документів (Word2Vec, GloVe, FastText, Sentence Transformers).\n",
    "    *   3.3. ***Моделі Глибокого Навчання (DL):*** Це **найбільш поширений та ефективний підхід** для цього завдання.\n",
    "        *   Побудуйте моделі на основі **LSTM, GRU, 1D CNN** або їх комбінацій.\n",
    "        *   Використовуйте шар `Embedding` (можливо, ініціалізований натренованими векторами).\n",
    "        *   Вихідний шар повинен мати **6 нейронів з сигмоїдною активацією** (оскільки кожна мітка є незалежним бінарним прогнозом). Функція втрат - **Binary Crossentropy**.\n",
    "        *   Розгляньте використання **Transformer-моделей** (наприклад, BERT), що зазвичай дає найкращі результати, але потребує значних обчислювальних ресурсів та досвіду.\n",
    "    *   3.4. *Порівняння Продуктивності:* Порівняйте ML підходи на TF-IDF з DL підходами.\n",
    "    *   3.5. *Підготовка до Подання (якщо потрібно):* Зробіть прогнози ймовірностей для `test.csv` за допомогою найкращої моделі та збережіть у форматі `submission.csv`.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель (ML або DL) та компоненти обробки тексту/векторизації.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для класифікації токсичності коментарів**. Користувач може ввести текст коментаря, і застосунок видасть **прогноз для кожної з 6 категорій токсичності**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `numpy`, `joblib`/`pickle`, `sklearn`, `nltk`, `re`. Якщо DL, то `tensorflow`/`pytorch`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, векторизатор (TF-IDF) або токенізатор/параметри обробки (для DL), функцію очищення тексту.\n",
    "    4.  *Створення інтерфейсу:* Створіть текстове поле (`st.text_area` / `gr.Textbox`) для введення коментаря.\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Отримайте текст.\n",
    "        *   **Застосуйте функцію очищення тексту**.\n",
    "        *   **Застосуйте векторизатор** (`vectorizer.transform()`) або **токенізацію/паддінг** (для DL).\n",
    "    6.  *Прогнозування:*\n",
    "        *   (ML/OvR) Зробіть прогнози для кожної мітки (або отримайте ймовірності `predict_proba`).\n",
    "        *   (DL) Отримайте вихідні ймовірності (6 значень від 0 до 1) з моделі.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовані мітки (або ймовірності) для кожної з 6 категорій (toxic, severe\\_toxic, ...). Можна встановити поріг (наприклад, 0.5) для визначення, чи присвоюється мітка.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки (зверніть увагу на `nltk` дані та потенційно великі DL моделі).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, векторизатор/токенізатор, скрипт очищення, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `nltk`, `re`, `scikit-multilearn` (опціонально), опціонально `keras`/`tensorflow`/`pytorch`, `gensim`, `transformers`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `nltk`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Теплова карта кореляції міток, хмари слів для кожної категорії токсичності, матриці плутанини (для кожної мітки окремо).\n",
    "*   **Метрики оцінки:** Hamming Loss, Accuracy Score (Subset Accuracy), Precision/Recall/F1 (**Micro, Macro, Weighted, Samples**), **AUC-ROC (per label / micro / macro)**.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   **Ретельне очищення тексту** є дуже важливим.\n",
    "*   **TF-IDF з символьними n-грамами** часто є сильним базовим рішенням.\n",
    "*   **Виберіть відповідні метрики** для багатозначної класифікації (Micro-F1 або середня AUC-ROC є поширеними).\n",
    "*   **DL моделі (LSTM/GRU/BERT)** зазвичай показують найкращі результати на цьому завданні, але є складнішими в реалізації та навчанні. One-vs-Rest з Logistic Regression або LinearSVC на TF-IDF є хорошою відправною точкою.\n",
    "*   Переконайтеся, що обробка тексту та векторизація/токенізація в застосунку **точно відповідають** тим, що використовувалися під час навчання.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 34: Інтелектуальна інформаційна система для прогнозування успіху стартапів\n",
    "\n",
    "**Набір даних:** Crunchbase Data (Спрощена версія Kaggle: [https://www.kaggle.com/datasets/manishkc06/startup-success-prediction](https://www.kaggle.com/datasets/manishkc06/startup-success-prediction) - `startup_data.csv`)\n",
    "\n",
    "**Мета:** **Прогнозувати статус (успіх/невдача) стартапу** на основі доступної інформації (фінансування, ринок, місцезнаходження, час існування тощо) (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та Дослідницький Аналіз:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `startup_data.csv`.\n",
    "    *   1.2. *Аналіз та Очищення:*\n",
    "        *   Огляньте стовпці, типи даних, пропуски.\n",
    "        *   **Очистіть 'funding_total_usd':** Цей стовпець може містити нечислові значення (наприклад, '-'). Перетворіть на числовий тип, обробляючи помилки (замінюючи їх на NaN). Обробіть отримані NaN (наприклад, імпутацією 0 або медіаною, але це потребує обережності).\n",
    "        *   **Оброблення Дат:** Розпарсіть 'first\\_funding\\_at', 'last\\_funding\\_at', 'founded\\_at'. Обробіть можливі помилки або нереалістичні дати.\n",
    "        *   Видаліть непотрібні стовпці (ID, посилання, описи, якщо вони не будуть використовуватися).\n",
    "    *   1.3. ***Визначення Цільової Змінної ('status'):***\n",
    "        *   Проаналізуйте унікальні значення стовпця 'status' ('acquired', 'operating', 'closed', 'ipo').\n",
    "        *   **Чітко визначте, що вважається \"успіхом\" та \"невдачею\"**. Поширений підхід: 'acquired' та 'ipo' - успіх (1), 'closed' - невдача (0). 'operating' може бути виключено або віднесено до окремої категорії (що ускладнить завдання). *Приймемо бінарний варіант: успіх (acquired/ipo) vs невдача (closed)*. Відфільтруйте дані, залишаючи лише ці статуси.\n",
    "        *   Перевірте **баланс класів** для нової цільової змінної.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Дослідіть взаємозв'язок між статусом (успіх/невдача) та ознаками:\n",
    "            *   Загальна сума фінансування ('funding\\_total\\_usd').\n",
    "            *   Кількість раундів фінансування ('funding\\_rounds').\n",
    "            *   Ринок ('category\\_code').\n",
    "            *   Країна ('country\\_code').\n",
    "            *   Час існування до першого/останнього фінансування.\n",
    "        *   Використовуйте `boxplot`, `countplot`, гістограми.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Інженерія Ознак:*\n",
    "        *   **Часові ознаки:** Розрахуйте тривалість існування стартапу до закриття/придбання ('lifespan' = 'closed\\_at'/'acquired\\_at' - 'founded\\_at'), час між раундами фінансування, час від заснування до першого фінансування.\n",
    "        *   **Ознаки фінансування:** Середня сума фінансування на раунд.\n",
    "    *   2.2. *Оброблення Ознак:*\n",
    "        *   **Категоріальні:** Закодуйте 'category\\_code', 'country\\_code' (висока кардинальність! Розгляньте Target Encoding, обмеження кількості категорій або OHE для найпопулярніших).\n",
    "        *   **Числові:** Застосуйте `StandardScaler` або `RobustScaler`.\n",
    "        *   **Дати:** Видаліть стовпці дат після створення часових ознак.\n",
    "    *   2.3. *Підготовка даних:* Розділіть на тренувальну/тестову вибірки (з стратифікацією).\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, Decision Tree, Naive Bayes. Врахуйте дисбаланс (`class_weight`).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі за допомогою AUC-ROC, $F_1$-score (для класу успіху та/або невдачі), Precision, Recall, матриці плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Інтерпретація:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей за допомогою крос-валідації.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх підходів.\n",
    "    *   3.4. ***Аналіз Важливості Ознак:*** **Визначте ключові фактори**, що впливають на успіх/невдачу стартапів (сума/раунди фінансування, ринок, країна, часові характеристики). Це може дати цінні інсайти для підприємців та інвесторів.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування ймовірності успіху стартапу**. Застосунок приймає на вхід ключові характеристики стартапу (ринок, фінансування, країна тощо) та видає **прогноз статусу (успіх/невдача)**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та пайплайн попереднього оброблення (скейлер, кодувальники).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для ключових ознак: `st.number_input` для суми фінансування, кількості раундів; `st.selectbox` для ринку ('category\\_code'), країни ('country\\_code'); можливо, поля для дат заснування/фінансування для розрахунку часових ознак. Спростіть введення для категорій з високою кардинальністю.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте збережений пайплайн попереднього оброблення** (розрахунок часових ознак, кодування, масштабування).\n",
    "    6.  *Прогнозування:* Отримайте ймовірність успіху (`model.predict_proba()[:, 1]`). Зробіть бінарний прогноз (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований статус (\"Ймовірний успіх\" / \"Ймовірна невдача\") та/або ймовірність успіху.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіли ознак проти статусу, тренди фінансування, **діаграма важливості ознак**, ROC-крива, PR-крива (якщо дисбаланс суттєвий).\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (для успіху/невдачі), AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Очищення даних та інженерія ознак (особливо часових) є вирішальними***.\n",
    "*   **Чітке визначення цільової змінної ('status')** є першочерговим кроком.\n",
    "*   Обережно обробляйте **категоріальні ознаки з високою кардинальністю**.\n",
    "*   Аналіз **важливості ознак надає практичні інсайти** для екосистеми стартапів.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 35: Інтелектуальна інформаційна система для прогнозування випадків лихоманки Денге\n",
    "\n",
    "**Набір даних:** DengAI: Predicting Disease Spread (DrivenData Competition Data) ([https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/data/](https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/data/)). Також на Kaggle: ([https://www.kaggle.com/datasets/rohanimori/denguedataset](https://www.kaggle.com/datasets/rohanimori/denguedataset)) - Містить `dengue_features_train.csv`, `dengue_features_test.csv`, `dengue_labels_train.csv`.\n",
    "\n",
    "**Мета:** **Прогнозувати кількість випадків лихоманки Денге ('total_cases')** на тиждень у двох конкретних містах (Сан-Хуан - 'sj', Ікітос - 'iq'), використовуючи дані про погоду та клімат (_Регресія часових рядів_). **Основна метрика оцінки - MAE**.\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних, Об'єднання та Дослідження:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `dengue_features_train.csv`, `dengue_features_test.csv` та `dengue_labels_train.csv`.\n",
    "    *   1.2. ***Об'єднання даних:*** Об'єднайте `dengue_features_train` та `dengue_labels_train` за ключовими стовпцями ('city', 'year', 'weekofyear').\n",
    "    *   1.3. *Оброблення Дат:* Перетворіть 'week\\_start\\_date' на datetime об'єкт. Можливо, встановіть його як індекс або використовуйте для створення часових ознак.\n",
    "    *   1.4. *Оброблення Пропущених Значень:*\n",
    "        *   Дослідіть **пропущені значення** в погодних ознаках (їх може бути багато).\n",
    "        *   Застосуйте **стратегії імпутації**: заповнення попереднім значенням (`ffill`), інтерполяція, використання середніх значень за тиждень року/місяць. **Імпутуйте окремо для кожного міста ('sj', 'iq')**.\n",
    "    *   1.5. *EDA:*\n",
    "        *   **Візуалізуйте часові ряди 'total_cases' окремо для Сан-Хуана та Ікітоса**. Зверніть увагу на спалахи та сезонність.\n",
    "        *   Візуалізуйте часові ряди ключових погодних ознак (наприклад, температури 'reanalysis\\_air\\_temp\\_k', 'ndvi\\_\\*').\n",
    "        *   Проаналізуйте кореляції між погодними ознаками та 'total\\_cases' (можливо, з певним лагом).\n",
    "        *   *Розгляньте дані та моделі для двох міст окремо*, оскільки їх клімат та епідеміологія можуть відрізнятися.\n",
    "\n",
    "2.  **Інженерія Ознак та Базова Регресія:**\n",
    "    *   2.1. *Інженерія Ознак:*\n",
    "        *   **Часові ознаки:** Місяць, тиждень року (вже є).\n",
    "        *   ***Лагові ознаки:*** Створіть лаги для погодних змінних та, **що дуже важливо, для самої 'total_cases'** (кількість випадків у попередні тижні).\n",
    "        *   **Ознаки ковзного вікна:** Ковзні середні/медіани/std для погодних ознак та 'total\\_cases'.\n",
    "    *   2.2. *Підготовка даних:* Розділіть дані на тренувальну та валідаційну вибірки (часовий поділ, останні роки для валідації). Масштабуйте числові ознаки за допомогою `StandardScaler` або `MinMaxScaler`.\n",
    "    *   2.3. *Навчання базових моделей:*\n",
    "        *   **Регресійні моделі:** Навчіть Linear Regression, Ridge, Lasso на створених ознаках.\n",
    "        *   **Моделі для лічильних даних:** Оскільки 'total\\_cases' - це кількість, розгляньте моделі, які краще підходять для лічильних даних, наприклад, **Poisson Regression** або **Negative Binomial Regression** (з бібліотеки `statsmodels` або `sklearn`), особливо якщо є надлишкова дисперсія (overdispersion). **Negative Binomial часто є кращим вибором для даних про захворюваність.**\n",
    "        *   **Часові ряди (прості):** ARIMA/SARIMA з екзогенними регресорами (погодні ознаки) - `ARIMAX`/`SARIMAX`.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на валідаційній вибірці, використовуючи **Mean Absolute Error (MAE)** як основну метрику (згідно змагання).\n",
    "\n",
    "3.  **Розширене Моделювання Часових Рядів та Розгортання:**\n",
    "    *   3.1. *Реалізація просунутих моделей:*\n",
    "        *   Навчіть **Random Forest Regressor, XGBoost Regressor, LightGBM Regressor** на інженерних ознаках.\n",
    "        *   Продовжуйте експериментувати з **Negative Binomial Regression**, можливо, з більш складними ознаками.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей, мінімізуючи MAE за допомогою крос-валідації (використовуючи `TimeSeriesSplit`).\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх підходів, особливо звертаючи увагу на MAE.\n",
    "    *   3.4. *Аналіз важливості ознак (для RF/XGB/LGBM):* Визначте, які погодні умови та минулі показники захворюваності найбільше впливають на прогноз.\n",
    "    *   3.5. *Підготовка до Подання (якщо потрібно):* Зробіть прогнози для `dengue_features_test.csv` та збережіть у форматі `submission_format.csv`.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель (окремо для 'sj' та 'iq', якщо моделювали роздільно) та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування щотижневих випадків лихоманки Денге** в Сан-Хуані та/або Ікітосі. Застосунок може показувати історичні дані та **прогноз на наступні тижні** на основі очікуваних (або введених) погодних умов.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `statsmodels`/`xgboost` тощо.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену(і) модель(і) та пайплайн(и) обробки (окремо для 'sj'/'iq'). Завантажте останні історичні дані.\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   Дозвольте користувачеві **вибрати місто** ('sj' або 'iq').\n",
    "        *   (Спрощений варіант) Покажіть прогноз на основі останніх даних.\n",
    "        *   (Складніший варіант) Дозвольте користувачеві ввести очікувані погодні умови на наступні тижні або вибрати типовий сценарій.\n",
    "    5.  *Оброблення вводу та генерація ознак:*\n",
    "        *   Отримайте останні історичні дані для вибраного міста.\n",
    "        *   **Згенеруйте лагові/ковзні ознаки** для прогнозного періоду.\n",
    "        *   **Застосуйте збережений пайплайн обробки** (імпутація, масштабування).\n",
    "    6.  *Прогнозування:* Зробіть прогноз 'total\\_cases' за допомогою відповідної моделі. Округліть до цілого невід'ємного числа.\n",
    "    7.  *Відображення результату:*\n",
    "        *   Побудуйте **графік**, що показує історичні дані та прогнозовану кількість випадків.\n",
    "        *   Виведіть прогнозні значення.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель(і)/пайплайн(и), можливо, зразок даних, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `statsmodels` (для NB/ARIMAX), `xgboost`, `lightgbm` (опціонально).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `plotly` (для графіків).\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Часові ряди (випадки, погода)**, кореляційна теплова карта (ознаки vs випадки), **графік прогнозу проти фактичних значень**.\n",
    "*   **Метрики оцінки:** ***Mean Absolute Error (MAE) - основна***, MSE, RMSE.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   **Ретельне об'єднання даних та вирівнювання за часом** є важливими першими кроками.\n",
    "*   **Стратегія імпутації пропущених погодних даних суттєво впливає на результат.** Імпутуйте окремо для кожного міста.\n",
    "*   Розгляньте **моделі, специфічні для лічильних даних (Negative Binomial)**, оскільки вони краще враховують природу цільової змінної.\n",
    "*   **Моделювання кожного міста ('sj', 'iq') окремо**, ймовірно, дасть кращі результати, ніж одна спільна модель.\n",
    "*   Інженерія **лагових ознак** (особливо для `total_cases`) є ключовою для часових рядів захворюваності.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 36: Інтелектуальна інформаційна система для визначення пульсарів за сигналами зірок\n",
    "\n",
    "**Набір даних:** HTRU2 Dataset ([https://archive.ics.uci.edu/ml/datasets/HTRU2](https://archive.ics.uci.edu/ml/datasets/HTRU2)). Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/pavanraj159/predicting-a-pulsar-star](https://www.kaggle.com/datasets/pavanraj159/predicting-a-pulsar-star)). Містить файл `HTRU_2.csv`.\n",
    "\n",
    "**Мета:** **Класифікувати сигнали** як такі, що походять від **пульсарів (class=1)** або радіочастотних перешкод/шуму (class=0), на основі 8 статистичних ознак, отриманих з інтегрованого профілю та кривої DM-SNR радіотелескопічних даних (_Класифікація / Виявлення аномалій з **сильним дисбалансом**_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження Даних та Аналіз Дисбалансу:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `HTRU_2.csv`. Назви стовпців зазвичай потрібно додати вручну відповідно до опису набору даних (8 ознак + 1 цільова змінна 'class').\n",
    "    *   1.2. *Аналіз даних:* Перевірте типи даних (всі 8 ознак числові), наявність пропусків (зазвичай немає).\n",
    "    *   1.3. ***Аналіз Дисбалансу Класів:***\n",
    "        *   Дослідіть розподіл цільової змінної 'class'.\n",
    "        *   Обчисліть відсоток класу пульсарів (class=1). Він буде **дуже малим** (зазвичай < 10%), що вказує на ***сильний дисбаланс класів***. Це ключова характеристика завдання.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Візуалізуйте **розподіли кожної з 8 ознак** (Mean, Standard deviation, Excess kurtosis, Skewness для інтегрованого профілю та кривої DM-SNR) **окремо для класу 0 та класу 1** (`boxplot`, `violinplot`, `histplot` з `hue='class'`). Це допоможе побачити, які ознаки найкраще розділяють пульсари від шуму.\n",
    "        *   Побудуйте парні діаграми (`pairplot`) для підмножини ознак, забарвлені за класом.\n",
    "        *   Проаналізуйте кореляції між ознаками.\n",
    "\n",
    "2.  **Попереднє Оброблення та Базове Моделювання (з урахуванням дисбалансу):**\n",
    "    *   2.1. ***Масштабування Ознак:*** Оскільки ознаки мають різні масштаби, **застосування `StandardScaler` є дуже важливим**.\n",
    "    *   2.2. *Підготовка даних:* Розділіть дані на ознаки та цільову змінну 'class'. Розділіть на тренувальну та тестову вибірки, **обов'язково використовуючи стратифікацію** (`stratify=y`).\n",
    "    *   2.3. *Стратегії для Дисбалансу:*\n",
    "        *   **Зважування Класів:** Навчіть Logistic Regression, SVM з параметром `class_weight='balanced'`.\n",
    "        *   **Семплінг (з `imblearn`):**\n",
    "            *   Застосуйте SMOTE (або його варіанти, як ADASYN, BorderlineSMOTE) для генерації синтетичних прикладів міноритарного класу (пульсарів) **тільки на тренувальній вибірці**.\n",
    "            *   Застосуйте RandomUnderSampler для зменшення мажоритарного класу (шуму) **тільки на тренувальній вибірці**.\n",
    "            *   Комбіновані методи (SMOTE + Tomek Links, SMOTE + ENN).\n",
    "        *   *Важливо:* Порівняйте результати з різними стратегіями.\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть моделі (Logistic Regression, SVM, Decision Tree, K-NN) на даних, оброблених однією з вищезазначених стратегій (або без неї, але з `class_weight`).\n",
    "    *   2.5. *Оцінка (Акцент на Дисбалансі):* Оцініть моделі на тестовій вибірці. **Критично важливо використовувати метрики, чутливі до дисбалансу**:\n",
    "        *   Precision, Recall, $F_1$-score (**для класу пульсарів, class=1**).\n",
    "        *   **AUC-PR (Precision-Recall Curve):** Площа під PR-кривою є *ключовою метрикою* для таких завдань.\n",
    "        *   AUC-ROC.\n",
    "        *   Geometric Mean (G-Mean).\n",
    "        *   Матриця плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Оптимізація Продуктивності:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, XGBoost, LightGBM. Ці моделі часто добре працюють з дисбалансом, особливо з параметром `scale_pos_weight` (співвідношення кількості негативних до позитивних прикладів).\n",
    "    *   3.2. *Порівняння Стратегій Дисбалансу:* Систематично порівняйте продуктивність кращих класифікаторів (наприклад, XGBoost) у поєднанні з різними стратегіями обробки дисбалансу (зважування класів, SMOTE, Undersampling).\n",
    "    *   3.3. *Налаштування гіперпараметрів:* Оптимізуйте гіперпараметри найкращої комбінації (модель + стратегія дисбалансу), **максимізуючи AUC-PR або $F_1$-score для класу пульсарів**.\n",
    "    *   3.4. *Аналіз важливості ознак:* Визначте, які з 8 статистичних характеристик є найбільш визначальними для ідентифікації пульсарів.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для класифікації радіосигналів**. Застосунок приймає на вхід 8 статистичних ознак сигналу та **прогнозує, чи є джерелом сигналу пульсар**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm` (залежно від моделі).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть 8 полів введення (`st.number_input` / `gr.Number`) для кожної з ознак (Mean, Std Dev, Kurtosis, Skewness для профілю та DM-SNR). Надайте інформацію про типові діапазони значень, якщо можливо.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Сформуйте вектор ознак. **Застосуйте збережений скейлер `StandardScaler`**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`) та, що важливіше, **ймовірності належності до класу пульсарів** (`model.predict_proba()[:, 1]`).\n",
    "    7.  *Відображення результату:* Покажіть прогноз (\"Пульсар\" / \"Не пульсар\"). Можна відобразити ймовірність і пояснити рівень впевненості або встановити поріг для класифікації.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt` (включно з `imblearn`, якщо використовувався для навчання).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально), `imblearn` (дуже рекомендовано).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Розподіли ознак (boxplot/violinplot) для кожного класу**, парні діаграми (підмножина), ***крива Precision-Recall (обов'язково!)***, ROC-крива, матриця плутанини, діаграма важливості ознак.\n",
    "*   **Метрики оцінки:** Precision, Recall, $F_1$-score (**для класу пульсарів, class=1**), AUC-ROC, ***AUC-PR (критично важливо через дисбаланс)***, G-Mean.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Масштабування ознак (`StandardScaler`) є необхідним***.\n",
    "*   **Ретельно підійдіть до проблеми сильного дисбалансу класів.** Порівняння різних стратегій (зважування, SMOTE, undersampling) є ключовою частиною проєкту.\n",
    "*   **Зосередьтеся на метриках, чутливих до продуктивності міноритарного класу**, таких як Recall (для class=1), F1 (для class=1) та **AUC-PR**. Accuracy тут буде вводити в оману.\n",
    "*   Моделі градієнтного бустингу (XGBoost, LightGBM) з параметром `scale_pos_weight` часто є хорошим вибором для таких завдань.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 37: Аналіз успішності кампаній на Kickstarter\n",
    "\n",
    "**Набір даних:** Kickstarter Projects ([https://www.kaggle.com/datasets/kemical/kickstarter-projects](https://www.kaggle.com/datasets/kemical/kickstarter-projects) - `ks-projects-201801.csv`)\n",
    "\n",
    "**Мета:** **Прогнозувати, чи буде кампанія на Kickstarter успішною** ('state' = 'successful') чи невдалою ('state' = 'failed') на основі інформації про кампанію (категорія, мета, тривалість, країна тощо) (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та EDA:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `ks-projects-201801.csv`.\n",
    "    *   1.2. *Аналіз та Очищення:*\n",
    "        *   Огляньте стовпці, типи даних, пропуски.\n",
    "        *   **Фільтрація за Статусом:** Залиште лише кампанії зі статусом 'successful' або 'failed'. Інші статуси ('live', 'canceled', 'suspended', 'undefined') видаліть для спрощення бінарної класифікації. Створіть бінарну цільову змінну (1 - successful, 0 - failed).\n",
    "        *   **Оброблення Валюти та Мети:** Ознака 'goal' вказана в оригінальній валюті. Використовуйте 'usd_goal_real' (або 'usd_pledged_real') як стандартизовану мету/зібрані кошти в USD. Видаліть стовпці, пов'язані з оригінальною валютою, якщо вони не потрібні.\n",
    "        *   **Оброблення Дат:** Розпарсіть 'launched' та 'deadline' як datetime об'єкти.\n",
    "        *   Обробіть пропуски (наприклад, в 'usd_pledged_real', хоча після фільтрації їх може не бути).\n",
    "        *   Видаліть непотрібні стовпці ('ID', 'name', 'usd pledged').\n",
    "    *   1.3. *Інженерія Ознак:*\n",
    "        *   **Тривалість Кампанії:** Розрахуйте тривалість у днях ('duration' = 'deadline' - 'launched').\n",
    "        *   **Часові Ознаки Запуску:** Витягніть місяць запуску, день тижня запуску, годину запуску (якщо час доступний).\n",
    "    *   1.4. *EDA:*\n",
    "        *   Дослідіть взаємозв'язок між статусом (успіх/невдача) та:\n",
    "            *   Категорією ('main\\_category', 'category').\n",
    "            *   Країною ('country').\n",
    "            *   Сумою мети ('usd\\_goal\\_real').\n",
    "            *   Тривалістю ('duration').\n",
    "            *   Місяцем/днем тижня запуску.\n",
    "        *   Використовуйте `countplot`, `histplot`, `boxplot`.\n",
    "        *   Проаналізуйте **рівень успішності** для різних категорій/країн.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Оброблення Категоріальних Ознак:* Закодуйте 'main\\_category', 'category', 'country' (висока кардинальність! Розгляньте OHE для 'main\\_category'/'country' та, можливо, Target Encoding або обмеження категорій для 'category').\n",
    "    *   2.2. *Оброблення Тексту (Опціонально):* Якщо використовується ознака 'name', застосуйте TF-IDF або інші техніки НЛП (це ускладнить модель).\n",
    "    *   2.3. *Масштабування Числових Ознак:* Застосуйте `StandardScaler` до 'usd\\_goal\\_real', 'duration', 'backers\\_count' (якщо використовується до моменту дедлайну як ознака - обережно з витоком даних!).\n",
    "    *   2.4. *Підготовка даних:* Видаліть стовпці дат. Розділіть дані на тренувальну/тестову вибірки (з стратифікацією).\n",
    "    *   2.5. *Навчання базових класифікаторів:* Навчіть Logistic Regression, Naive Bayes, Decision Tree.\n",
    "    *   2.6. *Оцінка:* Оцініть моделі за допомогою Accuracy, $F_1$-score (для класу 'successful'), AUC-ROC, Precision, Recall, матриці плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Прогнозування:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх підходів.\n",
    "    *   3.4. ***Аналіз Важливості Ознак:*** Визначте **ключові фактори успіху** кампаній на Kickstarter (категорія, сума мети, тривалість, час запуску тощо).\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування ймовірності успіху кампанії на Kickstarter**. Застосунок приймає на вхід основні параметри планованої кампанії та видає **прогноз її успішності**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та пайплайн попереднього оброблення (кодувальники, скейлер). Завантажте списки категорій/країн, якщо потрібно для інтерфейсу.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення: `st.selectbox` для основної категорії, категорії, країни; `st.number_input` для суми мети (USD); `st.date_input` для дати запуску та дедлайну (для розрахунку тривалості та часових ознак).\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Зберіть дані.\n",
    "        *   **Розрахуйте інженерні ознаки** (тривалість, місяць/день запуску).\n",
    "        *   **Застосуйте збережений пайплайн обробки** (кодування, масштабування).\n",
    "    6.  *Прогнозування:* Отримайте ймовірність успіху (`model.predict_proba()[:, 1]`) та бінарний прогноз (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований статус (\"Ймовірно успішна\" / \"Ймовірно невдала\") та/або ймовірність успіху.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, списки категорій, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально)\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Рівень успішності за категоріями/країнами**, розподіли мети/зібраних коштів (для успішних/невдалих), **діаграма важливості ознак**, ROC-крива.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score (**для класу 'successful'**), AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Ретельне очищення даних*** (фільтрація статусів, обробка валюти, дат) є важливим першим кроком.\n",
    "*   **Інженерія ознак**, пов'язаних з часом (тривалість, місяць/день запуску), може бути корисною.\n",
    "*   Обережно обробляйте **категоріальні ознаки з високою кардинальністю** ('category', 'country').\n",
    "*   Аналіз важливості ознак дає уявлення про те, що робить кампанію на Kickstarter більш схильною до успіху.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 38: Інтелектуальна інформаційна система для прогнозування ризику діабету\n",
    "\n",
    "**Набір даних:** PIMA Indians Diabetes Database ([https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) - `diabetes.csv`). Класичний набір даних.\n",
    "\n",
    "**Мета:** **Прогнозувати наявність діабету** ('Outcome' = 1) чи його відсутність ('Outcome' = 0) у жінок племені Піма індіанців віком від 21 року на основі 8 діагностичних медичних показників (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Очищення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `diabetes.csv`.\n",
    "    *   1.2. *Аналіз даних:* Огляньте стовпці ('Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'), типи даних, основні статистики.\n",
    "    *   1.3. ***Оброблення Неможливих Нульових Значень:***\n",
    "        *   Зверніть увагу, що значення 0 в ознаках 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI' **фізіологічно неможливі** і, ймовірно, **представляють пропущені значення**.\n",
    "        *   Замініть ці нулі на `NaN` (`df[['Glucose', ...]] = df[['Glucose', ...]].replace(0, np.nan)`).\n",
    "        *   Перевірте кількість новостворених пропусків.\n",
    "        *   **Імпутуйте пропущені значення:** Використовуйте медіану або середнє значення (обчислене на ненульових даних) або `KNNImputer`.\n",
    "    *   1.4. *Аналіз Цільової Змінної ('Outcome'):* Дослідіть розподіл класів (0 vs 1). Перевірте на дисбаланс.\n",
    "    *   1.5. *EDA:*\n",
    "        *   Візуалізуйте **розподіли кожної діагностичної ознаки** окремо для класу 0 та класу 1 (`histplot`, `boxplot` з `hue='Outcome'`).\n",
    "        *   Проаналізуйте кореляції між ознаками.\n",
    "\n",
    "2.  **Масштабування Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Масштабування Ознак:* ***Застосуйте `StandardScaler`*** до всіх 8 діагностичних ознак (після імпутації нулів).\n",
    "    *   2.2. *Підготовка даних:* Розділіть дані на ознаки та цільову змінну 'Outcome'. Розділіть на тренувальну та тестову вибірки (з стратифікацією).\n",
    "    *   2.3. *Навчання базових класифікаторів:* Навчіть Logistic Regression, K-NN, SVM (з різними ядрами), Decision Tree. Врахуйте дисбаланс, якщо він є (`class_weight`).\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на тестовій вибірці. Зосередьтеся на метриках для позитивного класу (Outcome=1): **Recall, $F_1$-score**, а також Accuracy, Precision, AUC-ROC. Використовуйте **стратифіковану крос-валідацію** для вибору моделі.\n",
    "\n",
    "3.  **Розширене Моделювання та Інтерпретація Моделі:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей, можливо, максимізуючи $F_1$-score або Recall для класу 1.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх підходів.\n",
    "    *   3.4. ***Інтерпретація Моделі:***\n",
    "        *   **Проаналізуйте важливість ознак** для найкращої моделі. Які діагностичні показники є найсильнішими предикторами діабету? ('Glucose', 'BMI', 'Age' часто є важливими).\n",
    "        *   _Просунуто (опціонально):_ Використайте бібліотеку **SHAP** для пояснення прогнозів моделі. Це дозволить зрозуміти внесок кожної ознаки у ризик діабету для окремих пацієнтів або в середньому.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування ризику діабету**. Застосунок приймає на вхід 8 діагностичних показників пацієнтки та видає **прогноз наявності діабету**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`, опціонально `shap`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер. Якщо використовували складну імпутацію (KNNImputer), завантажте і його. Якщо використовується SHAP, завантажте пояснювач.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input` / `gr.Number`) для всіх 8 ознак ('Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age').\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Зберіть дані.\n",
    "        *   **Важливо:** Якщо ви імпутували нулі під час навчання, вам потрібно обробити введені нулі (або значення NaN, якщо дозволили пропуски) у застосунку, застосовуючи ту саму стратегію імпутації (наприклад, заміна на медіану з тренувальних даних).\n",
    "        *   Сформуйте вектор ознак.\n",
    "        *   **Застосуйте збережений скейлер `StandardScaler`**.\n",
    "    6.  *Прогнозування:* Отримайте прогноз класу (`model.predict()`) та ймовірність (`model.predict_proba()[:, 1]`).\n",
    "    7.  *Відображення результату:* Покажіть прогноз (\"Ризик діабету: Високий/Низький\" або \"Діабет/Немає діабету\") та ймовірність.\n",
    "    8.  *Відображення Інтерпретації (опціонально):* Якщо використовується SHAP, покажіть графік SHAP values для введених даних.\n",
    "    9.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, імпутер (якщо є), `requirements.txt` (включно з `shap`, якщо використовується).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально), опціонально `shap`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, опціонально `shap`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Розподіли ознак для кожного класу Outcome**, кореляційна теплова карта, **діаграма важливості ознак**, матриця плутанини, ROC-крива, візуалізації SHAP (якщо використовуються).\n",
    "*   **Метрики оцінки:** Accuracy, Precision, **Recall (для Outcome=1)**, **$F_1$-score (для Outcome=1)**, AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Оброблення нульових значень як пропущених*** у 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI' є **вирішальним кроком** для отримання змістовних результатів.\n",
    "*   **Масштабування ознак** (`StandardScaler`) є дуже важливим.\n",
    "*   Зосередьтеся на **інтерпретації моделі** та важливості ознак, оскільки це класичний медичний датасет.\n",
    "*   Використання SHAP може додати значну цінність для пояснення прогнозів.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 39: Категоризація новинних статей\n",
    "\n",
    "**Набір даних:** AG News Classification Dataset (CSV версія: [https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)). Містить `train.csv` та `test.csv`.\n",
    "\n",
    "**Мета:** **Класифікувати новинні статті** за 4 категоріями (1 - World, 2 - Sports, 3 - Business, 4 - Sci/Tech) на основі їхнього **Заголовка ('Title')** та **Опису ('Description')** (_Багатокласова класифікація тексту_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження та Попереднє Оброблення Текстових Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `train.csv` та `test.csv`. Зверніть увагу на назви стовпців ('Class Index', 'Title', 'Description').\n",
    "    *   1.2. *Об'єднання Тексту:* **Об'єднайте стовпці 'Title' та 'Description'** в один текстовий стовпець (наприклад, 'text'), оскільки обидва містять релевантну інформацію для класифікації.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('Class Index'):* Перевірте розподіл 4 класів новин. Чи збалансований набір даних?\n",
    "    *   1.4. ***Попереднє Оброблення тексту ('text'):*** Створіть функцію очищення:\n",
    "        *   Приведення до нижнього регістру.\n",
    "        *   Видалення пунктуації, чисел, спеціальних символів.\n",
    "        *   Токенізація.\n",
    "        *   Видалення стоп-слів (`nltk`).\n",
    "        *   Стемінг або Лематизація (`nltk`).\n",
    "        *   Застосуйте функцію до об'єднаного тексту в `train.csv` та `test.csv`.\n",
    "    *   1.5. *EDA Тексту:*\n",
    "        *   Проаналізуйте середню довжину текстів для кожної категорії новин.\n",
    "        *   Створіть **хмари слів** для кожної з 4 категорій.\n",
    "\n",
    "2.  **Вилучення Ознак та Базова Класифікація:**\n",
    "    *   2.1. *Підготовка даних:* Розділіть `train.csv` на тренувальну та валідаційну вибірки (з стратифікацією за 'Class Index').\n",
    "    *   2.2. *Векторизація Тексту:* Перетворіть очищений 'text' на числові вектори за допомогою **CountVectorizer** або (краще) **TfidfVectorizer**. Експериментуйте з параметрами (`max_features`, `ngram_range`).\n",
    "    *   2.3. *Навчання базових багатокласових класифікаторів:* Навчіть:\n",
    "        *   Multinomial Naive Bayes (`MultinomialNB`).\n",
    "        *   Logistic Regression (`LogisticRegression`, можливо з `solver='liblinear'` або `saga`).\n",
    "        *   Linear Support Vector Classifier (`LinearSVC`).\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на валідаційній вибірці. Використовуйте:\n",
    "        *   Accuracy.\n",
    "        *   **Classification Report** (з Precision, Recall, F1 для кожної категорії).\n",
    "        *   **Macro/Weighted Average $F_1$-score**.\n",
    "        *   Матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Розгортання:**\n",
    "    *   3.1. *Просунуті ML Моделі:* Навчіть Random Forest або LightGBM/XGBoost (можуть бути повільними на високорозмірних TF-IDF даних).\n",
    "    *   3.2. *Використання Ембедингів (Опціонально):* Дослідіть використання натренованих ембедингів (Word2Vec, GloVe, FastText) + агрегація/прості класифікатори.\n",
    "    *   3.3. *Прості DL Моделі (Опціонально):* Реалізуйте **CNN** або **LSTM/GRU** для класифікації тексту.\n",
    "    *   3.4. *Порівняння Продуктивності:* Порівняйте результати базових моделей на TF-IDF з більш просунутими підходами.\n",
    "    *   3.5. *Фінальна Оцінка на `test.csv`*: Навчіть найкращу модель на всьому `train.csv` і зробіть прогнози для `test.csv`. Оцініть на тестових даних (використовуючи мітки з `test.csv`, якщо це той самий файл, що й на Kaggle).\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та векторизатор.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для категоризації новинних статей**. Користувач може ввести заголовок та/або опис новини, і застосунок **прогнозує її категорію** (World, Sports, Business, Sci/Tech).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `numpy`, `joblib`/`pickle`, `sklearn`, `nltk`, `re`. Якщо DL, то `tensorflow`/`pytorch`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, **векторизатор** та функцію очищення тексту. Створіть мапінг індексів класів (1-4) на назви категорій.\n",
    "    4.  *Створення інтерфейсу:* Створіть одне або два текстових поля (`st.text_input` для заголовка, `st.text_area` для опису / `gr.Textbox`) для введення тексту новини.\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Отримайте текст (об'єднайте заголовок та опис).\n",
    "        *   **Застосуйте функцію очищення тексту**.\n",
    "        *   **Застосуйте збережений векторизатор** (`vectorizer.transform()`) або токенізацію/паддінг (для DL).\n",
    "    6.  *Прогнозування:* Зробіть прогноз числового індексу класу (`model.predict()`). **Перетворіть індекс на назву категорії** за допомогою мапінгу.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану категорію новини.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки (переконайтесь у доступності даних `nltk`).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, векторизатор, скрипт очищення, мапінг категорій, `requirements.txt`. Налаштуйте завантаження даних `nltk`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `nltk` (обов'язково), `re`, опціонально `keras`/`tensorflow`/`pytorch`, `gensim`, `wordcloud`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `nltk`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл категорій, **хмари слів для кожної категорії**, матриця плутанини.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**Macro/Weighted та для кожного класу**), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Вибір методів попереднього оброблення тексту та векторизації (TF-IDF)*** суттєво впливає на результати. TF-IDF часто є дуже сильним базовим методом.\n",
    "*   Об'єднання заголовка та опису зазвичай дає більше інформації для класифікатора.\n",
    "*   Порівняйте класичні ML моделі (Logistic Regression, LinearSVC, MultinomialNB) з DL підходами, якщо їх реалізуєте.\n",
    "*   Переконайтеся, що обробка тексту в застосунку **точно відповідає** тій, що використовувалася під час навчання.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 40: Інтелектуальна інформаційна система для прогнозування генерації сонячної енергії\n",
    "\n",
    "**Набір даних:** Solar Power Generation Data (Дві сонячні електростанції в Індії) ([https://www.kaggle.com/datasets/anikannal/solar-power-generation-data](https://www.kaggle.com/datasets/anikannal/solar-power-generation-data)). Містить дані генерації (`Plant_1_Generation_Data.csv`, `Plant_2_Generation_Data.csv`) та погодні дані (`Plant_1_Weather_Sensor_Data.csv`, `Plant_2_Weather_Sensor_Data.csv`) для двох станцій. **Рекомендується вибрати одну станцію (наприклад, Станцію 1) для аналізу.**\n",
    "\n",
    "**Мета:** **Прогнозувати генерацію електроенергії** (постійного струму - 'DC_POWER' або змінного струму - 'AC_POWER') сонячною електростанцією на основі даних з погодних сенсорів (освітленість, температура модуля/навколишнього середовища) та часу (_Регресія часових рядів_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних, Об'єднання та Очищення:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте файли даних генерації та погоди для вибраної станції (наприклад, `Plant_1_Generation_Data.csv` та `Plant_1_Weather_Sensor_Data.csv`).\n",
    "    *   1.2. *Оброблення Дат та Часу:*\n",
    "        *   Перетворіть стовпець 'DATE_TIME' в обох файлах на **datetime об'єкти**.\n",
    "        *   Встановіть 'DATE_TIME' як індекс в обох DataFrame.\n",
    "    *   1.3. ***Об'єднання Даних:*** Об'єднайте DataFrame генерації та погоди за індексом 'DATE_TIME'. Використовуйте зовнішнє або внутрішнє об'єднання, залежно від того, як ви хочете обробляти невідповідності в часі.\n",
    "    *   1.4. *Аналіз та Очищення:*\n",
    "        *   Перевірте наявність пропусків у ключових стовпцях ('DC_POWER', 'AC_POWER', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION'). Обробіть їх, якщо є (наприклад, інтерполяцією).\n",
    "        *   Перевірте діапазони значень на адекватність. Генерація енергії повинна бути >= 0. Освітленість ('IRRADIATION') також >= 0.\n",
    "    *   1.5. *Вибір Цільової Змінної:* Виберіть, що прогнозувати: 'DC_POWER' чи 'AC_POWER'. 'AC_POWER' зазвичай є кінцевим продуктом.\n",
    "    *   1.6. *EDA Часових Рядів:*\n",
    "        *   **Візуалізуйте часові ряди** цільової змінної ('AC_POWER' або 'DC_POWER'), освітленості ('IRRADIATION') та температур ('MODULE_TEMPERATURE', 'AMBIENT_TEMPERATURE') за кілька днів або тижнів. Зверніть увагу на **чіткий добовий цикл** (генерація = 0 вночі).\n",
    "        *   Побудуйте діаграми розсіювання, щоб побачити **залежність генерації від освітленості та температури модуля**.\n",
    "        *   Дослідіть сезонні зміни в максимальній денній генерації або освітленості.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Прогнозування:**\n",
    "    *   2.1. *Інженерія Часових Ознак:*\n",
    "        *   Витягніть **годину дня** та **день року/місяць** з індексу 'DATE_TIME'. Це допоможе моделі вловити добові та річні цикли.\n",
    "        *   Створіть **лагові ознаки** для цільової змінної та ключових погодних ознак (значення за попередні часові кроки, наприклад, 15 хв, 1 год тому).\n",
    "        *   Створіть **ознаки ковзного вікна** (середнє, std освітленості/температури за останню годину тощо).\n",
    "    *   2.2. *Підготовка даних:* Створіть табличний набір даних. Розділіть дані на тренувальну та тестову вибірки (часовий поділ, наприклад, останні кілька днів/тижнів для тесту). Масштабуйте числові ознаки (`StandardScaler` або `MinMaxScaler`).\n",
    "    *   2.3. *Навчання базових моделей:*\n",
    "        *   **Прості регресійні моделі:** Навчіть Linear Regression, Ridge, використовуючи погодні та часові ознаки.\n",
    "        *   **Статистичні часові ряди (складніше):** Можна спробувати SARIMAX, де погодні умови виступають як екзогенні регресори, але це може бути складно через високу частоту даних та нелінійні залежності.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на тестовій вибірці за допомогою MAE, MSE, RMSE, R-squared.\n",
    "\n",
    "3.  **Розширене Моделювання та Розгортання:**\n",
    "    *   3.1. *Реалізація моделей на основі дерев:* Навчіть **Random Forest Regressor, XGBoost Regressor, LightGBM Regressor**. Ці моделі добре справляються з нелінійними залежностями та взаємодіями ознак.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей за допомогою крос-валідації (`TimeSeriesSplit`).\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх підходів.\n",
    "    *   3.4. *Аналіз Важливості Ознак:* Визначте, які фактори (**освітленість, температура модуля, час доби**) найбільше впливають на генерацію сонячної енергії.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який:\n",
    "*   (Опціонально) Відображає **історичні дані** генерації та погоди для вибраної станції.\n",
    "*   Дає змогу користувачеві ввести (або отримує з джерела) **поточні/очікувані погодні умови** (освітленість, температуру).\n",
    "*   **Прогнозує генерацію електроенергії** ('AC_POWER' або 'DC_POWER') на найближчий час (наприклад, наступні кілька годин).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`. Можливо `plotly` для графіків.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та пайплайн обробки (скейлер). Завантажте останні історичні дані для генерації лагових ознак.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення для **основних погодних ознак** ('IRRADIATION', 'MODULE_TEMPERATURE', 'AMBIENT_TEMPERATURE') та вибору **часу доби**.\n",
    "    5.  *Оброблення вводу та генерація ознак:*\n",
    "        *   Зберіть введені дані.\n",
    "        *   **Згенеруйте необхідні часові та лагові ознаки**, використовуючи введені дані та завантажені історичні дані.\n",
    "        *   **Застосуйте збережений пайплайн обробки** (масштабування).\n",
    "    6.  *Прогнозування:* Зробіть прогноз генерації (`model.predict()`). Переконайтеся, що прогноз невід'ємний.\n",
    "    7.  *Відображення результату:* Покажіть прогнозоване значення генерації (AC або DC Power). Можна додати графік прогнозу на кілька наступних кроків.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, зразок даних, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `statsmodels` (опціонально), `xgboost`, `lightgbm` (рекомендовано).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `plotly` (опціонально).\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Часові ряди (генерація, погода)**, **діаграми розсіювання (освітленість/температура vs генерація)**, **графік прогнозу проти фактичних значень**, діаграма важливості ознак.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, R-squared.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Ретельне вирівнювання за часом*** між даними генерації та погоди є критично важливим.\n",
    "*   **Освітленість ('IRRADIATION')** є домінуючим фактором, але температура модуля та час доби також важливі.\n",
    "*   **Врахуйте добовий цикл:** Модель повинна прогнозувати нульову генерацію вночі. Часові ознаки (година) та лаги допомагають це зробити.\n",
    "*   Моделі на основі дерев (RF, XGBoost, LightGBM) зазвичай добре справляються з цим завданням через нелінійні залежності.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 41: Інтелектуальна інформаційна система для прогнозування успішності студентів\n",
    "\n",
    "**Набір даних:** Student Performance Data Set (UCI) ([https://archive.ics.uci.edu/ml/datasets/Student+Performance](https://archive.ics.uci.edu/ml/datasets/Student+Performance)). Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/larsen0966/student-performance-data-set](https://www.kaggle.com/datasets/larsen0966/student-performance-data-set)). Містить дані для двох предметів: Математика (`student-mat.csv`) та Португальська мова (`student-por.csv`). **Рекомендується вибрати один предмет, наприклад, Математику.**\n",
    "\n",
    "**Мета:** **Прогнозувати фінальну оцінку студента ('G3')** на основі демографічних, соціальних та шкільних факторів (_Регресія або Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Попереднє Оброблення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте вибраний файл (наприклад, `student-mat.csv`) за допомогою `pandas`, вказавши роздільник `';'`.\n",
    "    *   1.2. *Аналіз даних:* Огляньте стовпці, типи даних (багато категоріальних 'yes'/'no' або текстових), пропуски (зазвичай немає).\n",
    "    *   1.3. *Аналіз Цільової Змінної ('G3'):*\n",
    "        *   **Візуалізуйте розподіл фінальних оцінок 'G3'** (шкала 0-20). Зверніть увагу на можливу концентрацію оцінок або наявність нулів.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Дослідіть взаємозв'язок між 'G3' та іншими ознаками:\n",
    "            *   **Попередні оцінки ('G1', 'G2'):** Побудуйте діаграми розсіювання G1/G2 проти G3. Очікується сильна кореляція.\n",
    "            *   **Демографічні/Соціальні:** 'sex', 'age', 'address' (Urban/Rural), 'famsize', 'Pstatus' (Parent's cohabitation status), 'Medu', 'Fedu' (батьківська освіта), 'Mjob', 'Fjob', 'guardian', 'traveltime', 'studytime', 'failures' (кількість минулих невдач), 'famrel', 'freetime', 'goout', 'Dalc', 'Walc' (споживання алкоголю), 'health', 'absences'. Використовуйте `boxplot`, `countplot`.\n",
    "            *   **Шкільні:** 'schoolsup', 'famsup', 'paid' (додаткові заняття), 'activities', 'nursery', 'higher' (бажання вищої освіти), 'internet', 'romantic'.\n",
    "        *   Проаналізуйте кореляції між числовими ознаками.\n",
    "    *   1.5. *Оброблення Категоріальних Ознак:* Закодуйте всі бінарні ('yes'/'no') та інші категоріальні ознаки (Mjob, Fjob, reason, guardian) за допомогою **One-Hot Encoding** або **Label Encoding** (для бінарних).\n",
    "\n",
    "2.  **Регресійне Моделювання:**\n",
    "    *   2.1. *Підхід:* Прогнозування точної оцінки 'G3'.\n",
    "    *   2.2. *Вибір Ознак:* **Вирішіть, чи включати 'G1' та 'G2' як ознаки.** Якщо так, модель, ймовірно, буде дуже точною, але менш корисною для прогнозування *до* отримання проміжних оцінок. Якщо ні, завдання стає складнішим, але більш реалістичним для раннього прогнозування. **Рекомендується спробувати обидва варіанти.**\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на ознаки та цільову 'G3'. Розділіть на тренувальну/тестову вибірки. Масштабуйте числові ознаки (`StandardScaler`).\n",
    "    *   2.4. *Навчання регресійних моделей:* Навчіть Linear Regression, Ridge, SVR, Random Forest Regressor, XGBoost Regressor.\n",
    "    *   2.5. *Оцінка:* Оцініть моделі за допомогою MAE, MSE, RMSE, R-squared. Порівняйте результати з використанням G1/G2 та без них.\n",
    "\n",
    "3.  **Класифікаційний Підхід та Інтерпретація:**\n",
    "    *   3.1. *Перетворення Цільової Змінної:* **Створіть категорії успішності**, наприклад:\n",
    "        *   _Бінарна:_ 'Pass' (G3 >= 10) vs 'Fail' (G3 < 10).\n",
    "        *   _Багатокласова:_ Наприклад, 'Fail' (0-9), 'Sufficient' (10-13), 'Good' (14-17), 'Excellent' (18-20).\n",
    "        *   *Обґрунтуйте вибір порогів/категорій.* Перевірте баланс класів.\n",
    "    *   3.2. *Підготовка даних:* Використовуйте ті ж оброблені ознаки, але нову категоріальну цільову змінну. Розділіть на тренувальну/тестову вибірки (з стратифікацією).\n",
    "    *   3.3. *Навчання класифікаторів:* Навчіть Logistic Regression, Random Forest Classifier, XGBoost Classifier. Врахуйте дисбаланс (`class_weight`). Знову ж таки, розгляньте варіант з/без G1/G2 як ознак.\n",
    "    *   3.4. *Оцінка класифікації:* Оцініть моделі за допомогою Accuracy, Precision, Recall, F1 (macro/weighted та для кожного класу), AUC-ROC.\n",
    "    *   3.5. ***Порівняння Підходів та Інтерпретація:***\n",
    "        *   Порівняйте регресійний та класифікаційний підходи. Який з них дає більш корисну інформацію?\n",
    "        *   Для найкращої моделі (регресійної або класифікаційної, ймовірно, без G1/G2 для кращої інтерпретації факторів ризику), **проаналізуйте важливість ознак**. Які демографічні, соціальні та шкільні фактори найбільше впливають на успішність студентів? ('studytime', 'failures', 'higher', 'absences', 'Medu'/'Fedu' часто є важливими).\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування успішності студентів**. Застосунок приймає на вхід дані про студента (демографічні, соціальні, шкільні) та видає **прогноз фінальної оцінки** (або категорії успішності).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost` (якщо використовується).\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель (регресійну або класифікаційну) та пайплайн обробки (кодувальники, скейлер).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для ключових ознак: `st.number_input`/'slider' для 'age', 'studytime', 'failures', 'absences'; `st.selectbox`/`radio` для 'sex', 'Mjob', 'Fjob', 'higher', 'internet' тощо. (Не включайте G1/G2, якщо мета - раннє прогнозування).\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте збережений пайплайн обробки** (кодування, масштабування).\n",
    "    6.  *Прогнозування:*\n",
    "        *   (Регресія) Зробіть прогноз оцінки (`model.predict()`). Округліть або обмежте діапазоном 0-20.\n",
    "        *   (Класифікація) Зробіть прогноз категорії (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану оцінку або категорію успішності. Можна додати інтерпретацію на основі важливості ознак.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіли оцінок G3, **взаємозв'язок ознак з G3/категоріями успішності (boxplot/countplot)**, кореляції ознак, **діаграма важливості ознак**.\n",
    "*   **Метрики оцінки:**\n",
    "    *   *Регресія:* MAE, MSE, RMSE, R-squared.\n",
    "    *   *Класифікація:* Accuracy, Precision, Recall, F1, AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   **Чітко визначте, чи будете використовувати G1 та G2 як предиктори**, і обґрунтуйте свій вибір. Прогнозування без них є складнішим, але дає більше інсайтів про фактори ризику.\n",
    "*   Порівняйте **регресійний та класифікаційний підходи** та обговоріть їх доцільність.\n",
    "*   ***Інтерпретація моделі та аналіз важливості ознак*** є ключовими для розуміння факторів, що впливають на академічну успішність.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 42: Аналіз тональності твітів (Загальний)\n",
    "\n",
    "**Набір даних:** Sentiment140 Dataset ([http://help.sentiment140.com/for-students/](http://help.sentiment140.com/for-students/)). Містить 1.6 мільйона твітів, автоматично розмічених за емотиконами (0 - негативний, 4 - позитивний). Kaggle Mirror: ([https://www.kaggle.com/datasets/kazanova/sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)). ***Дуже великий набір даних! Потребує значної вибірки*** (наприклад, 50k-200k записів).\n",
    "\n",
    "**Мета:** **Класифікувати твіти** як позитивні (мітка 4) чи негативні (мітка 0) на основі їхнього текстового вмісту (_Бінарна класифікація тексту_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Вибірка Даних, Завантаження та Очищення:**\n",
    "    *   1.1. ***Вибірка та Завантаження:***\n",
    "        *   **Обов'язково зробіть вибірку!** Завантаження та обробка 1.6M твітів може бути дуже ресурсомісткою. Використайте `pandas` з `chunksize` для читання частинами та відбору випадкової підмножини, або інструменти командного рядка для створення меншого файлу. Виберіть розмір вибірки (наприклад, 100,000 записів, зберігаючи баланс класів).\n",
    "        *   Завантажте вибрані дані. Файл не має заголовка, тому **призначте назви стовпців** (наприклад, 'target', 'ids', 'date', 'flag', 'user', 'text').\n",
    "    *   1.2. *Підготовка даних:*\n",
    "        *   Виберіть стовпці 'target' та 'text'.\n",
    "        *   Перетворіть 'target' (0 та 4) на 0 та 1.\n",
    "        *   Перевірте баланс класів у вашій вибірці.\n",
    "    *   1.3. ***Очищення Тексту Твітів:*** Створіть функцію очищення, адаптовану до твітів:\n",
    "        *   Приведення до нижнього регістру.\n",
    "        *   **Видалення згадок (@username) та URL-адрес.**\n",
    "        *   Обробка хештегів (#hashtag) - можливо, видалення '#' або збереження слова.\n",
    "        *   Видалення пунктуації, чисел, спеціальних символів.\n",
    "        *   **Оброблення емотиконів та смайлів** (можна видалити або спробувати замінити на спеціальні токени, але це складно).\n",
    "        *   Обробка скорочень та сленгу (дуже складно, зазвичай ігнорується).\n",
    "        *   Токенізація.\n",
    "        *   Видалення стоп-слів (`nltk`).\n",
    "        *   Стемінг або Лематизація (`nltk`).\n",
    "        *   Застосуйте функцію до стовпця 'text'.\n",
    "\n",
    "2.  **Вилучення Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Розділення даних:* Розділіть вашу вибірку на тренувальну та валідаційну (з стратифікацією).\n",
    "    *   2.2. *Векторизація Тексту:* Перетворіть очищені твіти на вектори за допомогою **CountVectorizer** або **TfidfVectorizer**. Експериментуйте з параметрами (`ngram_range=(1, 2)`, `max_features`).\n",
    "    *   2.3. *Навчання базових класифікаторів:* Навчіть Logistic Regression, MultinomialNB, LinearSVC на TF-IDF ознаках.\n",
    "    *   2.4. *Оцінка:* Оцініть моделі на валідаційній вибірці за допомогою Accuracy, $F_1$-score, AUC-ROC, Precision, Recall, матриці плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Розгортання:**\n",
    "    *   3.1. *Просунуті ML Моделі:* Навчіть LightGBM/XGBoost, якщо TF-IDF матриця не занадто велика.\n",
    "    *   3.2. *Використання Ембедингів (Опціонально):* Дослідіть натреновані ембединги (Word2Vec, GloVe, FastText). Можна усереднити вектори слів у твіті або використати більш складні методи (TF-IDF-зважене усереднення).\n",
    "    *   3.3. *Прості DL Моделі (Опціонально):* Реалізуйте **CNN** або **LSTM/GRU** для класифікації твітів.\n",
    "    *   3.4. *Порівняння Продуктивності:* Порівняйте різні підходи.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та векторизатор/компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для аналізу тональності твітів (або будь-якого короткого тексту)**. Користувач вводить текст, і застосунок **прогнозує його тональність** (позитивна/негативна).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `numpy`, `joblib`/`pickle`, `sklearn`, `nltk`, `re`. Якщо DL, то `tensorflow`/`pytorch`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте модель, **векторизатор** та функцію очищення тексту.\n",
    "    4.  *Створення інтерфейсу:* Створіть текстове поле (`st.text_area` / `gr.Textbox`) для введення твіту/тексту.\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Отримайте текст.\n",
    "        *   **Застосуйте функцію очищення тексту**, адаптовану для твітів.\n",
    "        *   **Застосуйте збережений векторизатор** (`vectorizer.transform()`) або обробку для DL.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`) та/або ймовірності (`predict_proba`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану тональність (\"Позитивний\" / \"Негативний\") та, можливо, ймовірність.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки (зверніть увагу на `nltk` дані).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, векторизатор, скрипт очищення, `requirements.txt`. Налаштуйте завантаження даних `nltk`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `nltk` (обов'язково), `re`, опціонально `keras`/`tensorflow`/`pytorch`, `gensim`, `wordcloud`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `nltk`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Хмари слів для кожної тональності**, матриця плутанини, ROC-крива.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, $F_1$-score, AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Вибірка даних є необхідною*** через величезний розмір датасету.\n",
    "*   ***Очищення тексту твітів є критично важливим*** і потребує уваги до специфіки мови Twitter (згадки, хештеги, URL).\n",
    "*   **TF-IDF з n-грамами (1, 2)** та Logistic Regression/LinearSVC часто є дуже ефективним базовим рішенням.\n",
    "*   **Оброблення неформальної мови, сарказму, емотиконів** є основними викликами в аналізі тональності твітів.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 43: Інтелектуальна інформаційна система для визначення присутності людей за даними сенсорів\n",
    "\n",
    "**Набір даних:** Occupancy Detection Dataset (UCI) ([https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+)). Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/robmarkcole/occupancy-detection-dataset](https://www.kaggle.com/datasets/robmarkcole/occupancy-detection-dataset)). Містить три файли: `datatraining.txt`, `datatest.txt`, `datatest2.txt` (останній - дані з іншого періоду, корисний для перевірки узагальнення).\n",
    "\n",
    "**Мета:** **Виявити присутність людей у кімнаті** ('Occupancy', 0 - немає, 1 - є) на основі показників сенсорів температури, вологості, освітленості (Light) та CO2 (_Класифікація / Класифікація часових рядів_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних та Дослідження:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `datatraining.txt`, `datatest.txt`, `datatest2.txt`. Зверніть увагу, що це `.txt` файли з роздільником-комою, може знадобитися вказати `sep=','` та назви стовпців ('date', 'Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio', 'Occupancy').\n",
    "    *   1.2. *Оброблення Дат та Часу:* Перетворіть стовпець 'date' на datetime індекс. Відсортуйте за індексом.\n",
    "    *   1.3. *Аналіз даних:* Перевірте типи даних, пропуски (зазвичай немає).\n",
    "    *   1.4. *Аналіз Цільової Змінної ('Occupancy'):* Дослідіть розподіл класів. Чи є дисбаланс?\n",
    "    *   1.5. *EDA:*\n",
    "        *   **Візуалізуйте часові ряди** показників сенсорів ('Temperature', 'Humidity', 'Light', 'CO2') та цільової змінної 'Occupancy' на тренувальному наборі. Це допоможе побачити, як змінюються показники при вході/виході людей. **Освітленість ('Light') часто є дуже сильним індикатором.**\n",
    "        *   Візуалізуйте **розподіли показників сенсорів** окремо для класу 0 та класу 1 (`boxplot` або `histplot` з `hue='Occupancy'`).\n",
    "        *   Проаналізуйте кореляції між показниками сенсорів.\n",
    "\n",
    "2.  **Масштабування Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Вибір Ознак:* Всі сенсорні показники ('Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio') є потенційно корисними.\n",
    "    *   2.2. *Масштабування Ознак:* ***Застосуйте `StandardScaler`*** до всіх сенсорних ознак.\n",
    "    *   2.3. *Підготовка даних:* Розділіть `datatraining.txt` на тренувальну та валідаційну вибірки (можна використовувати часовий поділ або випадковий, оскільки явних часових залежностей, що вимагають складних моделей, може не бути). Використовуйте `datatest.txt` та `datatest2.txt` як **незалежні тестові набори**.\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, K-NN, Decision Tree, Gaussian Naive Bayes.\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на **обох тестових наборах** (`datatest.txt`, `datatest2.txt`). Використовуйте Accuracy, Precision, Recall, F1 (для класу Occupancy=1), AUC-ROC, матрицю плутанини. Порівняйте результати на двох тестових наборах, щоб оцінити стабільність моделі.\n",
    "\n",
    "3.  **Розширене Моделювання та Часові Аспекти:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, XGBoost, LightGBM.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей.\n",
    "    *   3.3. *Часові Аспекти (Опціонально):*\n",
    "        *   Спробуйте додати **лагові ознаки** для показників сенсорів (значення за кілька хвилин до поточного моменту). Чи покращує це модель?\n",
    "        *   Розгляньте використання моделей, що працюють з послідовностями (LSTM/GRU), хоча для цього завдання простіші моделі часто працюють дуже добре.\n",
    "    *   3.4. *Порівняння Моделей:* Порівняйте продуктивність усіх підходів на обох тестових наборах.\n",
    "    *   3.5. *Аналіз Важливості Ознак:* Визначте, які показники сенсорів ('Light', 'CO2', 'Temperature'?) є найважливішими для визначення присутності.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для визначення присутності людей у кімнаті**. Застосунок приймає на вхід поточні показники сенсорів та видає **прогноз присутності (Є/Немає)**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input` / `gr.Number`) для всіх сенсорних ознак ('Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio').\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Сформуйте вектор ознак. **Застосуйте збережений скейлер `StandardScaler`**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`) та, можливо, ймовірності (`predict_proba`).\n",
    "    7.  *Відображення результату:* Покажіть прогноз: \"Присутність: Є\" або \"Присутність: Немає\".\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Часові ряди сенсорів, забарвлені за Occupancy**, **розподіли ознак для кожного класу**, матриця плутанини, ROC-крива, діаграма важливості ознак.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**для Occupancy=1**), AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Масштабування ознак (`StandardScaler`) є важливим***.\n",
    "*   **Оцінюйте модель на обох наданих тестових наборах** (`datatest.txt`, `datatest2.txt`), щоб перевірити її здатність до узагальнення в трохи інших умовах.\n",
    "*   Ознака **'Light' (освітленість)**, ймовірно, буде **дуже сильним предиктором**.\n",
    "*   Хоча дані є часовим рядом, прості класифікатори, що розглядають кожен часовий зріз незалежно, часто показують **дуже високу точність** на цьому наборі даних. Додавання лагових ознак може дати невелике покращення.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 44: Аналіз та прогнозування злочинності в Чикаго\n",
    "\n",
    "**Набір даних:** Chicago Crimes (2012-2017) ([https://www.kaggle.com/datasets/currie32/crimes-in-chicago](https://www.kaggle.com/datasets/currie32/crimes-in-chicago)). ***Дуже великий набір даних! Потребує вибірки*** (наприклад, за одним-двома роками, або за певними районами (Community Areas/Police Districts)).\n",
    "\n",
    "**Мета:** Аналіз патернів злочинності та **прогнозування або типу злочину ('Primary Type')**, або **кількості злочинів** у певних географічних/часових проміжках (_Класифікація або Прогнозування Часових Рядів/Регресія_). **Рекомендується вибрати одне з двох основних завдань.**\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Вибірка Даних, Очищення та Геопросторовий/Часовий EDA:**\n",
    "    *   1.1. ***Вибірка та Завантаження:***\n",
    "        *   **Зробіть вибірку!** Наприклад, завантажте дані лише за 2016-2017 роки або для кількох обраних районів (Community Area).\n",
    "        *   Завантажте вибрані дані.\n",
    "    *   1.2. *Очищення та Підготовка:*\n",
    "        *   Перетворіть 'Date' на datetime об'єкт. Встановіть як індекс або використовуйте для витягнення ознак.\n",
    "        *   Обробіть пропуски (особливо в 'Latitude', 'Longitude', 'Location'). Видаліть рядки з критичними пропусками.\n",
    "        *   Витягніть часові ознаки: **рік, місяць, день тижня, година**.\n",
    "        *   Перетворіть бінарні ознаки ('Arrest', 'Domestic') на 0/1.\n",
    "    *   1.3. *EDA:*\n",
    "        *   **Часовий Аналіз:** Візуалізуйте загальну кількість злочинів у часі (по роках, місяцях, днях тижня, годинах). Дослідіть тренди та сезонність.\n",
    "        *   **Аналіз Типів Злочинів:** Дослідіть розподіл 'Primary Type'. Які типи є найчастішими?\n",
    "        *   **Геопросторовий Аналіз:**\n",
    "            *   Візуалізуйте **розподіл злочинів на карті Чикаго**, використовуючи 'Latitude' та 'Longitude'. Можна використовувати `matplotlib` (scatter plot), `seaborn` (kdeplot) або інтерактивні бібліотеки `folium`/`geopandas` для створення **теплових карт (heatmaps)** або кластеризації злочинів.\n",
    "            *   Проаналізуйте, як розподіл злочинів (загальний або за типом) відрізняється між районами (Community Area, Police District, Ward).\n",
    "\n",
    "2.  **Завдання 1: Інтелектуальна інформаційна система для класифікування Типу Злочину ('Primary Type'):**\n",
    "    *   2.1. *Підготовка даних:*\n",
    "        *   Виберіть ознаки: часові (година, день тижня, місяць), локаційні (Community Area, District, Ward, X Coordinate, Y Coordinate), можливо, бінарні ('Arrest', 'Domestic').\n",
    "        *   Через високу кількість класів 'Primary Type', можливо, варто **агрегувати їх** у більш загальні категорії (наприклад, \"Крадіжка\", \"Насильницькі злочини\", \"Пов'язані з наркотиками\") або **обмежитися прогнозуванням лише найчастіших типів**.\n",
    "        *   Закодуйте категоріальні ознаки (райони, агреговані типи, якщо є). Масштабуйте координати/часові ознаки.\n",
    "        *   Розділіть на тренувальну/тестову вибірки (з стратифікацією).\n",
    "    *   2.2. *Навчання Моделей:* Навчіть базові (Logistic Regression, Decision Tree) та просунуті (Random Forest, LightGBM) багатокласові класифікатори. Врахуйте дисбаланс.\n",
    "    *   2.3. *Оцінка:* Оцініть за допомогою Accuracy, **Weighted/Macro $F_1$-score**, Classification Report, матриці плутанини.\n",
    "\n",
    "3.  **Завдання 2: Інтелектуальна інформаційна система для прогнозування Кількісті Злочинів (Часові Ряди/Регресія):**\n",
    "    *   3.1. *Агрегація Даних:* **Агрегуйте кількість злочинів** за певним часовим інтервалом (наприклад, **день або тиждень**) та географічною одиницею (наприклад, **Community Area**). Створіть часовий ряд кількості злочинів для кожної обраної географічної одиниці.\n",
    "    *   3.2. *Інженерія Ознак:* Створіть часові ознаки (день тижня, місяць), **лагові ознаки** (кількість злочинів у попередні періоди), **ознаки ковзного вікна**. Можна додати зовнішні ознаки, якщо доступні (погода, свята).\n",
    "    *   3.3. *Навчання Моделей:*\n",
    "        *   **Часові Ряди:** Застосуйте SARIMA(X) до кожного ряду окремо.\n",
    "        *   **Регресія:** Навчіть регресійні моделі (Ridge, Random Forest, XGBoost, LightGBM) на табличних даних з інженерними ознаками. Розгляньте **Poisson Regression** або **Negative Binomial Regression**, оскільки цільова змінна - кількість.\n",
    "    *   3.4. *Оцінка:* Оцініть моделі на відкладеному тестовому періоді за допомогою MAE, MSE, RMSE.\n",
    "    *   3.5. *Аналіз/Збереження:* Проаналізуйте важливість ознак (для ML регресії). Збережіть найкращу модель.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який (залежно від обраного завдання):\n",
    "*   **(Завдання 1):** Приймає на вхід час та місце і **прогнозує найімовірніший тип злочину**.\n",
    "*   **(Завдання 2):** Відображає карту або часовий ряд та **прогнозує очікувану кількість злочинів** у вибраному районі на наступний період (день/тиждень).\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`/`statsmodels`. Для карт: `folium`/`geopandas`/`pydeck`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте модель, пайплайн обробки, можливо, геодані районів (GeoJSON).\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   **(Завдання 1):** Поля для введення години, дня тижня, району/координат.\n",
    "        *   **(Завдання 2):** Карта (`st.pydeck_chart`, `st.map`, `folium_static`) для вибору району або відображення прогнозу. Вибір дати/періоду.\n",
    "    5.  *Оброблення вводу та Прогнозування:*\n",
    "        *   Зберіть дані. Застосуйте пайплайн обробки. Зробіть прогноз (клас або кількість).\n",
    "    6.  *Відображення результату:*\n",
    "        *   **(Завдання 1):** Покажіть прогнозований тип злочину.\n",
    "        *   **(Завдання 2):** Відобразіть прогноз на карті (кольором/розміром) або на графіку часового ряду.\n",
    "    7.  *Запуск та Розгортання:* Стандартні кроки (можуть знадобитися додаткові бібліотеки для геовізуалізації).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, геодані (якщо є), `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, `statsmodels` (опціонально), опціонально `geopandas`, `folium` (для карт).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, опціонально `folium`/`pydeck`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл типів злочинів, **часові ряди кількості злочинів**, **теплові карти злочинності на карті Чикаго**, важливість ознак, матриця плутанини (класифікація), графіки прогнозу (часові ряди).\n",
    "*   **Метрики оцінки:**\n",
    "    *   *Класифікація:* Accuracy, F1 (weighted/macro).\n",
    "    *   *Часові Ряди/Регресія:* MAE, MSE, RMSE.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Вибірка даних є необхідною***.\n",
    "*   **Геопросторовий аналіз та візуалізація** значно збагачують проєкт. Використання `geopandas` для роботи з районами може бути корисним.\n",
    "*   **Чітко визначте та сфокусуйтеся на одному завданні:** або класифікація типу злочину, або прогнозування їх кількості.\n",
    "*   Для прогнозування кількості розгляньте моделі для лічильних даних (Poisson/NB).\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 45: Інтелектуальна інформаційна система для прогнозування успіху маркетингової кампанії банку\n",
    "\n",
    "**Набір даних:** Bank Marketing Dataset (UCI) ([https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)). Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/janiobachmann/bank-marketing-dataset](https://www.kaggle.com/datasets/janiobachmann/bank-marketing-dataset)). Зазвичай використовують `bank-full.csv` (більший) або `bank.csv` (менший).\n",
    "\n",
    "**Мета:** **Прогнозувати, чи підпишеться клієнт на терміновий депозит** (цільова змінна 'y', 'yes'/'no') за результатами телефонної маркетингової кампанії, на основі даних про клієнта та попередні контакти (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Попереднє Оброблення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте вибраний файл (наприклад, `bank-full.csv`), вказавши роздільник `';'`.\n",
    "    *   1.2. *Аналіз даних:* Огляньте стовпці (демографічні: age, job, marital, education; пов'язані з кампанією: contact, month, day_of_week, duration, campaign, pdays, previous, poutcome; цільова: y). Перевірте типи даних, пропуски (можуть бути значення 'unknown').\n",
    "    *   1.3. *Оброблення 'unknown':* Вирішіть, як обробляти значення 'unknown' в категоріальних ознаках (залишити як окрему категорію, імпутувати модою, видалити рядки).\n",
    "    *   1.4. *Аналіз Цільової Змінної ('y'):*\n",
    "        *   Дослідіть розподіл класів ('yes'/'no'). Набір даних **сильно незбалансований** (мало 'yes').\n",
    "        *   Перетворіть 'y' на бінарний формат (0/1).\n",
    "    *   1.5. *EDA:*\n",
    "        *   Візуалізуйте взаємозв'язки між ознаками та цільовою змінною 'y':\n",
    "            *   `countplot` з `hue='y'` для категоріальних ('job', 'marital', 'education', 'month', 'poutcome').\n",
    "            *   `boxplot` або `histplot` з `hue='y'` для числових ('age', 'duration', 'campaign', 'previous').\n",
    "        *   **Зверніть особливу увагу на ознаку 'duration' (тривалість останнього дзвінка).** Вона зазвичай є **дуже сильним предиктором**, але її значення *не відоме до завершення дзвінка*, тому використання її для прогнозування *майбутнього* успіху дзвінка є проблематичним (витік даних). Для реалістичного прогнозування її слід **видалити**. Однак, для аналізу факторів *після* кампанії вона є важливою. Вирішіть, чи включати її, і обґрунтуйте. *Рекомендується видалити 'duration' для основного моделювання.*\n",
    "    *   1.6. *Оброблення Категоріальних Ознак:* Закодуйте всі категоріальні ознаки (job, marital, education, ...) за допомогою One-Hot Encoding.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання (з урахуванням дисбалансу):**\n",
    "    *   2.1. *Інженерія Ознак (Опціонально):* Можна спробувати створити бінарні ознаки з 'pdays' (чи контактували раніше) або згрупувати рідкісні категорії 'job'.\n",
    "    *   2.2. *Масштабування Числових Ознак:* Застосуйте `StandardScaler` до числових ознак (age, campaign, pdays, previous тощо, **але не 'duration', якщо видалили**).\n",
    "    *   2.3. *Підготовка даних:* Розділіть на тренувальну/тестову вибірки (з стратифікацією за 'y').\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, Decision Tree. **Обов'язково врахуйте дисбаланс** (`class_weight='balanced'` або застосуйте `imblearn` техніки семплінгу (SMOTE/Undersampling) на тренувальній вибірці).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі, зосереджуючись на **AUC-PR** та **$F_1$-score для класу 'yes'**. Також дивіться на AUC-ROC, Precision, Recall (для 'yes'), матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Важливість Ознак:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM. Використовуйте параметр `scale_pos_weight` для роботи з дисбалансом.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей, максимізуючи AUC-PR або F1 для класу 'yes'.\n",
    "    *   3.3. *Порівняння Моделей та Стратегій Дисбалансу:* Порівняйте продуктивність різних моделей та підходів до дисбалансу (зважування vs семплінг).\n",
    "    *   3.4. ***Аналіз Важливості Ознак:*** Визначте **ключові фактори**, що впливають на успіх підписки на депозит (наприклад, результат попередньої кампанії 'poutcome', місяць контакту 'month', кількість попередніх контактів 'previous', вік 'age', тип роботи 'job'). Якщо 'duration' не видаляли, вона буде найважливішою, що підтверджує необхідність її видалення для реалістичного прогнозу.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування ймовірності успіху контакту** в банківській маркетинговій кампанії. Застосунок приймає на вхід дані про клієнта та кампанію (без 'duration') і **прогнозує ймовірність підписки на депозит**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та пайплайн попереднього оброблення (кодувальники, скейлер).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для ключових ознак: `st.number_input` для 'age', 'campaign', 'pdays', 'previous'; `st.selectbox` для 'job', 'marital', 'education', 'contact', 'month', 'poutcome'.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте збережений пайплайн обробки** (кодування, масштабування). Переконайтесь, що порядок ознак правильний.\n",
    "    6.  *Прогнозування:* Отримайте **ймовірність підписки (клас 'yes')** за допомогою `model.predict_proba()[:, 1]`.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану ймовірність підписки. Можна додати інтерпретацію (наприклад, \"Висока/Низька ймовірність підписки\").\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt` (включно з `imblearn`, якщо використовувався).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально), `imblearn` (рекомендовано).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіли ознак проти 'y', **діаграма важливості ознак**, матриця плутанини, ROC-крива, ***PR-крива (обов'язково!)***.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**для класу 'yes'**), AUC-ROC, ***AUC-PR (дуже важливо через дисбаланс)***.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Адресуйте сильний дисбаланс класів*** за допомогою зважування або технік семплінгу (`imblearn`).\n",
    "*   **Обов'язково обговоріть та обґрунтуйте рішення щодо ознаки 'duration'.** Для реалістичного прогнозування її краще видалити.\n",
    "*   **AUC-PR є кращою метрикою**, ніж AUC-ROC, для оцінки моделей на сильно незбалансованих даних.\n",
    "*   Аналіз важливості ознак допоможе зрозуміти, які фактори найбільше впливають на рішення клієнтів.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 46: Інтелектуальна інформаційна система для прогнозування цін на авокадо\n",
    "\n",
    "**Набір даних:** Avocado Prices ([https://www.kaggle.com/datasets/neuromusic/avocado-prices](https://www.kaggle.com/datasets/neuromusic/avocado-prices) - `avocado.csv`)\n",
    "\n",
    "**Мета:** **Прогнозувати середню ціну авокадо ('AveragePrice')** на основі дати, типу (звичайний/органічний), загального обсягу продажів ('Total Volume'), обсягів за розмірами (4046, 4225, 4770), загальної кількості мішків ('Total Bags'), та регіону (_Регресія / Часові ряди_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження Даних та Аналіз Часових Рядів:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `avocado.csv`.\n",
    "    *   1.2. *Підготовка даних:*\n",
    "        *   Перетворіть 'Date' на datetime об'єкт. Відсортуйте дані за датою.\n",
    "        *   Видаліть стовпець 'Unnamed: 0', якщо він є.\n",
    "        *   Перевірте типи даних, пропуски (зазвичай немає).\n",
    "    *   1.3. *EDA:*\n",
    "        *   **Візуалізуйте часові ряди 'AveragePrice'**:\n",
    "            *   Загальний тренд.\n",
    "            *   **Окремо для типу 'conventional' та 'organic'**. Очікується значна різниця в ціні.\n",
    "            *   **Окремо для кількох великих регіонів ('region')**.\n",
    "        *   Дослідіть розподіл 'AveragePrice'.\n",
    "        *   Проаналізуйте **кореляцію** між обсягами продажів ('Total Volume', 'Total Bags' тощо) та 'AveragePrice'. Часто спостерігається негативна кореляція.\n",
    "        *   Дослідіть сезонність цін (наприклад, за місяцями).\n",
    "\n",
    "2.  **Інженерія Ознак та Базова Регресія:**\n",
    "    *   2.1. *Інженерія Ознак:*\n",
    "        *   Витягніть **часові ознаки** з 'Date': місяць, рік, тиждень року, день року.\n",
    "    *   2.2. *Оброблення Категоріальних Ознак:*\n",
    "        *   Закодуйте 'type' (conventional/organic) - можна Label Encoding (0/1) або OHE.\n",
    "        *   Закодуйте 'region' (висока кардинальність!). Використовуйте One-Hot Encoding (можливо, лише для підмножини даних/регіонів) або Target Encoding.\n",
    "        *   Закодуйте 'year' як категоріальну ознаку (OHE), якщо років небагато, або залиште числовою.\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (використовуйте **часовий поділ**, наприклад, останній рік для тестування). Масштабуйте числові ознаки (обсяги, часові ознаки, якщо числові).\n",
    "    *   2.4. *Навчання базових регресійних моделей:* Навчіть Linear Regression, Ridge, Lasso.\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на тестовій вибірці за допомогою MAE, MSE, RMSE, R-squared.\n",
    "\n",
    "3.  **Розширене Моделювання (Часові Ряди / ML) та Розгортання:**\n",
    "    *   3.1. *Реалізація моделей на основі дерев:* Навчіть Random Forest Regressor, XGBoost Regressor, LightGBM Regressor на табличних даних з інженерними ознаками.\n",
    "    *   3.2. *Моделювання Часових Рядів (Опціонально):*\n",
    "        *   Спробуйте моделювати 'AveragePrice' як **часовий ряд окремо для кожного регіону та типу авокадо** (або для агрегованих даних).\n",
    "        *   Використовуйте моделі SARIMA(X), де X можуть бути обсяги продажів або часові індикатори.\n",
    "    *   3.3. *Порівняння Підходів:* Порівняйте продуктивність табличних ML моделей з підходами на основі часових рядів (якщо реалізовано).\n",
    "    *   3.4. *Аналіз Важливості Ознак (для ML):* Визначте, які фактори (тип, регіон, обсяги, час) найбільше впливають на ціну авокадо.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування середньої ціни авокадо**. Застосунок дає змогу користувачеві вибрати дату, тип авокадо, регіон та (опціонально) ввести обсяги продажів, щоб отримати **прогноз ціни**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та пайплайн обробки (кодувальники, скейлер). Завантажте списки регіонів/типів.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення: `st.date_input` для дати; `st.selectbox` для типу ('conventional'/'organic') та регіону; `st.number_input` для обсягів (Total Volume, Total Bags тощо), якщо модель їх використовує.\n",
    "    5.  *Оброблення вводу та генерація ознак:*\n",
    "        *   Зберіть дані.\n",
    "        *   **Витягніть часові ознаки** (місяць, рік, тиждень) з вибраної дати.\n",
    "        *   **Застосуйте збережений пайплайн обробки** (кодування категоріальних, масштабування числових).\n",
    "    6.  *Прогнозування:* Зробіть прогноз 'AveragePrice' (`model.predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану середню ціну авокадо.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, списки регіонів, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально), `statsmodels` (опціонально).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Часовий ряд цін (загальний, за типом/регіоном)**, діаграми розсіювання (обсяг vs ціна), **діаграма важливості ознак**.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, R-squared.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Розгляньте **моделювання 'conventional' та 'organic' авокадо окремо** або включіть 'type' як дуже важливу ознаку.\n",
    "*   Обробіть **регіональні ефекти** (кодування 'region' або побудова окремих моделей для регіонів). Target Encoding може бути корисним для 'region'.\n",
    "*   Порівняйте підхід з використанням **табличних ML моделей** (використовуючи час як ознаку) з класичними **моделями часових рядів** (ARIMA/SARIMA).\n",
    "*   Використовуйте **часовий поділ** даних для валідації.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 47: Система виявлення вторгнень з використанням CICIDS2017\n",
    "\n",
    "**Набір даних:** CICIDS2017 Dataset ([https://www.unb.ca/cic/datasets/ids-2017.html](https://www.unb.ca/cic/datasets/ids-2017.html)). Набір складається з даних захоплення мережевого трафіку (.pcap) та оброблених потоків з витягнутими ознаками у CSV файлах (окремі файли для кожного дня тижня, що містять різні атаки). Kaggle Mirror (об'єднаний та очищений): ([https://www.kaggle.com/datasets/cicdataset/cicids2017](https://www.kaggle.com/datasets/cicdataset/cicids2017)) - **Рекомендується використовувати версію з Kaggle** для спрощення обробки. ***Дуже великий! Потребує вибірки або роботи з частиною даних.***\n",
    "\n",
    "**Мета:** **Виявляти мережеві вторгнення/атаки**, класифікуючи мережеві потоки як **'BENIGN'** або за **типом атаки** (DoS, PortScan, DDoS, Web Attack тощо) на основі статистичних ознак потоку (_Багатокласова класифікація з сильним дисбалансом_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Вибірка Даних, Завантаження та Попереднє Оброблення:**\n",
    "    *   1.1. *Вибірка та Завантаження:*\n",
    "        *   **Обов'язково зробіть вибірку!** Повний набір даних дуже великий. Або візьміть невеликий відсоток рядків, або працюйте з даними одного-двох днів (файлів).\n",
    "        *   Завантажте вибрані дані (CSV).\n",
    "    *   1.2. *Аналіз та Очищення:*\n",
    "        *   Огляньте велику кількість ознак (>70), що описують тривалість потоку, кількість пакетів/байтів, прапори TCP, статистику розмірів пакетів тощо.\n",
    "        *   **Обробіть нескінченні значення та NaN:** Часто виникають через ділення на нуль при розрахунку ознак. Замініть `np.inf` на велике число або NaN, а потім імпутуйте NaN (наприклад, 0 або медіаною).\n",
    "        *   Видаліть стовпці з однаковим значенням або низькою дисперсією.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('Label'):*\n",
    "        *   Дослідіть **розподіл класів** ('BENIGN' та різні типи атак). Зверніть увагу на **надзвичайний дисбаланс** (дуже багато 'BENIGN', деякі типи атак дуже рідкісні).\n",
    "    *   1.4. *Оброблення Ознак:*\n",
    "        *   Переконайтесь, що всі ознаки числові.\n",
    "        *   ***Масштабування/Нормалізація:*** Застосуйте `StandardScaler` або `MinMaxScaler` до всіх ознак. Це **дуже важливо** для багатьох класифікаторів.\n",
    "\n",
    "2.  **Базова Класифікація/Виявлення Аномалій:**\n",
    "    *   2.1. *Спрощення Завдання (Опціонально):* Для початку можна розглянути **бінарну класифікацію** ('BENIGN' vs 'Attack') або класифікацію лише **найчастіших типів атак**.\n",
    "    *   2.2. *Підготовка даних:* Розділіть дані на тренувальну/тестову вибірки (з стратифікацією).\n",
    "    *   2.3. *Навчання базових моделей:* Навчіть Decision Tree, K-NN (може бути повільним), Gaussian Naive Bayes. **Врахуйте сильний дисбаланс** (використовуйте `class_weight='balanced'`, `imblearn` техніки семплінгу на тренувальній вибірці - SMOTE може генерувати багато даних, **RandomUnderSampler** може бути швидшим варіантом).\n",
    "    *   2.4. *Оцінка:* Оцініть моделі, використовуючи метрики, **чутливі до дисбалансу**:\n",
    "        *   Precision, Recall, $F_1$-score (**для кожного класу атаки**).\n",
    "        *   **Weighted/Macro $F_1$-score**.\n",
    "        *   AUC-PR / AUC-ROC (з обережністю щодо інтерпретації AUC-ROC при сильному дисбалансі).\n",
    "        *   Матриця плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Оптимізація Продуктивності:**\n",
    "    *   3.1. *Реалізація Ефективних Класифікаторів:* Навчіть **Random Forest, LightGBM, XGBoost**. Ці моделі зазвичай найкраще працюють на таких табличних даних. Використовуйте параметри для роботи з дисбалансом (`scale_pos_weight`, `is_unbalance=True` в LightGBM).\n",
    "    *   3.2. *Вибір Ознак (Опціонально):* Через велику кількість ознак, розгляньте вибір найважливіших (на основі RF/LGBM feature importance) для спрощення моделі та потенційного покращення узагальнення.\n",
    "    *   3.3. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей, максимізуючи Weighted/Macro F1 або іншу релевантну метрику.\n",
    "    *   3.4. *Порівняння Моделей та Стратегій:* Порівняйте продуктивність різних моделей та методів обробки дисбалансу.\n",
    "    *   3.5. *Аналіз Важливості Ознак:* Визначте, які характеристики мережевих потоків є ключовими для виявлення різних типів атак.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки (скейлер, список ознак).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для класифікації мережевих потоків**. Застосунок приймає на вхід (можливо, спрощений набір) ознак потоку та **прогнозує, чи є потік безпечним ('BENIGN') або належить до певного типу атаки**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `lightgbm`/`xgboost`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та пайплайн обробки (скейлер, можливо, список вибраних ознак).\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input`) для **ключових ознак потоку**. Оскільки ознак дуже багато, **необхідно вибрати підмножину** найважливіших або створити спрощений інтерфейс, де користувач вибирає типовий сценарій, а ознаки генеруються за замовчуванням.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте збережений пайплайн обробки** (масштабування, вибір ознак).\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`). Перетворіть мітку на назву класу ('BENIGN', 'DoS' тощо).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований клас мережевого потоку.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt` (включно з `imblearn`, якщо використовувався).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (рекомендовано), `imblearn` (рекомендовано).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл класів атак, розподіли ключових ознак (BENIGN vs Attacks), **матриця плутанини** (дуже важлива для аналізу помилок між класами), **діаграма важливості ознак**.\n",
    "*   **Метрики оцінки:** Accuracy (з великою обережністю), Precision, Recall, F1 (**Macro/Weighted та для кожного класу атаки**). **Зосередьтеся на Recall для класів атак**.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Вибірка даних є необхідною***.\n",
    "*   **Оброблення NaN та нескінченних значень**, а також **масштабування ознак** є критично важливими кроками попереднього оброблення.\n",
    "*   **Надзвичайний дисбаланс класів** потребує спеціальних методів (зважування, семплінг) та метрик оцінки (F1, Recall для міноритарних класів, PR-криві).\n",
    "*   **Ефективність моделі (швидкість навчання/прогнозування)** є важливою через потенційно великий обсяг даних у реальних системах виявлення вторгнень (LightGBM часто є хорошим вибором).\n",
    "*   Створення зручного інтерфейсу для введення ~70 ознак у вебзастосунку є **непрактичним**. Потрібно або значно скоротити кількість ознак (на основі аналізу важливості), або симулювати введення.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 48: Інтелектуальна інформаційна система для прогнозування кредитного ризику (German Credit Data)\n",
    "\n",
    "**Набір даних:** German Credit Data (Statlog) (UCI: [https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)](https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data))). Набір містить 1000 записів з 20 ознаками (7 числових, 13 категоріальних). Категоріальні ознаки закодовані (A11, A32 тощо). Kaggle Mirror: ([https://www.kaggle.com/datasets/uciml/german-credit](https://www.kaggle.com/datasets/uciml/german-credit)) - може мати вже розкодовані назви стовпців. Цільова змінна 'Risk' (1 - Good, 2 - Bad).\n",
    "\n",
    "**Мета:** **Класифікувати заявників на кредит** як таких, що мають **хороший ('Good', мітка 1) чи поганий ('Bad', мітка 2) кредитний ризик**, на основі їхніх атрибутів (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження Даних та Попереднє Оброблення:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте дані (файл `german.data` з UCI або CSV з Kaggle). Якщо з UCI, **призначте назви стовпців** відповідно до файлу опису (`german.doc`).\n",
    "    *   1.2. *Аналіз та Розуміння Ознак:*\n",
    "        *   Огляньте ознаки. **Уважно прочитайте `german.doc`**, щоб зрозуміти значення кодів для категоріальних ознак (наприклад, A11='<0 DM', A12='0<= ... <200 DM' для Status of existing checking account; A32='critical account/other credits existing' для Credit history тощо).\n",
    "        *   Перевірте типи даних, наявність пропусків (зазвичай немає).\n",
    "    *   1.3. *Оброблення Цільової Змінної:* Перетворіть 'Risk' (1=Good, 2=Bad) на **0 (Bad) та 1 (Good)** або навпаки (0=Good, 1=Bad). **Стандартно, клас \"ризику\" (Bad) розглядають як позитивний (1).** Перевірте **баланс класів** (зазвичай близько 70% Good, 30% Bad).\n",
    "    *   1.4. *Оброблення Категоріальних Ознак:*\n",
    "        *   **Розкодуйте значення:** Замініть коди (A11, A32) на зрозумілі текстові значення (опціонально, але покращує інтерпретацію EDA).\n",
    "        *   Визначте порядкові та номінальні категоріальні ознаки.\n",
    "        *   Застосуйте **One-Hot Encoding** до номінальних ознак. Для порядкових (наприклад, 'Credit history') можна спробувати Ordinal Encoding, але OHE є безпечнішим.\n",
    "    *   1.5. *EDA:*\n",
    "        *   Дослідіть взаємозв'язок між ризиком ('Risk') та ключовими ознаками: 'Duration', 'Credit amount', 'Age', 'Status of existing checking account', 'Credit history', 'Purpose', 'Savings account/bonds', 'Present employment since'. Використовуйте `countplot`, `histplot`, `boxplot` з `hue='Risk'`.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Інженерія Ознак (Опціонально):* Можна спробувати створити співвідношення (наприклад, Credit amount / Duration).\n",
    "    *   2.2. *Масштабування Числових Ознак:* Застосуйте `StandardScaler` до числових ознак ('Duration', 'Credit amount', 'Installment rate in percentage of disposable income', 'Present residence since', 'Age', 'Number of existing credits at this bank', 'Number of people being liable to provide maintenance for').\n",
    "    *   2.3. *Підготовка даних:* Розділіть на тренувальну/тестову вибірки (з стратифікацією за 'Risk').\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, Decision Tree, K-NN, GaussianNB. Врахуйте дисбаланс (`class_weight='balanced'` або семплінг `imblearn`).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі. Зверніть увагу на **Recall для класу 'Bad' (1)** (щоб не пропустити ризикованих клієнтів) та **AUC-ROC/AUC-PR**. Використовуйте $F_1$-score (для класу 'Bad'), Accuracy, матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Навчання з Урахуванням Витрат:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM. Використовуйте `scale_pos_weight` або `is_unbalance`.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте всі моделі за ключовими метриками.\n",
    "    *   3.4. ***Навчання з Урахуванням Витрат (Cost-Sensitive Learning) (Опціонально, просунуто):***\n",
    "        *   Визначте **матрицю витрат**: Штраф за помилкову класифікацію 'Bad' як 'Good' (False Negative - видача кредиту ризикованому клієнту) зазвичай **набагато вищий**, ніж штраф за класифікацію 'Good' як 'Bad' (False Positive - відмова надійному клієнту). Наприклад, FN може коштувати 5 одиниць, а FP - 1 одиницю.\n",
    "        *   Деякі алгоритми (`scikit-learn`) не підтримують матрицю витрат напряму. Можна **змінити поріг класифікації** `predict_proba` для оптимізації за загальними витратами.\n",
    "        *   Можна **модифікувати ваги класів (`class_weight`)**, щоб відобразити співвідношення витрат.\n",
    "        *   Існують спеціалізовані бібліотеки або техніки для cost-sensitive learning.\n",
    "        *   *Мета:* Знайти модель або поріг, що мінімізує очікувані витрати на тестовій вибірці.\n",
    "    *   3.5. *Аналіз Важливості Ознак:* Визначте, які характеристики заявника є найбільш важливими для оцінки кредитного ризику.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для оцінки кредитного ризику**. Застосунок приймає на вхід дані заявника на кредит та **прогнозує його кредитний ризик** (Хороший/Поганий), можливо, з урахуванням вартості помилок.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте модель та пайплайн обробки (кодувальники категоріальних ознак, скейлер).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для ключових ознак: `st.number_input` для 'Duration', 'Credit amount', 'Age'; `st.selectbox` для 'Checking account status', 'Credit history', 'Purpose', 'Savings account', 'Employment duration' тощо (використовуйте зрозумілі текстові значення, а не коди Axx).\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Зберіть дані.\n",
    "        *   **Перетворіть текстові значення категорій назад у формат, очікуваний пайплайном** (або пайплайн має приймати текстові значення).\n",
    "        *   **Застосуйте збережений пайплайн обробки** (кодування, масштабування).\n",
    "    6.  *Прогнозування:* Отримайте ймовірність класу 'Bad' (`predict_proba()[:, class_index_for_bad]`). Зробіть прогноз класу (`predict()`). Якщо використовується cost-sensitive поріг, застосуйте його до ймовірності.\n",
    "    7.  *Відображення результату:* Покажіть прогнозований ризик (\"Хороший\"/\"Поганий\") та/або ймовірність поганого ризику. Якщо реалізовано cost-sensitive, поясніть рішення.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm` (опціонально), `imblearn` (опціонально).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Розподіли ознак проти Risk**, **діаграма важливості ознак**, матриця плутанини, ROC-крива, PR-крива.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, **Recall (для 'Bad' ризику)**, **$F_1$-score (для 'Bad' ризику)**, AUC-ROC, AUC-PR. Розгляньте **метрики на основі витрат**, якщо реалізовано cost-sensitive learning.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Правильна інтерпретація та кодування категоріальних ознак*** (згідно `german.doc`) є ключовим першим кроком.\n",
    "*   Врахуйте **дисбаланс класів**.\n",
    "*   **Навчання з урахуванням витрат (Cost-Sensitive Learning)** додає значну практичну цінність проєкту, оскільки ціна помилок у кредитному скорингу асиметрична. Це може бути реалізовано через зміну порогу класифікації або використання спеціалізованих технік/бібліотек.\n",
    "*   Зосередьтеся на метриках, що відображають здатність моделі виявляти ризикованих клієнтів (Recall для 'Bad').\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 49: Інтелектуальна інформаційна система для розпізнавання людської активності за допомогою даних смартфона\n",
    "\n",
    "**Набір даних:** Human Activity Recognition Using Smartphones (UCI) ([https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)). Містить попередньо оброблені дані з акселерометра та гіроскопа смартфонів для 6 видів активності. Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/uciml/human-activity-recognition-with-smartphones](https://www.kaggle.com/datasets/uciml/human-activity-recognition-with-smartphones)). Дані розділені на тренувальний та тестовий набори.\n",
    "\n",
    "**Мета:** **Класифікувати людську активність** (1-WALKING, 2-WALKING_UPSTAIRS, 3-WALKING_DOWNSTAIRS, 4-SITTING, 5-STANDING, 6-LAYING) на основі 561 ознаки, отриманої з сенсорів смартфона (_Багатокласова класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних та Дослідження Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте дані з папок 'train' та 'test':\n",
    "        *   `train/X_train.txt`: Тренувальні ознаки (розділені пробілами).\n",
    "        *   `train/y_train.txt`: Тренувальні мітки активності (1-6).\n",
    "        *   `test/X_test.txt`: Тестові ознаки.\n",
    "        *   `test/y_test.txt`: Тестові мітки активності.\n",
    "        *   `features.txt`: Назви для 561 ознаки.\n",
    "        *   `activity_labels.txt`: Мапінг міток 1-6 на назви активностей.\n",
    "    *   1.2. *Підготовка DataFrame:* Завантажте дані в DataFrame `pandas`, **призначивши назви стовпців** з `features.txt`. Додайте стовпець з мітками активності.\n",
    "    *   1.3. *Аналіз даних:* Перевірте розмірність, типи даних, наявність пропусків (зазвичай немає).\n",
    "    *   1.4. *Аналіз Цільової Змінної:* Дослідіть **розподіл 6 класів активності** в тренувальному наборі. Чи збалансований він?\n",
    "    *   1.5. *Аналіз Ознак:* Ознаки є результатом складної попереднього оброблення (середні, стандартні відхилення, частотні компоненти тощо). EDA може бути складним. Можна проаналізувати кореляції між деякими ознаками або візуалізувати розподіли кількох вибраних ознак для різних активностей.\n",
    "\n",
    "2.  **Зниження Розмірності та Базова Класифікація:**\n",
    "    *   2.1. *Масштабування Ознак:* ***Застосуйте `StandardScaler`*** до всіх 561 ознак (навчіть на `X_train`, застосуйте до `X_train` та `X_test`).\n",
    "    *   2.2. *Зниження Розмірності (Опціонально):*\n",
    "        *   **PCA:** Застосуйте PCA до масштабованих тренувальних даних. Проаналізуйте кумулятивну пояснену дисперсію. Виберіть кількість компонент, що зберігає більшу частину інформації (наприклад, 95-99%). Трансформуйте `X_train` та `X_test`.\n",
    "        *   **t-SNE (для візуалізації):** Застосуйте t-SNE (з `n_components=2`) до **невеликої підмножини** тренувальних даних для візуалізації розділення класів у 2D.\n",
    "    *   2.3. *Візуалізація (якщо застосовано PCA/t-SNE):* Побудуйте 2D діаграму розсіювання результатів PCA/t-SNE, **забарвлюючи точки за класом активності**.\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, SVM (LinearSVC або з RBF ядром), K-NN на:\n",
    "        *   а) Оригінальних масштабованих 561 ознаках.\n",
    "        *   б) Зменшених за допомогою PCA ознаках (якщо застосовано).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на тестовому наборі (`X_test`, `y_test`). Використовуйте Accuracy, **Classification Report** (Precision, Recall, F1 для кожної активності), Macro/Weighted F1, матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Порівняння:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, XGBoost, LightGBM на оригінальних масштабованих ознаках та/або PCA-зменшених ознаках.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей.\n",
    "    *   3.3. ***Порівняння Продуктивності:***\n",
    "        *   Порівняйте точність та швидкість моделей, навчених на **оригінальних даних проти PCA-зменшених даних**. Чи сильно зниження розмірності вплинуло на точність? Наскільки пришвидшило навчання/прогнозування?\n",
    "        *   Порівняйте всі моделі між собою.\n",
    "    *   3.4. *Аналіз Помилок:* Проаналізуйте матрицю плутанини найкращої моделі. Які активності найчастіше плутаються (наприклад, SITTING та STANDING, або різні типи ходьби)?\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та скейлер (і PCA об'єкт, якщо використовується).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для розпізнавання людської активності**. Застосунок може:\n",
    "*   (Простіший варіант) Приймати на вхід значення для **підмножини ключових ознак** (вибраних на основі важливості) та прогнозувати активність.\n",
    "*   (Складніший варіант) Демонструвати класифікацію для прикладу з тестового набору або візуалізувати PCA/t-SNE ембединги.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, PCA (якщо потрібно), мапінг міток на назви активностей.\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   **Варіант 1 (Введення ознак):** Створіть поля введення (`st.number_input`) для **невеликої кількості найважливіших ознак**. Це складно, бо ознак багато і вони неінтуїтивні.\n",
    "        *   **Варіант 2 (Демонстрація):** Дозвольте вибрати приклад з тестового набору та показати його ознаки та прогноз моделі.\n",
    "        *   **Варіант 3 (Візуалізація):** Відобразіть інтерактивний PCA/t-SNE графік.\n",
    "    5.  *Оброблення вводу (для Варіанту 1):* Зберіть дані. Застосуйте скейлер (та PCA, якщо модель на зменшених даних).\n",
    "    6.  *Прогнозування:* Зробіть прогноз числової мітки класу. **Перетворіть мітку на назву активності**.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану активність.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, PCA, мапінг активностей, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **PCA пояснена дисперсія**, **PCA/t-SNE scatter plot (забарвлений за активністю)**, **матриця плутанини**, діаграма важливості ознак (якщо використовуються відповідні моделі).\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**macro/weighted та для кожної активності**), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Масштабування ознак (`StandardScaler`) є стандартним та необхідним кроком***.\n",
    "*   **PCA є корисним інструментом** для зменшення розмірності (прискорення навчання) та візуалізації даних. Порівняння моделей на повних та зменшених даних є цікавим аналізом.\n",
    "*   t-SNE чудово підходить для **візуалізації розділення класів**, але використовуйте його на підмножині даних.\n",
    "*   Аналіз **матриці плутанини** важливий для розуміння, які активності модель плутає.\n",
    "*   Створення інтерфейсу для введення 561 ознаки нереалістичне; зосередьтеся на демонстрації або візуалізації.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 50: Аналіз трендових відео на YouTube\n",
    "\n",
    "**Набір даних:** Trending YouTube Video Statistics (наприклад, для США `USvideos.csv`) ([https://www.kaggle.com/datasets/datasnaek/youtube-new](https://www.kaggle.com/datasets/datasnaek/youtube-new)). Містить дані про відео, що потрапляли в тренди YouTube у певній країні протягом певного періоду.\n",
    "\n",
    "**Мета:** Проаналізувати фактори, пов'язані з популярністю відео, та побудувати модель для:\n",
    "*   А) **Прогнозування кількості переглядів ('views') або лайків ('likes')**.\n",
    "*   Б) **Класифікації відео за категоріями ('category_id')**.\n",
    "(_Регресія або Класифікація_). **Рекомендується вибрати ОДНУ з цих цілей.**\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних, Очищення та EDA Тексту/Часу:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте CSV файл для вибраної країни (наприклад, `USvideos.csv`) та відповідний JSON файл з мапінгом `category_id` на назви категорій (наприклад, `US_category_id.json`).\n",
    "    *   1.2. *Аналіз та Очищення:*\n",
    "        *   Огляньте стовпці: 'video_id', 'trending_date', 'title', 'channel_title', 'category_id', 'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed', 'description'.\n",
    "        *   **Оброблення Дат/Часу:** Перетворіть 'trending_date' та 'publish_time' на datetime об'єкти. Створіть ознаки: час публікації (година), день тижня публікації, **час до потрапляння в тренди** (`trending_date` - `publish_date`).\n",
    "        *   **Оброблення Категорій:** Об'єднайте дані з JSON файлом, щоб додати назву категорії ('category_name').\n",
    "        *   **Оброблення Тексту:** Очистіть 'title', 'tags', 'description' (нижній регістр, видалення пунктуації/спецсимволів). Розпарсіть 'tags' (розділені '|').\n",
    "        *   Обробіть пропуски (особливо в 'description').\n",
    "        *   Обробіть булеві ознаки ('comments_disabled' тощо).\n",
    "    *   1.3. *Аналіз Цільової Змінної:*\n",
    "        *   (Регресія) **Візуалізуйте розподіли 'views', 'likes', 'dislikes', 'comment_count'**. Вони будуть **сильно асиметричними**. Застосуйте **логарифмічне перетворення** (`np.log1p`) для моделювання.\n",
    "        *   (Класифікація) Дослідіть **розподіл відео за категоріями ('category_name')**.\n",
    "    *   1.4. *EDA:*\n",
    "        *   Проаналізуйте **тренди популярності** (views/likes) у часі (за датою потрапляння в тренди або датою публікації).\n",
    "        *   Дослідіть середні показники популярності (views, likes) **для різних категорій**.\n",
    "        *   Проаналізуйте кореляції між 'views', 'likes', 'dislikes', 'comment_count'.\n",
    "        *   Дослідіть зв'язок між часом публікації/потрапляння в тренди та популярністю.\n",
    "        *   Проаналізуйте характеристики тексту (довжина заголовка/опису, кількість тегів) та їх зв'язок з популярністю/категорією.\n",
    "\n",
    "2.  **Моделювання (Виберіть ОДИН підхід):**\n",
    "\n",
    "    **2.А. Прогнозування Популярності ('views'/'likes') (Регресія):**\n",
    "    *   2.А.1. *Інженерія Ознак:*\n",
    "        *   **Текстові Ознаки:** Використайте TF-IDF на 'title', 'tags', 'description' (окремо або об'єднано). Через високу розмірність TF-IDF, можливо, знадобиться зменшення розмірності (PCA, TruncatedSVD) або використання моделей, стійких до великої кількості ознак (Ridge, Lasso, LightGBM).\n",
    "        *   **Часові Ознаки:** Година/день тижня/місяць публікації, час до тренду.\n",
    "        *   **Інші:** Кількість тегів, довжина заголовка/опису, категорія (закодована), булеві ознаки.\n",
    "    *   2.А.2. *Підготовка даних:* Розділіть дані на тренувальну/тестову вибірки (можна за часом публікації). Масштабуйте числові ознаки. Використовуйте **log(views)** або **log(likes)** як ціль.\n",
    "    *   2.А.3. *Навчання моделей:* Навчіть Ridge, Lasso (добре для відбору ознак з TF-IDF), Random Forest, XGBoost, LightGBM.\n",
    "    *   2.А.4. *Оцінка:* Оцініть за допомогою MAE, MSE, RMSE, R-squared (на log-шкалі). Розгляньте метрики на оригінальній шкалі після зворотного перетворення (`np.expm1`).\n",
    "\n",
    "    **2.Б. Класифікація за Категоріями ('category_id'):**\n",
    "    *   2.Б.1. *Інженерія Ознак:* Подібно до регресії, використовуйте текстові ознаки (TF-IDF), часові, кількісні (довжина, кількість тегів), булеві. **Не використовуйте 'views'/'likes' як ознаки**, якщо хочете класифікувати *до* того, як відео стало популярним.\n",
    "    *   2.Б.2. *Підготовка даних:* Розділіть дані на тренувальну/тестову вибірки (з стратифікацією за категорією). Масштабуйте числові ознаки.\n",
    "    *   2.Б.3. *Навчання моделей:* Навчіть Logistic Regression, MultinomialNB (на TF-IDF), LinearSVC, Random Forest, LightGBM.\n",
    "    *   2.Б.4. *Оцінка:* Оцініть за допомогою Accuracy, Weighted/Macro $F_1$-score, Classification Report, матриці плутанини.\n",
    "\n",
    "3.  **Аналіз та Розгортання:**\n",
    "    *   3.1. *Аналіз Важливості Ознак:* Для найкращої моделі визначте, які фактори (текст заголовка/тегів, канал, час публікації, категорія) найбільше впливають на популярність або визначають категорію відео.\n",
    "    *   3.2. *Порівняння Підходів (якщо робили обидва):* Обговоріть, яке завдання (регресія чи класифікація) є більш доцільним або цікавим для цих даних.\n",
    "    *   3.3. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки (векторизатор, скейлер, кодувальник категорій).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який (залежно від вибраної мети):\n",
    "*   А) **Прогнозує очікувану кількість переглядів/лайків** для відео на основі його метаданих (заголовок, теги, категорія, час публікації).\n",
    "*   Б) **Прогнозує категорію відео** на основі його метаданих.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `nltk`, `re`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте модель, векторизатор TF-IDF, скейлер, кодувальник категорій, функцію очищення тексту.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення: `st.text_input` для заголовка, каналу; `st.text_area` для опису; `st.multiselect` або `st.text_input` для тегів; `st.selectbox` для категорії (якщо не прогнозується); `st.time_input`, `st.date_input` для часу/дати публікації.\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Зберіть дані.\n",
    "        *   **Очистіть текст**.\n",
    "        *   **Застосуйте збережений пайплайн попереднього оброблення** (векторизація тексту, кодування категорій, масштабування числових/часових ознак).\n",
    "    6.  *Прогнозування:*\n",
    "        *   (Регресія) Зробіть прогноз log(views/likes). **Застосуйте `np.expm1()`**.\n",
    "        *   (Класифікація) Зробіть прогноз категорії.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану кількість переглядів/лайків або прогнозовану категорію.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки (з `nltk`).\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt` (з `nltk`). Налаштуйте `nltk`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `nltk`, `re`, `xgboost`, `lightgbm`, `wordcloud`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `nltk`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл views/likes (log), тренди у часі, **показники популярності за категоріями**, **діаграма важливості ознак** (регресія), матриця плутанини (класифікація).\n",
    "*   **Метрики оцінки:**\n",
    "    *   *Регресія:* MAE, MSE, RMSE, R-squared (на log або оригінальній шкалі).\n",
    "    *   *Класифікація:* Accuracy, F1 (Weighted/Macro).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Інженерія текстових ознак (TF-IDF) є дуже важливою*** для обох завдань.\n",
    "*   **Логарифмічне перетворення** 'views'/'likes' необхідне для регресії.\n",
    "*   **Чітко виберіть ОДНУ основну мету** (регресія АБО класифікація).\n",
    "*   Аналіз важливості ознак покаже, що найбільше впливає на успіх відео на YouTube (контент, канал, час публікації тощо).\n",
    "*   Дані містять записи для одного `video_id` на різні `trending_date`. Можливо, знадобиться агрегація або вибір останнього запису для кожного відео, якщо моделюється кінцева популярність.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 51: Інтелектуальна інформаційна система для класифікування листя\n",
    "\n",
    "**Набір даних:** Leaf Classification ([https://www.kaggle.com/competitions/leaf-classification/data](https://www.kaggle.com/competitions/leaf-classification/data)). _Потрібен акаунт Kaggle_. Набір даних містить попередньо витягнуті ознаки форми, краю та текстури для різних видів листя.\n",
    "\n",
    "**Мета:** **Класифікувати види листя ('species')** на основі 192 інженерних ознак (_Багатокласова класифікація_). **Основна метрика оцінки - Log Loss**.\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних та Дослідження Ознак:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `train.csv` та `test.csv`. Також може бути корисним `sample_submission.csv`.\n",
    "    *   1.2. *Аналіз даних:*\n",
    "        *   Огляньте стовпці в `train.csv`: 'id', 'species' (цільова змінна), 'margin1'...'margin64', 'shape1'...'shape64', 'texture1'...'texture64'.\n",
    "        *   Перевірте типи даних, пропуски (зазвичай немає).\n",
    "        *   В `test.csv` відсутній стовпець 'species'.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('species'):*\n",
    "        *   Дослідіть **кількість унікальних видів листя**.\n",
    "        *   Перевірте **розподіл класів**. Чи є він збалансованим?\n",
    "    *   1.4. *Оброблення Цільової Змінної:* Використайте `LabelEncoder` з `sklearn.preprocessing` для **перетворення назв видів ('species') на числові мітки** (0, 1, 2, ...). Збережіть енкодер для зворотного перетворення назв.\n",
    "    *   1.5. *Аналіз Ознак:*\n",
    "        *   Ознаки вже є числовими, але мають різну природу (край, форма, текстура).\n",
    "        *   Проаналізуйте кореляції між ознаками. Можлива висока кореляція всередині груп (margin, shape, texture).\n",
    "    *   1.6. *Масштабування Ознак:* ***Застосуйте `StandardScaler`*** до всіх 192 вхідних ознак.\n",
    "\n",
    "2.  **Зниження Розмірності та Базове Моделювання:**\n",
    "    *   2.1. *Підготовка даних:* Розділіть тренувальні дані на власне тренувальну та валідаційну вибірки (з стратифікацією за класом). Масштабуйте ознаки (навчіть скейлер на тренувальній, застосуйте до всіх).\n",
    "    *   2.2. *Зниження Розмірності (PCA):*\n",
    "        *   Застосуйте PCA до масштабованих тренувальних даних.\n",
    "        *   Проаналізуйте кумулятивну пояснену дисперсію. Визначте, скільки компонент потрібно для збереження, наприклад, 99% дисперсії. Це може значно зменшити кількість ознак.\n",
    "        *   Трансформуйте тренувальні та валідаційні/тестові дані.\n",
    "    *   2.3. *Візуалізація (Опціонально):* Візуалізуйте дані у 2D просторі перших двох PCA компонент, забарвлюючи точки за класом (видом листя).\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть K-NN, Logistic Regression, SVM (`LinearSVC`) на:\n",
    "        *   а) Оригінальних масштабованих ознаках.\n",
    "        *   б) PCA-зменшених ознаках.\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на валідаційній вибірці. **Основна метрика - Log Loss (`log_loss` з `sklearn.metrics`)**. Також дивіться на Accuracy та $F_1$-score (Weighted/Macro). Порівняйте результати на повних та PCA-даних.\n",
    "\n",
    "3.  **Розширене Моделювання та Підготовка до Подання:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, XGBoost, LightGBM на повних або PCA-зменшених даних (виберіть те, що дало кращі результати в базових моделях або спробуйте обидва).\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей, **мінімізуючи Log Loss** за допомогою крос-валідації.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте всі моделі за Log Loss.\n",
    "    *   3.4. ***Підготовка Файлу для Подання на Kaggle:***\n",
    "        *   Навчіть найкращу модель на **всьому** тренувальному наборі (`train.csv`).\n",
    "        *   **Зробіть прогнози ймовірностей (`predict_proba`)** для кожного класу (виду) на даних з `test.csv` (після масштабування/PCA).\n",
    "        *   Створіть файл `submission.csv` у форматі, вказаному в `sample_submission.csv`. Стовпці: 'id' та по одному стовпцю для **кожного** виду листя з відповідними ймовірностями. Назви стовпців видів повинні відповідати оригінальним назвам (використовуйте `label_encoder.classes_`).\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель, скейлер, PCA (якщо використовується) та LabelEncoder.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для класифікації видів листя**. Застосунок може приймати на вхід значення інженерних ознак (або демонструвати на прикладах) та **прогнозувати вид листя**. Також підготовлено файл `submission.csv` для Kaggle.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, скейлер, PCA (якщо потрібно), LabelEncoder.\n",
    "    4.  *Створення інтерфейсу:* Через велику кількість ознак (192), введення вручну непрактичне. Варіанти:\n",
    "        *   **Демонстрація:** Дозвольте вибрати `id` з тестового набору, завантажте відповідні ознаки та покажіть прогноз.\n",
    "        *   **Візуалізація:** Покажіть PCA візуалізацію.\n",
    "        *   (Дуже спрощено) Створіть поля введення лише для кількох найважливіших ознак (якщо їх вдалося визначити).\n",
    "    5.  *Оброблення вводу:* Якщо дані вводяться/вибираються, застосуйте скейлер (та PCA).\n",
    "    6.  *Прогнозування:* Зробіть прогноз ймовірностей (`predict_proba`). Знайдіть клас з найвищою ймовірністю. **Перетворіть числовий індекс класу на назву виду** (`label_encoder.inverse_transform()`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований вид листя та, можливо, ймовірність.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, PCA, енкодер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** PCA scatter plot (забарвлений за класом), матриця плутанини, діаграма важливості ознак.\n",
    "*   **Метрики оцінки:** Accuracy, ***Log Loss (основна)***, $F_1$-score (Macro/Weighted).\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Масштабування ознак (`StandardScaler`) є вирішальним***.\n",
    "*   **PCA може значно зменшити розмірність**, потенційно покращивши швидкість та навіть точність деяких моделей.\n",
    "*   **Log Loss є основною метрикою**, тому моделі повинні видавати добре калібровані ймовірності.\n",
    "*   Підготовка **файлу для подання на Kaggle** у правильному форматі (з ймовірностями для всіх класів) є важливою частиною завдання.\n",
    "*   Інтерфейс застосунку, ймовірно, буде демонстраційним через велику кількість неінтуїтивних ознак.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 52: Інтелектуальна інформаційна система для прогнозування хвороби Альцгеймера за даними МРТ\n",
    "\n",
    "**Набір даних:** Alzheimer's Dataset (4 class of images) ([https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images](https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images)). Містить МРТ зображення мозку, розділені на 4 категорії: Non Demented, Very Mild Demented, Mild Demented, Moderate Demented. ***Потребує знань з обробки зображень та глибокого навчання (CNN)***.\n",
    "\n",
    "**Мета:** **Класифікувати МРТ зображення** за стадіями хвороби Альцгеймера (_Класифікація зображень_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних Зображень та Дослідження:**\n",
    "    *   1.1. *Організація даних:* Завантажте та розпакуйте дані. Структура зазвичай: папки 'train' та 'test', всередині яких підпапки для кожного з 4 класів ('NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented').\n",
    "    *   1.2. *Завантаження Зображень:* Використовуйте бібліотеки `tensorflow.keras.preprocessing.image_dataset_from_directory` або `torchvision.datasets.ImageFolder` для зручного завантаження зображень та створення наборів даних (train/validation). Вкажіть бажаний розмір зображення (`image_size`, наприклад, (128, 128) або (224, 224)) та розмір батчу (`batch_size`).\n",
    "    *   1.3. *Аналіз даних:* Перевірте кількість зображень у кожному класі в тренувальному та валідаційному наборах. Чи є **дисбаланс**? (Клас 'ModerateDemented' зазвичай має найменше зображень).\n",
    "    *   1.4. *Візуалізація Зображень:* Відобразіть кілька прикладів зображень з кожного класу.\n",
    "    *   1.5. *Попереднє Оброблення/Аугментація:*\n",
    "        *   **Нормалізація:** Значення пікселів зазвичай нормалізують до діапазону [0, 1] (діленням на 255) або стандартизують відповідно до вимог натренованих моделей.\n",
    "        *   **Аугментація Даних (для тренувального набору):** Застосуйте техніки аугментації (випадкові повороти, зсуви, масштабування, віддзеркалення) за допомогою шарів `tensorflow.keras.layers.RandomFlip`, `RandomRotation` тощо або `torchvision.transforms`, щоб штучно збільшити розмір тренувального набору та покращити стійкість моделі.\n",
    "\n",
    "2.  **Базове Моделювання (Проста CNN):**\n",
    "    *   2.1. *Побудова Простої CNN:* Створіть просту згорткову нейронну мережу (CNN) з нуля за допомогою Keras/TensorFlow або PyTorch. Архітектура може включати:\n",
    "        *   Кілька згорткових шарів (`Conv2D`) з активацією ReLU.\n",
    "        *   Шари підвибірки (`MaxPooling2D`).\n",
    "        *   Шар вирівнювання (`Flatten`).\n",
    "        *   Кілька повнозв'язних шарів (`Dense`) з ReLU або іншими активаціями.\n",
    "        *   **Вихідний шар:** `Dense` з **4 нейронами** (за кількістю класів) та активацією **softmax**.\n",
    "    *   2.2. *Компіляція Моделі:* Виберіть оптимізатор (наприклад, `Adam`), функцію втрат (`categorical_crossentropy`, оскільки класи взаємовиключні) та метрики (`accuracy`).\n",
    "    *   2.3. *Навчання Моделі:* Навчіть модель на тренувальних даних (з аугментацією), використовуючи валідаційні дані для моніторингу (`model.fit(..., validation_data=...)`). Використовуйте техніки типу EarlyStopping для запобігання перенавчанню.\n",
    "    *   2.4. *Оцінка:* Оцініть навчену модель на тестовому наборі (якщо він є окремо, або на валідаційному). Використовуйте Accuracy, Classification Report, матрицю плутанини. Візуалізуйте історію навчання (графіки втрат та точності).\n",
    "\n",
    "3.  **Трансферне Навчання (Transfer Learning) та Розгортання:**\n",
    "    *   3.1. ***Використання Натренованої Моделі:*** Це **рекомендований підхід** для задач класифікації зображень, коли даних відносно небагато.\n",
    "        *   Завантажте натреновану на великому наборі даних (наприклад, ImageNet) модель (VGG16, ResNet50, InceptionV3, EfficientNet тощо) з Keras Applications або TorchVision Models, **без верхнього класифікаційного шару** (`include_top=False`).\n",
    "        *   **Заморозьте ваги** завантаженої моделі (щоб не змінювати їх на початку навчання).\n",
    "        *   Додайте **власні класифікаційні шари** зверху: `GlobalAveragePooling2D` (або `Flatten`), `Dense` шари з `Dropout` для регуляризації, та фінальний `Dense(4, activation='softmax')`.\n",
    "    *   3.2. *Компіляція та Навчання (Fine-tuning):*\n",
    "        *   Скомпілюйте модель з меншою швидкістю навчання (`learning_rate`).\n",
    "        *   Навчіть спочатку лише додані шари (з замороженою базою).\n",
    "        *   (Опціонально) **Розморозьте** кілька останніх шарів натренованої бази та продовжте навчання з **дуже низькою швидкістю навчання** (fine-tuning).\n",
    "    *   3.3. *Оцінка та Порівняння:* Оцініть модель на основі трансферного навчання. **Порівняйте її продуктивність** з простою CNN, навченою з нуля. Трансферне навчання зазвичай дає значно кращі результати.\n",
    "    *   3.4. *Збереження Моделі:* Збережіть найкращу модель (ймовірно, на основі трансферного навчання) у форматі, зручному для розгортання (`.h5` або SavedModel для TensorFlow/Keras).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який дає змогу користувачеві **завантажити МРТ зображення** мозку та **отримати прогноз стадії деменції** (Non/Very Mild/Mild/Moderate Demented) від навченої моделі CNN.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `numpy`, `PIL` (або `opencv-python`), `tensorflow` або `pytorch`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель CNN (`keras.models.load_model` або `torch.load`). Завантажте мапінг індексів класів на назви стадій.\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   (`streamlit`) Використовуйте `st.file_uploader(type=['jpg', 'png', 'jpeg'])`.\n",
    "        *   (`gradio`) Використовуйте `gr.Image(type=\"pil\")`.\n",
    "    5.  ***Оброблення Зображення:***\n",
    "        *   Коли зображення завантажено, відкрийте його за допомогою `PIL.Image.open()`.\n",
    "        *   **Перетворіть зображення до розміру**, на якому навчалася модель (наприклад, 128x128).\n",
    "        *   Перетворіть на масив NumPy.\n",
    "        *   **Нормалізуйте значення пікселів** так само, як під час тренування (наприклад, діленням на 255.0).\n",
    "        *   Додайте вимір для батчу (`np.expand_dims(img_array, axis=0)`).\n",
    "    6.  *Прогнозування:* Зробіть прогноз за допомогою моделі (`model.predict()`). Результатом буде масив ймовірностей для 4 класів. Знайдіть індекс класу з максимальною ймовірністю (`np.argmax()`).\n",
    "    7.  *Відображення результату:* Перетворіть індекс класу на назву стадії. Покажіть завантажене зображення та прогнозовану стадію деменції.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки. Переконайтесь, що середовище розгортання має достатньо ресурсів для DL моделі та відповідні бібліотеки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl (можливо, знадобиться платний план або потужніший Repl для DL).\n",
    "    2.  *Завантаження файлів:* `app.py`, збережена модель (`.h5`/SavedModel), мапінг класів, `requirements.txt` (з `tensorflow`/`pytorch`, `Pillow`).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `tensorflow` або `pytorch`, `PIL` (Pillow) або `opencv-python`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `tensorflow`/`pytorch`, `Pillow`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Приклади зображень, **графіки історії навчання (втрати/точність)**, **матриця плутанини**, опціонально Grad-CAM або інші методи візуалізації уваги CNN.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**macro/weighted та для кожного класу**), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Оброблення даних зображень потребує специфічних бібліотек та підходів*** (TensorFlow/Keras або PyTorch).\n",
    "*   ***Трансферне навчання є найбільш ефективним підходом*** для цього завдання, особливо якщо немає величезного обсягу даних.\n",
    "*   **Аугментація даних** є важливою для покращення стійкості моделі та зменшення перенавчання.\n",
    "*   **Переконайтеся, що Попереднє Оброблення зображення** (зміна розміру, нормалізація) у застосунку **точно відповідає** тій, що використовувалася під час тренування моделі.\n",
    "*   Налаштування гіперпараметрів (швидкість навчання, архітектура класифікатора) є важливим для трансферного навчання.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 53: Аналіз Звіту про світове щастя\n",
    "\n",
    "**Набір даних:** World Happiness Report (Дані за різні роки, наприклад, до 2019, 2020, 2021, 2022). Можна знайти на Kaggle ([https://www.kaggle.com/datasets/unsdsn/world-happiness](https://www.kaggle.com/datasets/unsdsn/world-happiness) - містить дані до 2019, або пошукайте новіші версії). **Рекомендується вибрати дані за один конкретний рік** для спрощення аналізу.\n",
    "\n",
    "**Мета:** **Проаналізувати фактори, пов'язані зі щастям країн**, та **прогнозувати оцінку щастя ('Score' або 'Ladder score')** на основі 6-8 ключових факторів (ВВП на душу населення, соціальна підтримка, очікувана тривалість здорового життя, свобода робити життєвий вибір, щедрість, сприйняття корупції) (_Регресія_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних, Очищення та EDA:**\n",
    "    *   1.1. *Вибір Року та Завантаження:* Виберіть файл даних за конкретний рік (наприклад, `2019.csv`). Завантажте його за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз та Очищення:*\n",
    "        *   Огляньте стовпці. Назви можуть трохи відрізнятися між роками, але зазвичай включають: 'Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption'.\n",
    "        *   Перейменуйте стовпці для зручності (наприклад, видаліть пробіли, скоротіть довгі назви).\n",
    "        *   Перевірте наявність пропусків та обробіть їх (наприклад, імпутацією середнім/медіаною, хоча в даних за один рік їх може не бути).\n",
    "        *   Видаліть стовпець рангу, якщо він не потрібен.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('Score'):* Візуалізуйте розподіл оцінок щастя (гістограма).\n",
    "    *   1.4. *EDA:*\n",
    "        *   **Проаналізуйте кореляції** між 6-ма факторами та оцінкою щастя 'Score' (кореляційна матриця, теплова карта). Очікуйте сильні позитивні кореляції для ВВП, соц. підтримки, тривалості життя, свободи.\n",
    "        *   **Візуалізуйте взаємозв'язки** за допомогою діаграм розсіювання ('GDP per capita' vs 'Score', 'Social support' vs 'Score' тощо).\n",
    "        *   **Візуалізуйте географічний розподіл щастя:** Створіть карту світу, забарвлену за 'Score' (використовуйте `plotly.express.choropleth` або `geopandas`).\n",
    "        *   Порівняйте середні значення факторів для топ-10 та останніх 10 країн за рівнем щастя.\n",
    "\n",
    "2.  **Вибір Ознак та Базова Регресія:**\n",
    "    *   2.1. *Вибір Ознак:* Використовуйте 6 основних факторів ('GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption') як предиктори для 'Score'.\n",
    "    *   2.2. *Масштабування Ознак:* Застосуйте `StandardScaler` до вибраних 6 ознак.\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки.\n",
    "    *   2.4. *Навчання базових регресійних моделей:* Навчіть Linear Regression, Ridge, Lasso, SVR.\n",
    "    *   2.5. *Оцінка та Аналіз Залишків:* Оцініть моделі за допомогою MAE, MSE, RMSE, R-squared. Проаналізуйте залишки (графік залишків проти прогнозів).\n",
    "\n",
    "3.  **Розширене Моделювання та Важливість Факторів:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest Regressor, XGBoost Regressor, LightGBM Regressor.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх моделей. Linear Regression, ймовірно, вже покаже досить хороші результати через сильну кореляцію ознак з ціллю.\n",
    "    *   3.4. ***Аналіз Важливості Факторів:***\n",
    "        *   Для найкращої моделі (або навіть для Linear Regression/Ridge) **проаналізуйте важливість ознак** або коефіцієнти моделі. **Визначте, які з 6 факторів мають найбільший вплив** на оцінку щастя. Чи відповідає це результатам кореляційного аналізу?\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та скейлер.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для аналізу та прогнозування світового щастя**. Застосунок може:\n",
    "*   Відображати **карту світу** з рівнем щастя.\n",
    "*   Дозволяти ввести значення для 6 ключових факторів та **прогнозувати оцінку щастя ('Score')**.\n",
    "*   Відображати **важливість факторів**, що впливають на щастя.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`, опціонально `plotly`/`geopandas`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте дані за вибраний рік, збережену модель, скейлер.\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   (Опціонально) Відобразіть карту щастя (`st.plotly_chart(fig)` де `fig = px.choropleth(...)`).\n",
    "        *   Створіть поля введення/слайдери (`st.slider`/`gr.Slider`) для 6 факторів (GDP, Social support, ... Corruption). Встановіть діапазони на основі даних.\n",
    "        *   Кнопка для отримання прогнозу.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Сформуйте вектор ознак. **Застосуйте збережений скейлер**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз 'Score'.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану оцінку щастя. Можна також відобразити діаграму важливості ознак моделі.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки. Для карт `plotly` потрібна інтернет-зв'язок.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, дані (якщо потрібні для карти), `requirements.txt` (з `plotly`, якщо використовується).\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, опціонально `plotly`, `geopandas` (для карт).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, опціонально `plotly`/`geopandas`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіл 'Score', **діаграми розсіювання (фактори vs Score)**, **кореляційна теплова карта**, **діаграма важливості ознак**, **карта світу, забарвлена за Score**.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, R-squared.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   Використання даних **за один рік** спрощує аналіз. Якщо аналізувати дані за кілька років, потрібно враховувати часові зміни та можливі зміни у методології звіту.\n",
    "*   6 ключових факторів зазвичай добре пояснюють значну частину варіації в 'Score', тому навіть лінійні моделі можуть показати високий R-squared.\n",
    "*   **Аналіз важливості ознак** та їх коефіцієнтів (для лінійних моделей) дає цікаві інсайти щодо детермінант щастя на національному рівні.\n",
    "*   Візуалізація на карті світу є дуже наочною для цього набору даних.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 54: Інтелектуальна інформаційна система для класифікування ліків на основі даних пацієнтів\n",
    "\n",
    "**Набір даних:** Drug Classification ([https://www.kaggle.com/datasets/prathamtripathi/drug-classification](https://www.kaggle.com/datasets/prathamtripathi/drug-classification)). Невеликий, чистий набір даних, добре підходить для навчальних цілей.\n",
    "\n",
    "**Мета:** **Прогнозувати тип ліків ('Drug')**, які були б призначені пацієнту, на основі його характеристик: вік ('Age'), стать ('Sex'), артеріальний тиск ('BP'), рівень холестерину ('Cholesterol'), співвідношення натрію до калію в крові ('Na_to_K') (_Багатокласова класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Дослідження та Попереднє Оброблення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `drug200.csv` за допомогою `pandas`.\n",
    "    *   1.2. *Аналіз даних:* Огляньте стовпці, типи даних (деякі категоріальні), пропуски (зазвичай немає).\n",
    "    *   1.3. *Аналіз Цільової Змінної ('Drug'):*\n",
    "        *   Дослідіть **унікальні типи ліків** (DrugY, DrugA, DrugB, DrugC, DrugX).\n",
    "        *   Перевірте **розподіл класів**. Чи є він збалансованим? (DrugY зазвичай найчастіший).\n",
    "    *   1.4. *EDA:*\n",
    "        *   Дослідіть зв'язок між вхідними ознаками та типом призначених ліків:\n",
    "            *   **Числові ознаки ('Age', 'Na_to_K'):** Використовуйте `boxplot` або `violinplot`, згруповані за 'Drug'.\n",
    "            *   **Категоріальні ознаки ('Sex', 'BP', 'Cholesterol'):** Використовуйте `countplot` (з параметром `hue='Drug'`) або `crosstab`.\n",
    "        *   Побудуйте парні діаграми (`pairplot`) для числових ознак, забарвлені за типом ліків.\n",
    "    *   1.5. ***Оброблення Категоріальних Ознак:***\n",
    "        *   **Порядкові ознаки:** 'BP' (Low, Normal, High) та 'Cholesterol' (Normal, High) мають природний порядок. Застосуйте **Ordinal Encoding** (наприклад, `OrdinalEncoder` з `sklearn.preprocessing` або ручне мапування {'Low':0, 'Normal':1, 'High':2}).\n",
    "        *   **Номінальна ознака:** 'Sex' (F, M). Застосуйте **One-Hot Encoding** або бінарне кодування (0/1).\n",
    "    *   1.6. *Масштабування Числових Ознак:* Застосуйте `StandardScaler` до 'Age' та 'Na_to_K'.\n",
    "\n",
    "2.  **Масштабування Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (з стратифікацією за 'Drug').\n",
    "    *   2.2. *Навчання базових багатокласових класифікаторів:* Навчіть K-NN, Decision Tree, Logistic Regression (`multi_class='ovr'`), GaussianNB.\n",
    "    *   2.3. *Оцінка:* Оцініть моделі на тестовій вибірці. Використовуйте:\n",
    "        *   Accuracy.\n",
    "        *   **Classification Report** (Precision, Recall, F1 для кожного типу ліків).\n",
    "        *   **Macro/Weighted $F_1$-score**.\n",
    "        *   Матрицю плутанини.\n",
    "\n",
    "3.  **Розширене Моделювання та Розгортання:**\n",
    "    *   3.1. *Реалізація просунутих класифікаторів:* Навчіть Random Forest, SVM (з різними ядрами), XGBoost, LightGBM.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей.\n",
    "    *   3.3. *Порівняння Моделей:* Порівняйте продуктивність усіх моделей. Зверніть увагу на те, як добре класифікуються менш представлені типи ліків.\n",
    "    *   3.4. *Аналіз Важливості Ознак:* Для найкращої моделі (ймовірно, на основі дерев) визначте, які **характеристики пацієнта** ('Na_to_K', 'BP', 'Age' тощо) є найбільш важливими для вибору типу ліків.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та компоненти обробки (кодувальники, скейлер).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для класифікації типу ліків**. Користувач може ввести дані пацієнта (вік, стать, тиск, холестерин, Na_to_K), і застосунок **прогнозує найбільш ймовірний тип ліків**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель, OrdinalEncoder (якщо використовувався), OneHotEncoder (якщо використовувався), StandardScaler.\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення: `st.number_input` для 'Age', 'Na_to_K'; `st.selectbox` або `st.radio` для 'Sex', 'BP', 'Cholesterol'.\n",
    "    5.  *Оброблення вводу:*\n",
    "        *   Зберіть дані.\n",
    "        *   **Застосуйте збережені кодувальники** до категоріальних ознак ('Sex', 'BP', 'Cholesterol').\n",
    "        *   **Застосуйте збережений скейлер** до числових ознак ('Age', 'Na_to_K').\n",
    "        *   Сформуйте вектор ознак у правильному порядку.\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (`model.predict()`). Можна також отримати ймовірності (`predict_proba`).\n",
    "    7.  *Відображення результату:* Покажіть прогнозований тип ліків ('DrugY', 'DrugA' тощо).\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, кодувальники, скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Розподіли ознак для кожного типу ліків (boxplot/countplot)**, парні діаграми (підмножина), **матриця плутанини**, **діаграма важливості ознак**.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**macro/weighted та для кожного типу ліків**), Confusion Matrix.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Правильне кодування категоріальних ознак є важливим***, особливо **Ordinal Encoding** для 'BP' та 'Cholesterol', щоб зберегти інформацію про порядок.\n",
    "*   **Масштабування числових ознак** (`StandardScaler`) рекомендується.\n",
    "*   Набір даних невеликий, тому результати можуть бути чутливими до поділу train/test; використовуйте **крос-валідацію** для надійної оцінки.\n",
    "*   Аналіз важливості ознак, ймовірно, покаже, що **'Na_to_K' є дуже впливовим фактором**. Моделі на основі дерев (Decision Tree, Random Forest) можуть добре відобразити правила прийняття рішень, що лежать в основі цього набору даних.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 55: Інтелектуальна інформаційна система для прогнозування дощу в Австралії\n",
    "\n",
    "**Набір даних:** Rain in Australia ([https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package)) - `weatherAUS.csv`. Містить щоденні спостереження погоди з багатьох місць Австралії.\n",
    "\n",
    "**Мета:** **Прогнозувати, чи буде дощ завтра ('RainTomorrow' = 'Yes')** на основі сьогоднішніх погодних умов (_Бінарна класифікація з потенційним дисбалансом_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та EDA:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `weatherAUS.csv` за допомогою `pandas`.\n",
    "    *   1.2. ***Оброблення Пропущених Значень:*** Цей набір даних має **багато пропусків** у багатьох стовпцях ('Sunshine', 'Evaporation', 'Cloud9am', 'Cloud3pm', 'Pressure9am', 'Pressure3pm' тощо).\n",
    "        *   Проаналізуйте відсоток пропусків для кожного стовпця.\n",
    "        *   Виберіть стратегію імпутації:\n",
    "            *   Для числових: медіана/середнє (можливо, згруповане за 'Location' або місяцем).\n",
    "            *   Для категоріальних: мода.\n",
    "            *   Розгляньте видалення стовпців з *дуже* великою кількістю пропусків (наприклад, > 40-50%).\n",
    "    *   1.3. *Оброблення Дати:* Перетворіть 'Date' на datetime об'єкт. Витягніть **місяць** та, можливо, день року як ознаки.\n",
    "    *   1.4. *Оброблення Цільової Змінної:* Перетворіть 'RainTomorrow' ('Yes'/'No') на бінарний формат (1/0). **Перевірте на дисбаланс класів** ('No' зазвичай більше). Видаліть рядки, де 'RainTomorrow' пропущено.\n",
    "    *   1.5. *Оброблення 'RainToday':* Перетворіть 'RainToday' на бінарний формат (1/0), імпутуйте можливі пропуски (наприклад, 0).\n",
    "    *   1.6. *EDA:*\n",
    "        *   Дослідіть зв'язок між сьогоднішньою погодою ('Rainfall', 'Humidity3pm', 'Cloud3pm', 'Pressure3pm', 'Sunshine', 'WindGustSpeed', 'RainToday') та 'RainTomorrow'. Використовуйте `boxplot`, `countplot`.\n",
    "        *   Проаналізуйте, як частота дощу завтра залежить від місяця та локації ('Location').\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Оброблення Категоріальних Ознак:*\n",
    "        *   **'Location':** Має високу кардинальність. Використовуйте One-Hot Encoding (може створити багато ознак) або Target Encoding (обережно з витоком даних, використовуйте згладжування або застосовуйте лише на тренувальних даних).\n",
    "        *   **'WindGustDir', 'WindDir9am', 'WindDir3pm':** Закодуйте напрямки вітру (One-Hot Encoding).\n",
    "    *   2.2. *Масштабування Числових Ознак:* Застосуйте `StandardScaler` до всіх числових погодних ознак.\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (з стратифікацією за 'RainTomorrow').\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, Decision Tree, Naive Bayes. **Врахуйте дисбаланс класів** (`class_weight='balanced'` або `imblearn`).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі. Зосередьтеся на **AUC-PR**, **$F_1$-score для класу 'Yes'**, Recall(Yes), Precision(Yes). Також дивіться AUC-ROC.\n",
    "\n",
    "3.  **Розширене Моделювання та Аналіз за Локацією:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM.\n",
    "    *   3.2. *Порівняння Стратегій:* Порівняйте продуктивність моделей, навчених з різними стратегіями імпутації пропусків або кодування 'Location'.\n",
    "    *   3.3. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей, максимізуючи AUC-PR або F1(Yes).\n",
    "    *   3.4. *Аналіз Важливості Ознак:* Визначте, які сьогоднішні погодні умови ('Humidity3pm', 'Pressure3pm', 'Sunshine', 'WindGustSpeed', 'RainToday') є найкращими предикторами дощу завтра.\n",
    "    *   3.5. *Аналіз за Локацією (Опціонально):* Оцініть продуктивність найкращої моделі **окремо для різних локацій**. Чи є місця, де модель працює значно краще/гірше? Чому?\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та пайплайн попереднього оброблення.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування дощу в Австралії**. Застосунок приймає на вхід сьогоднішні погодні умови для певної локації та **прогнозує ймовірність дощу на завтра**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та **повний пайплайн попереднього оброблення** (імпутація, кодування, масштабування).\n",
    "    4.  *Створення інтерфейсу:* Створіть елементи введення для ключових ознак: `st.selectbox` для 'Location', 'WindGustDir'; `st.number_input`/`st.slider` для 'MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'WindGustSpeed'; `st.radio` для 'RainToday'.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Створіть DataFrame. **Застосуйте збережений пайплайн попереднього оброблення**.\n",
    "    6.  *Прогнозування:* Отримайте ймовірність дощу завтра (клас 'Yes') - `model.predict_proba()[:, 1]`.\n",
    "    7.  *Відображення результату:* Покажіть прогнозовану ймовірність дощу та/або висновок (\"Ймовірно, буде дощ\" / \"Дощ малоймовірний\").\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, `imblearn` (опціонально).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Патерни пропущених значень, розподіли ознак проти 'RainTomorrow', **діаграма важливості ознак**, ROC/PR криві.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**для класу 'Yes'**), AUC-ROC, ***AUC-PR***.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Оброблення великої кількості пропущених значень є головним викликом*** цього проєкту. Вибір стратегії імпутації важливий.\n",
    "*   **Адресуйте дисбаланс класів** 'RainTomorrow'.\n",
    "*   Обробка категоріальних ознак з високою кардинальністю ('Location') потребує уваги.\n",
    "*   Аналіз важливості ознак покаже, які метеорологічні параметри є ключовими для прогнозування опадів.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 56: Інтелектуальна інформаційна система для прогнозування ризику раку шийки матки\n",
    "\n",
    "**Набір даних:** Cervical Cancer (Risk Factors) Dataset (UCI: [https://archive.ics.uci.edu/ml/datasets/Cervical+cancer+%28Risk+Factors%29](https://archive.ics.uci.edu/ml/datasets/Cervical+cancer+%28Risk+Factors%29)). Містить демографічні дані, звички та результати медичних тестів. Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/loveall/cervical-cancer-risk-classification](https://www.kaggle.com/datasets/loveall/cervical-cancer-risk-classification))\n",
    "\n",
    "**Мета:** **Прогнозувати результат біопсії ('Biopsy')** (0 - Healthy, 1 - Cancer), що є індикатором ризику раку, на основі факторів ризику та результатів тестів (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Очищення Даних та Імпутація:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `risk_factors_cervical_cancer.csv` (або аналогічний).\n",
    "    *   1.2. ***Оброблення Пропущених Значень ('?'):***\n",
    "        *   **Замініть '?' на `NaN`**.\n",
    "        *   Перетворіть стовпці на числовий тип (`pd.to_numeric`, `errors='coerce'`).\n",
    "        *   **Проаналізуйте велику кількість пропусків** у багатьох стовпцях (особливо 'STDs: Time since first diagnosis', 'STDs: Time since last diagnosis').\n",
    "        *   **Агресивно імпутуйте пропуски:**\n",
    "            *   Видаліть стовпці з *надзвичайно* високим відсотком NaN (як вищезгадані).\n",
    "            *   Для решти використовуйте **медіану** (краще за середнє для потенційно асиметричних даних) або **`KNNImputer`**. Обґрунтуйте вибір.\n",
    "    *   1.3. *Аналіз Цільової Змінної ('Biopsy'):* Дослідіть розподіл класів. Чи є дисбаланс? (Клас 1 зазвичай міноритарний).\n",
    "    *   1.4. *EDA (після імпутації):*\n",
    "        *   Дослідіть зв'язок між ключовими факторами ризику (вік, кількість партнерів, вік першого статевого акту, кількість вагітностей, куріння, гормональні контрацептиви, STDs) та результатом біопсії.\n",
    "        *   Проаналізуйте кореляції між ознаками.\n",
    "\n",
    "2.  **Вибір Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Масштабування Ознак:* Застосуйте `StandardScaler` до всіх числових ознак.\n",
    "    *   2.2. *Вибір Ознак (Опціонально):* Через велику кількість початкових ознак та можливу надмірність після імпутації, розгляньте застосування **методів вибору ознак** (наприклад, на основі важливості з RF, RFECV, Lasso) для зменшення розмірності та потенційного покращення моделі.\n",
    "    *   2.3. *Підготовка даних:* Розділіть дані на тренувальну та тестову вибірки (з стратифікацією за 'Biopsy').\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, SVM, K-NN. Врахуйте дисбаланс (`class_weight='balanced'` або `imblearn`).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі. **Пріоритет - Recall та $F_1$-score для позитивного класу (Biopsy=1)**. Також дивіться Precision(1), AUC-ROC. Використовуйте стратифіковану крос-валідацію.\n",
    "\n",
    "3.  **Розширене Моделювання та Порівняння:**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри, **максимізуючи Recall(1) або F1(1)**.\n",
    "    *   3.3. *Порівняння Стратегій:* Порівняйте продуктивність моделей, навчених з різними стратегіями імпутації та/або вибору ознак (якщо застосовувалося).\n",
    "    *   3.4. *Аналіз Важливості Ознак:* Визначте, які фактори ризику та результати тестів є найбільш значущими для прогнозування результату біопсії.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель та пайплайн обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для оцінки ризику раку шийки матки** (на основі прогнозу біопсії). Застосунок приймає на вхід дані пацієнтки та видає **прогноз ризику**.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та **повний пайплайн попереднього оброблення** (імпутація, масштабування, можливо, вибір ознак).\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення для ключових ознак, що використовуються моделлю (Age, Number of sexual partners, First sexual intercourse, Num of pregnancies, Smokes (years), Hormonal Contraceptives (years), STDs, Dx:Cancer, Dx:HPV тощо).\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте збережений пайплайн попереднього оброблення**.\n",
    "    6.  *Прогнозування:* Отримайте ймовірність позитивного результату біопсії (`predict_proba()[:, 1]`). Зробіть прогноз класу (`predict()`).\n",
    "    7.  *Відображення результату:* Покажіть прогноз (\"Високий/Низький ризик за результатами біопсії\") та/або ймовірність.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`.\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Теплова карта пропущених значень**, розподіли ознак, **діаграма важливості ознак**, матриця плутанини, ROC-крива.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, ***Recall (для Biopsy=1)***, **$F_1$-score (для Biopsy=1)**, AUC-ROC.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Агресивна та обґрунтована обробка пропущених значень ('?') є абсолютно необхідною*** для роботи з цим набором даних.\n",
    "*   **Вибір ознак може бути корисним** для покращення моделі та зменшення шуму.\n",
    "*   **Пріоритезуйте метрику Recall для позитивного класу (Biopsy=1)**, оскільки пропуск випадку раку є дуже небажаним у медичному контексті.\n",
    "*   Інтерпретація важливих ознак може вказати на ключові фактори ризику.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 57: Аналіз популярності статей Вікіпедії\n",
    "\n",
    "**Набір даних:** Wikipedia Web Traffic Time Series Forecasting ([https://www.kaggle.com/competitions/web-traffic-time-series-forecasting/data](https://www.kaggle.com/competitions/web-traffic-time-series-forecasting/data)). _Потрібен акаунт Kaggle_. Містить дані про щоденні перегляди для ~145 тис. статей Вікіпедії за тривалий період. **Дуже великий набір даних! Потребує значної вибірки/фільтрації сторінок.**\n",
    "\n",
    "**Мета:** **Прогнозувати щоденну кількість переглядів ('Visits')** для вибраних статей Вікіпедії на майбутній період (_Прогнозування множинних часових рядів_). **Основна метрика оцінки - SMAPE**.\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження Даних, Фільтрація та EDA:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте один з тренувальних файлів (`train_1.csv` або `train_2.csv`). Це може зайняти багато часу/пам'яті.\n",
    "    *   1.2. ***Фільтрація/Вибірка Сторінок:*** **Необхідно вибрати невелику підмножину статей** для аналізу (наприклад, 10-100 статей). Критерії вибору:\n",
    "        *   Випадкова вибірка.\n",
    "        *   Статті з різною середньою популярністю (високою, середньою, низькою).\n",
    "        *   Статті з різних проєктів/мов (інформація в стовпці 'Page').\n",
    "    *   1.3. *Трансформація Даних (`melt`):* Поточний формат даних \"широкий\" (кожен стовпець - дата). Перетворіть його на \"довгий\" формат за допомогою `pandas.melt`, щоб отримати стовпці 'Page', 'Date', 'Visits'.\n",
    "    *   1.4. *Оброблення Дат та Пропусків:* Перетворіть 'Date' на datetime. Обробіть пропуски в 'Visits' (наприклад, інтерполяцією або заміною на 0). Розгляньте **логарифмічне перетворення `np.log1p(Visits)`** для стабілізації дисперсії.\n",
    "    *   1.5. *EDA:*\n",
    "        *   Візуалізуйте **часові ряди переглядів** для кількох вибраних статей.\n",
    "        *   Дослідіть **тренди, річну та тижневу сезонність** (графіки декомпозиції, ACF/PACF).\n",
    "        *   Проаналізуйте середні рівні переглядів для різних проєктів/мов.\n",
    "\n",
    "2.  **Статистичне Прогнозування Часових Рядів:**\n",
    "    *   2.1. *Моделювання Окремих Рядів:* Для кожної вибраної статті **індивідуально** застосуйте статистичні моделі:\n",
    "        *   Naive forecast.\n",
    "        *   Exponential Smoothing (ETS / Holt-Winters) (`statsmodels`).\n",
    "        *   ARIMA/SARIMA (`statsmodels`).\n",
    "    *   2.2. *Оцінка:* Розділіть дані кожної статті на тренувальну та валідаційну вибірки (часовий поділ). Оцініть моделі за допомогою **SMAPE (Symmetric Mean Absolute Percentage Error)** - основна метрика змагання. Також можна використовувати MAE, RMSE. Обчисліть середній SMAPE по всіх вибраних статтях.\n",
    "\n",
    "3.  **Прогнозування за допомогою Машинного Навчання та Порівняння:**\n",
    "    *   3.1. ***Глобальне ML Моделювання:*** Побудуйте **одну модель ML**, яка прогнозує перегляди для **всіх вибраних статей одночасно**.\n",
    "        *   *Інженерія Ознак:* Створіть ознаки:\n",
    "            *   Часові: день тижня, день року, місяць, рік, тиждень року.\n",
    "            *   Лагові: значення (log)Visits за попередні дні/тижні.\n",
    "            *   Ковзні вікна: середнє/медіана (log)Visits за попередні періоди.\n",
    "            *   Ідентифікатори Статті/Проєкту: Закодуйте 'Page' або витягнуті з неї характеристики (проєкт, мова) як категоріальні ознаки.\n",
    "        *   *Підготовка даних:* Створіть табличний DataFrame з цими ознаками. Розділіть на тренувальну/валідаційну вибірки (часовий поділ). Масштабуйте числові ознаки.\n",
    "    *   3.2. *Навчання ML Моделей:* Навчіть **LightGBM** (рекомендовано через ефективність) або XGBoost, Random Forest.\n",
    "    *   3.3. *Порівняння Підходів:* **Порівняйте продуктивність** (середній SMAPE) глобальної ML моделі зі статистичними моделями, навченими індивідуально.\n",
    "    *   3.4. *Підготовка до Подання (якщо потрібно):* Зробіть прогнози на майбутній період для всіх ~145 тис. сторінок (дуже ресурсоємно!) і збережіть у потрібному форматі.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель (статистичну для окремої статті або глобальну ML) та компоненти обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який дає змогу:\n",
    "*   Вибрати одну зі **зразкових статей Вікіпедії** (тих, що аналізувалися).\n",
    "*   Відобразити **історичний графік переглядів**.\n",
    "*   Показати **прогноз переглядів** на майбутній період.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `statsmodels`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель(і) (глобальну ML або індивідуальні статистичні), компоненти обробки, історичні дані для вибраних статей.\n",
    "    4.  *Створення інтерфейсу:*\n",
    "        *   Створіть **випадаючий список (`st.selectbox`)** для вибору статті зі списку тих, що аналізувалися.\n",
    "        *   Дозвольте вибрати горизонт прогнозування.\n",
    "    5.  *Оброблення вводу та прогнозування:*\n",
    "        *   Для вибраної статті:\n",
    "            *   (Статистична модель) Завантажте відповідну модель і зробіть `.forecast()`.\n",
    "            *   (Глобальна ML) Згенеруйте необхідні лагові/часові/категоріальні ознаки для майбутніх дат цієї статті. Зробіть прогноз за допомогою ML моделі.\n",
    "        *   **Застосуйте обернене логарифмічне перетворення (`np.expm1`)**, якщо моделювали log(Visits). Округліть до цілого.\n",
    "    6.  *Відображення результату:* Побудуйте **графік** з історією та прогнозом для вибраної статті.\n",
    "    7.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель(і), компоненти, зразки даних, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `statsmodels`, `xgboost`, `lightgbm` (рекомендовано).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`, `plotly`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** **Часові ряди** переглядів для окремих статей, графіки декомпозиції, ACF/PACF, **графік прогнозу проти фактичних значень**.\n",
    "*   **Метрики оцінки:** MAE, MSE, RMSE, ***SMAPE (основна)***.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Фільтрація/вибірка статей є абсолютно необхідною*** через величезний розмір даних.\n",
    "*   **Ефективна обробка та зберігання даних** є важливими.\n",
    "*   **Логарифмічне перетворення `np.log1p(Visits)`** зазвичай покращує стабільність моделей.\n",
    "*   **Глобальні ML моделі** (одна модель для всіх рядів з використанням ознак статті) часто є більш масштабованими та можуть показувати кращі результати, ніж індивідуальні статистичні моделі, особливо якщо є багато схожих рядів.\n",
    "*   **SMAPE** є специфічною метрикою, зверніть увагу на її формулу та реалізацію.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 58: Інтелектуальна інформаційна система для прогнозування рівня доходу (Census Data)\n",
    "\n",
    "**Набір даних:** Adult Census Income ([https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult)). Класичний набір даних для бінарної класифікації, витягнутий з бази даних перепису населення США 1994 року. Дзеркало на Kaggle: ([https://www.kaggle.com/datasets/uciml/adult-census-income](https://www.kaggle.com/datasets/uciml/adult-census-income))\n",
    "\n",
    "**Мета:** **Прогнозувати, чи перевищує дохід людини $50K на рік** ('>50K' або '<=50K') на основі даних перепису населення (_Класифікація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Завантаження та Очищення Даних:**\n",
    "    *   1.1. *Завантаження даних:* Завантажте `adult.data` та `adult.test`. **Увага:** ці файли **не мають заголовків**, а тестовий файл має трохи інший формат (зайва крапка в кінці рядків). **Призначте назви стовпців** згідно з `adult.names`: 'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'.\n",
    "    *   1.2. *Очищення та Попереднє Оброблення:*\n",
    "        *   **Оброблення Пропусків ('?'):** Пропущені значення позначені як ' ?' (з пробілом попереду). Замініть їх на `NaN`.\n",
    "        *   **Видалення Пропусків:** Найпростіший підхід - видалити рядки з `NaN`, оскільки їх відносно небагато. Альтернативно - імпутація модою для категоріальних.\n",
    "        *   **Оброблення Тестового Файлу:** Виправте формат тестового файлу (видаліть крапку в кінці цільової змінної).\n",
    "        *   **Приведення Цільової Змінної ('income'):** Перетворіть '<=50K' та '>50K' (і '<=50K.', '>50K.' у тесті) на бінарний формат (0/1).\n",
    "        *   **Видалення 'fnlwgt':** Ця ознака (final weight) зазвичай не використовується для моделювання.\n",
    "        *   **Видалення 'education':** Ознака 'education-num' містить ту саму інформацію в числовому форматі, тому 'education' можна видалити для уникнення дублювання.\n",
    "    *   1.3. *Аналіз Цільової Змінної:* Дослідіть розподіл класів доходу. Чи є **дисбаланс**? (Клас '>50K' є міноритарним).\n",
    "    *   1.4. *EDA:*\n",
    "        *   Дослідіть зв'язок між демографічними ('age', 'workclass', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex') та економічними ('capital-gain', 'capital-loss', 'hours-per-week') ознаками та рівнем доходу. Використовуйте `boxplot`, `countplot`, гістограми.\n",
    "\n",
    "2.  **Інженерія Ознак та Базове Моделювання:**\n",
    "    *   2.1. *Оброблення Категоріальних Ознак:* Застосуйте **One-Hot Encoding** до всіх категоріальних ознак ('workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'). Враховуйте високу кардинальність 'native-country'.\n",
    "    *   2.2. *Масштабування Числових Ознак:* Застосуйте `StandardScaler` до числових ознак ('age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week').\n",
    "    *   2.3. *Підготовка даних:* Використовуйте `adult.data` для тренування/валідації, `adult.test` для фінальної оцінки. Розділіть тренувальні дані на власне тренувальну та валідаційну вибірки (з стратифікацією). Застосуйте обробку (OHE, Scaling) однаково до всіх наборів.\n",
    "    *   2.4. *Навчання базових класифікаторів:* Навчіть Logistic Regression, Decision Tree, GaussianNB. Врахуйте дисбаланс (`class_weight`).\n",
    "    *   2.5. *Оцінка:* Оцініть моделі на валідаційній вибірці. Зосередьтеся на $F_1$-score, AUC-ROC, Precision/Recall для класу '>50K'.\n",
    "\n",
    "3.  **Розширене Моделювання та Міркування щодо Справедливості (Опціонально):**\n",
    "    *   3.1. *Реалізація просунутих моделей:* Навчіть Random Forest, XGBoost, LightGBM.\n",
    "    *   3.2. *Налаштування гіперпараметрів:* Оптимізуйте параметри кращих моделей.\n",
    "    *   3.3. *Фінальна Оцінка:* Оцініть найкращу модель на **незалежному тестовому наборі (`adult.test`)**.\n",
    "    *   3.4. *Аналіз Важливості Ознак:* Визначте ключові фактори, що впливають на рівень доходу.\n",
    "    *   3.5. ***Аналіз Справедливості (Fairness Analysis - Опціонально, Просунуто):***\n",
    "        *   Оскільки набір даних містить **чутливі атрибути** ('race', 'sex', 'age'), дослідіть, чи є у вашої найкращої моделі **упередження (bias)** щодо певних груп.\n",
    "        *   Використовуйте інструменти/метрики з бібліотек типу **`fairlearn`** або `AIF360` для вимірювання розбіжностей у продуктивності (наприклад, різниця в точності, рівні хибних спрацювань/пропусків) між різними групами (наприклад, чоловіки vs жінки, різні раси).\n",
    "        *   Обговоріть потенційні соціальні наслідки таких упереджень.\n",
    "    *   3.6. *Збереження Моделі:* Збережіть найкращу модель та пайплайн обробки.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit` для прогнозування рівня доходу**. Застосунок приймає на вхід демографічні та економічні дані людини та **прогнозує, чи перевищує її дохід $50K/рік**. (Опціонально) Може містити обговорення аспектів справедливості моделі.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`, `xgboost`/`lightgbm`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та **повний пайплайн попереднього оброблення**.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення для ключових ознак: 'age', 'workclass', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. **Застосуйте збережений пайплайн** (OHE, Scaling).\n",
    "    6.  *Прогнозування:* Зробіть прогноз класу (0/1) та/або ймовірності класу '>50K'.\n",
    "    7.  *Відображення результату:* Покажіть прогноз (\"Дохід >50K\" / \"Дохід <=50K\").\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель/пайплайн, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `xgboost`, `lightgbm`, опціонально бібліотеки для справедливості (`fairlearn`).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіли ознак для кожного класу доходу, **діаграма важливості ознак**, матриця плутанини, ROC-крива. Якщо аналізується справедливість - графіки розбіжностей метрик між групами.\n",
    "*   **Метрики оцінки:** Accuracy, Precision, Recall, F1 (**для класу '>50K'**), AUC-ROC. Метрики справедливості (якщо аналізується): Demographic Parity Difference, Equalized Odds Difference тощо.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Ретельна Попереднє Оброблення є ключовою***: правильне призначення назв стовпців, обробка '?', видалення/обробка 'education', 'fnlwgt', кодування категорій.\n",
    "*   **Масштабування числових ознак** важливе.\n",
    "*   **Аналіз справедливості** є просунутим, але дуже актуальним та важливим доповненням при роботі з даними, що містять чутливі атрибути. Це показує розуміння етичних аспектів машинного навчання.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 59: Інтелектуальна інформаційна система для виявлення аномалій у логах сервера\n",
    "\n",
    "**Набір даних:** Log Anomaly Detection Datasets. **Отримання якісних мічених лог-даних може бути складним.** Варіанти:\n",
    "*   **Академічні репозиторії:** Пошук наборів даних, як-от HDFS, BGL, HPC, які використовуються в дослідженнях (наприклад, репозиторій **LogHub**: [https://github.com/logpai/loghub]). Ці дані часто потребують **значного парсингу** для вилучення структурованих подій або ознак.\n",
    "*   **Спрощені/Синтетичні набори:** Пошук на Kaggle або в інших місцях наборів, де логи вже представлені у вигляді векторів ознак або простих послідовностей подій.\n",
    "*   **Використання даних з іншого проєкту:** Як альтернативу, можна використати часові ряди з **NAB (Тема 25)**, інтерпретуючи їх як показники роботи сервера (наприклад, CPU utilization).\n",
    "***Припустимо, що вдалося знайти або підготувати набір даних, де кожному запису (наприклад, часовому вікну або сесії) відповідає вектор числових ознак та бінарна мітка аномалії (0 - норма, 1 - аномалія), або завдання виконується в некерованому режимі.***\n",
    "\n",
    "**Мета:** **Виявляти аномальні події** або стани системи (наприклад, збої, атаки, незвична поведінка) на основі агрегованих ознак з логів сервера (_Виявлення аномалій - кероване або некероване_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Підготовка Даних та Дослідження:**\n",
    "    *   1.1. *Парсинг/Агрегація (Якщо потрібно):* Якщо працюєте з сирими логами (складно), застосуйте техніки парсингу (регулярні вирази, шаблони) для вилучення подій/шаблонів. Потім агрегуйте ознаки (наприклад, кількість різних типів подій за часове вікно, послідовності подій). **Для курсового проєкту рекомендується знайти вже структуровані дані.**\n",
    "    *   1.2. *Завантаження структурованих даних:* Завантажте дані (вектори ознак та, можливо, мітки аномалій).\n",
    "    *   1.3. *Аналіз даних:* Дослідіть ознаки. Якщо є мітки, проаналізуйте **розподіл аномалій** (зазвичай їх дуже мало - **сильний дисбаланс**).\n",
    "    *   1.4. *EDA:* Візуалізуйте розподіли ознак для нормальних та аномальних записів (якщо є мітки). Використовуйте PCA/t-SNE для візуалізації даних у 2D.\n",
    "    *   1.5. *Масштабування Ознак:* Застосуйте `StandardScaler` або `RobustScaler`.\n",
    "\n",
    "2.  **Застосування Алгоритмів Виявлення Аномалій:**\n",
    "    *   2.1. ***Некеровані (Unsupervised) методи:*** Ці методи не використовують мітки під час навчання і є основними, коли міток немає або їх мало.\n",
    "        *   **Isolation Forest (`sklearn.ensemble.IsolationForest`):** Ефективний для багатовимірних даних.\n",
    "        *   **Local Outlier Factor (LOF) (`sklearn.neighbors.LocalOutlierFactor`):** Базується на локальній щільності.\n",
    "        *   **One-Class SVM (`sklearn.svm.OneClassSVM`):** Намагається знайти межу навколо нормальних даних.\n",
    "        *   **Autoencoders (DL, просунуто):** Навчіть автокодувальник на нормальних даних (або на всіх даних). Висока помилка реконструкції вказує на аномалію.\n",
    "        *   *Налаштування:* Важливо налаштувати параметр `contamination` (очікувана частка аномалій) або інші параметри (`nu` для OneClassSVM, `n_neighbors` для LOF).\n",
    "    *   2.2. *Керовані (Supervised) методи (Якщо є мітки):*\n",
    "        *   Використовуйте стандартні класифікатори (Logistic Regression, Random Forest, XGBoost), **враховуючи сильний дисбаланс** (зважування класів, семплінг `imblearn`).\n",
    "\n",
    "3.  **Оцінка та Візуалізація:**\n",
    "    *   3.1. *Генерація Прогнозів:* Отримайте прогнози аномалій від вибраних моделей. Некеровані методи часто видають оцінку аномальності або бінарну мітку.\n",
    "    *   3.2. ***Оцінка (Якщо є мітки):***\n",
    "        *   Використовуйте Precision, Recall, $F_1$-score **для класу аномалій (1)**.\n",
    "        *   AUC-ROC, **AUC-PR** (важливо через дисбаланс).\n",
    "        *   Матриця плутанини.\n",
    "    *   3.3. *Оцінка (Якщо міток немає):*\n",
    "        *   **Візуальний аналіз:** Візуалізуйте виявлені аномалії на PCA/t-SNE графіку. Чи утворюють вони окремі групи? Чи відповідають вони очікуванням?\n",
    "        *   **Аналіз ознак аномалій:** Порівняйте середні значення ознак для виявлених аномалій та нормальних точок. Чи мають аномалії екстремальні значення?\n",
    "    *   3.4. *Порівняння Алгоритмів:* Порівняйте результати різних методів.\n",
    "    *   3.5. *Збереження Моделі:* Збережіть найкращу модель виявлення аномалій.\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Створено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який:\n",
    "*   Приймає на вхід вектор ознак, що представляє стан системи/логу.\n",
    "*   **Використовує навчену модель для прогнозування**, чи є цей стан **аномальним**.\n",
    "*   (Опціонально) Візуалізує дані або результати оцінки.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit` / `gradio`:**\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`/`gradio`, `pandas`, `numpy`, `joblib`/`pickle`, `sklearn`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте збережену модель та скейлер.\n",
    "    4.  *Створення інтерфейсу:* Створіть поля введення (`st.number_input`) для ознак, що використовуються моделлю. Через потенційно велику кількість ознак, можливо, доведеться спростити інтерфейс або дозволити завантаження файлу з ознаками.\n",
    "    5.  *Оброблення вводу:* Зберіть дані. Сформуйте вектор ознак. **Застосуйте збережений скейлер**.\n",
    "    6.  *Прогнозування:* Зробіть прогноз аномалії (`model.predict()`). Деякі некеровані моделі можуть також видавати оцінку аномальності (`decision_function` або `score_samples`).\n",
    "    7.  *Відображення результату:* Покажіть прогноз (\"Норма\" / \"Аномалія\") та, можливо, оцінку аномальності.\n",
    "    8.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `replit`:**\n",
    "    1.  *Створення Repl:* Створіть Python Repl.\n",
    "    2.  *Завантаження файлів:* `app.py`, модель, скейлер, `requirements.txt`.\n",
    "    3.  *Налаштування та Запуск:* Стандартні кроки.\n",
    "    4.  *Доступ:* Через публічне посилання `Replit`.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `imblearn` (якщо кероване), опціонально `pyod` (бібліотека для виявлення аномалій), `tensorflow`/`pytorch` (для автокодувальників).\n",
    "*   **Розгортання:** `streamlit` або `gradio`, `joblib` або `pickle`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Розподіли ознак, **PCA/t-SNE графіки з виділеними аномаліями**, графіки часових рядів (якщо застосовно).\n",
    "*   **Метрики оцінки:**\n",
    "    *   *(З мітками)* Precision, Recall, F1 (для аномалій), AUC-ROC, **AUC-PR**.\n",
    "    *   *(Без міток)* Візуальна оцінка, аналіз характеристик виявлених аномалій.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Парсинг логів та вилучення ознак є найскладнішою частиною***, якщо не використовуються готові структуровані дані. Рекомендується спростити цей етап для курсового проєкту.\n",
    "*   **Некеровані методи є основними** для виявлення аномалій, оскільки мічені дані часто відсутні або неповні.\n",
    "*   **Налаштування параметрів** (особливо `contamination`) для некерованих методів є критично важливим і часто вимагає експертних знань або ітеративного підходу.\n",
    "*   Якщо мітки доступні, **оцінка на сильно незбалансованих даних** потребує використання відповідних метрик (AUC-PR, F1 для аномалій).\n",
    "*   Розгляньте використання **NAB (Тема 25)** як проксі-даних, якщо отримання/обробка логів виявиться занадто складною.\n",
    "\n",
    "---\n",
    "\n",
    "### Тема 60: Інтелектуальна інформаційна система для кластеризування країн за індикаторами розвитку\n",
    "\n",
    "**Набір даних:** World Development Indicators (WDI) від Світового Банку ([https://databank.worldbank.org/source/world-development-indicators](https://databank.worldbank.org/source/world-development-indicators)). **Потребує вибору** конкретних індикаторів та року. Альтернативно, використайте готові підмножини з Kaggle, наприклад, ([https://www.kaggle.com/datasets/theworldbank/world-development-indicators](https://www.kaggle.com/datasets/theworldbank/world-development-indicators) - перевірте вміст) або ([https://www.kaggle.com/datasets/nitishabharathi/analysis-of-world-development-indicatorswdi](https://www.kaggle.com/datasets/nitishabharathi/analysis-of-world-development-indicatorswdi)).\n",
    "\n",
    "**Мета:** **Згрупувати (кластеризувати) країни світу** на основі вибраних **індикаторів соціально-економічного розвитку** (наприклад, ВВП, охорона здоров'я, освіта, інфраструктура, екологія) за певний рік (_Кластеризація_).\n",
    "\n",
    "**Проміжні завдання:**\n",
    "\n",
    "1.  **Вибір Даних, Завантаження та Очищення:**\n",
    "    *   1.1. ***Вибір Індикаторів та Року:***\n",
    "        *   Виберіть **конкретний рік** для аналізу (наприклад, останній доступний).\n",
    "        *   Виберіть **набір (5-15) релевантних індикаторів** з WDI, що відображають різні аспекти розвитку (економіка, здоров'я, освіта, технології, сталий розвиток тощо). Приклади: ВВП на душу населення (GDP per capita, PPP), Очікувана тривалість життя при народженні, Рівень грамотності дорослих, Доступ до Інтернету (% населення), Викиди CO2 (метричних тонн на душу населення), Рівень урбанізації, Смертність дітей до 5 років.\n",
    "        *   Завантажте дані для вибраних індикаторів та року (з сайту Світового Банку або з підмножини Kaggle).\n",
    "    *   1.2. *Підготовка Даних:*\n",
    "        *   Очистіть дані: перейменуйте стовпці, можливо, переформатуйте таблицю (з довгого формату в широкий, де кожен стовпець - індикатор, а рядки - країни).\n",
    "        *   **Обробіть пропущені значення:** Імпутуйте пропуски (наприклад, середнім/медіаною по регіону або глобально) або видаліть країни з великою кількістю пропусків за вибраними індикаторами.\n",
    "        *   Збережіть назви країн для подальшої інтерпретації.\n",
    "\n",
    "2.  **Масштабування Ознак та Зниження Розмірності:**\n",
    "    *   2.1. *Масштабування Ознак:* Індикатори мають дуже різні одиниці виміру та діапазони. ***Застосування `StandardScaler` є абсолютно необхідним***.\n",
    "    *   2.2. *Зниження Розмірності (PCA):*\n",
    "        *   Застосуйте PCA до масштабованих даних.\n",
    "        *   Проаналізуйте кумулятивну пояснену дисперсію. Визначте, скільки компонент пояснюють значну частину варіації (наприклад, >80-90%).\n",
    "        *   **Візуалізуйте країни у просторі перших двох PCA компонент**. Це дасть перше уявлення про можливу структуру кластерів.\n",
    "\n",
    "3.  **Кластеризація та Профілювання Кластерів:**\n",
    "    *   3.1. *Застосування Алгоритмів Кластеризації:* Застосуйте до **масштабованих даних** (оригінальних або PCA-зменшених):\n",
    "        *   **K-Means:** Визначте **оптимальне k** (кількість кластерів) за допомогою методу ліктя та силуетного аналізу. Застосуйте K-Means з цим k.\n",
    "        *   **Agglomerative Clustering (Ієрархічна):** Побудуйте дендрограму для візуалізації ієрархії та вибору кількості кластерів. Застосуйте алгоритм з вибраною кількістю кластерів.\n",
    "        *   (Опціонально) DBSCAN.\n",
    "    *   3.2. *Призначення Міток Кластерів:* Додайте мітки кластерів до DataFrame з країнами та їхніми індикаторами.\n",
    "    *   3.3. ***Профілювання та Інтерпретація Кластерів:*** Це **основна мета**.\n",
    "        *   Для кожного кластера обчисліть **середні (або медіанні) значення вибраних індикаторів розвитку**.\n",
    "        *   **Порівняйте ці середні значення між кластерами.** Які кластери мають високий ВВП? Які - високу тривалість життя? Де низький доступ до Інтернету?\n",
    "        *   **Візуалізуйте профілі кластерів** (наприклад, за допомогою радарних діаграм, стовпчастих діаграм або теплових карт середніх значень).\n",
    "        *   **Подивіться, які країни потрапили до кожного кластера.** Чи відповідає це інтуїтивному уявленню про рівні розвитку країн?\n",
    "        *   ***Дайте кожному кластеру змістовну назву*** (наприклад, \"Високорозвинені країни\", \"Країни, що розвиваються, з акцентом на індустрію\", \"Найменш розвинені країни\" тощо).\n",
    "\n",
    "**Результат Курсового Проєкту:**\n",
    "Розроблено **вебзастосунок на `streamlit`, `gradio` або `replit`**, який:\n",
    "*   Візуалізує **розподіл країн за кластерами розвитку** (на PCA графіку або карті світу).\n",
    "*   Представляє **детальні профілі** (середні значення індикаторів) та **описи** для кожного виявленого кластера країн.\n",
    "*   (Опціонально) Показує список країн, що належать до кожного кластера.\n",
    "\n",
    "**Кроки Розгортання Вебзастосунку:**\n",
    "\n",
    "*   **Використання `streamlit`:** (Добре підходить для візуалізації)\n",
    "    1.  *Створення файлу застосунку:* `app.py`.\n",
    "    2.  *Імпорт бібліотек:* `streamlit`, `pandas`, `numpy`, `matplotlib`/`seaborn`/`plotly`, `sklearn`, `joblib`/`pickle`, опціонально `geopandas`.\n",
    "    3.  *Завантаження ресурсів:* Завантажте оброблені дані з мітками кластерів, PCA модель (якщо використовувалася для візуалізації), збережені профілі кластерів. Можливо, знадобляться геодані для карти.\n",
    "    4.  *Створення інтерфейсу та візуалізацій:*\n",
    "        *   Відобразіть **PCA графік** (`st.scatter_chart`/`st.plotly_chart`) або **карту світу** (`st.plotly_chart` з `px.choropleth` або `st.pydeck_chart`/`st.map`), забарвлену за кластерами. Додайте назви країн у спливаючі підказки.\n",
    "        *   Використовуйте вкладки або селектор для вибору кластера.\n",
    "        *   Для вибраного кластера покажіть його назву, опис, **профіль індикаторів** (графік або таблиця), список країн.\n",
    "    5.  *Запуск та Розгортання:* Стандартні кроки.\n",
    "\n",
    "*   **Використання `gradio` / `replit`:**\n",
    "    *   `Gradio` менше підходить для інтерактивних карт/графіків, але може показувати статичні візуалізації та таблиці.\n",
    "    *   `Replit` може хостити `streamlit` застосунок.\n",
    "\n",
    "**Необхідні бібліотеки Python:**\n",
    "*   **Основні:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `scikit-learn`\n",
    "*   **Специфічні для завдання:** `scipy` (для ієрархічної кластеризації), опціонально `plotly`, `geopandas` (для карт).\n",
    "*   **Розгортання:** `streamlit` (рекомендовано), `plotly`, `joblib` або `pickle`, опціонально `geopandas`.\n",
    "\n",
    "**Рекомендовані методи візуалізації та оцінки:**\n",
    "*   **Візуалізація:** Графіки Elbow/Silhouette, **PCA scatter plot (забарвлений за кластером)**, **карта світу (забарвлена за кластером)**, **профілі кластерів (радарні/стовпчасті діаграми середніх значень)**.\n",
    "*   **Метрики оцінки:** Silhouette Score, Davies-Bouldin Index. ***Основний акцент - на інтерпретованості та змістовності отриманих кластерів країн***.\n",
    "\n",
    "**Поради щодо реалізації:**\n",
    "*   ***Вибір релевантних індикаторів розвитку є ключовим*** для отримання змістовних кластерів. Уникайте надмірної кількості сильно скорельованих індикаторів.\n",
    "*   **Оброблення пропущених значень** може суттєво вплинути на результати кластеризації.\n",
    "*   **Масштабування ознак (`StandardScaler`) є необхідним**.\n",
    "*   **Профілювання та інтерпретація кластерів є головною метою**. Кластери повинні мати логічне пояснення з точки зору соціально-економічного розвитку.\n",
    "*   Візуалізація на карті світу є дуже ефективним способом представлення результатів.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
